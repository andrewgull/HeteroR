---
title: "Prediction of incomplete assemblies"
author: "AG"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 5, message = F, warning = F)

library(tidyverse)
library(GGally)
library(caret)

```

Now, when I have more data on incomplete and complete assemblies, I can build a better model
predicting assembly completeness.

If an assembly is predicted to be incomplete, then you can use full amount of reads or adjust filtering strategy

# Nanopore coverage data

## Get coverage data

Get table with coverage using your python script from HeteroR workflow

```{bash, eval=FALSE}
# run from ~/Data/HeteroR

python workflow/scripts/coverage.py resources/strain_lists/nanopore_sequenced_strains.txt 5131220 results/coverage/nanopore_sequenced_strains_raw.tsv raw

```

Read the results and add completeness information

## Read and clean raw coverage data

```{r}
nano_cov_raw <- read_delim("~/Data/HeteroR/results/coverage/nanopore_sequenced_strains_raw.tsv", na = c("NaN", ""))
nano_cov_raw$strain <- str_extract(nano_cov_raw$file, "DA[0-9]*")
nano_cov_raw <- relocate(nano_cov_raw, strain, .before = "file")

nano_cov_raw
```

Strains DA70* are not E.coli and their coverage should be calculated differently.

Remove them.

```{r}
da70 <- str_detect(nano_cov_raw$strain, "DA70", negate = T)
nano_cov_raw <- nano_cov_raw[da70,]
```

Also there is one strain (DA63062) that was not copied from Argos.

Remove its row from the data set.

```{r}
nano_cov_raw <- nano_cov_raw %>%  filter(strain != "DA63062")

sum(!complete.cases(nano_cov_raw))
```



## Get and clean chromosome completeness data

```{r}
assemblies_summary <- read_csv("~/Data/HeteroR/results/tables/genome_assembly_summary.csv")
assemblies_summary <- assemblies_summary %>% 
  select(Length, N50, Longest_component, Status, Status2, Strain, Type) %>% 
  filter(Type == "Chromosome") %>% 
  distinct() %>% 
  rename(strain="Strain")

```


Strains DA68* are absent from `nano_cov_raw` because they don't exist on Argos' IMB_SAL_RAW (reason's unknown).

But raw sequencing files exist on my desktop

I will calculate them separately

### Get list of DA8*

```{r, eval=FALSE}
da68 <- assemblies_summary$strain[str_detect(assemblies_summary$strain, "DA68")]

write(da68, file="~/Data/HeteroR/resources/strain_lists/strainsDA68.txt", ncolumns = 1)
```


```{python, eval=FALSE}
python workflow/scripts/coverage.py resources/strain_lists/strainsDA68.txt 5131220 results/coverage/nanopore_sequenced_strains_DA68_raw.tsv raw
```

### Add DA68* to the rest

```{r}
da68 <- read_delim("~/Data/HeteroR/results/coverage/nanopore_sequenced_strains_DA68_raw.tsv")

da68$strain <- str_extract(da68$file, "DA[0-9]*")
da68 <- relocate(da68, strain, .before = "file")

nano_cov_raw <- bind_rows(nano_cov_raw, da68)

sum(!complete.cases(nano_cov_raw))
```


# Data set creation and EDA

Attempt number 2, on a bigger data set.

Add raw data statistics to assemblies summary

## Make a data set

```{r}
my_data <- left_join(
  select(assemblies_summary, c(strain, Status)),
  select(nano_cov_raw, c(strain, num_seqs, sum_len, min_len, avg_len, max_len, coverage)),
  by="strain"
  )

my_data$Status <- as.factor(my_data$Status)
my_data$strain <- NULL

summary(my_data)
```

One NA is the same old DA63062. Remove it for now

```{r}
# 
my_data <- my_data[complete.cases(my_data),]

summary(my_data)
```


## Make tidy

```{r}
my_data_td <- my_data  %>% gather(key="statistic", value="value", 2:7)
```

## Box plots

```{r, fig.width=10}
ggplot(my_data_td, aes(Status, value))+
  geom_boxplot(varwidth = T, aes(fill=Status))+
  coord_trans(y="log")+
  facet_grid(. ~ statistic)+
  scale_fill_brewer(palette = "Set1")+
  xlab("")+
  ylab("")
```

## Pairs plot

```{r, fig.width=10}
ggpairs(my_data, 
        columns=c(2:7), 
        aes(color=Status, alpha=0.2, dotsize=0.02), 
        upper = list(continuous = wrap("cor", size = 2.5)),
        diag=list(continuous ="barDiag"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")

```

Correlated pairs:

 - coverage - num_seqs (0.8)
 - coverage - sum_len (1.0)
 - sum_len - num_seqs (0.8)


# Split data set

I will remove correlated features, leaving only `sum_len`

```{r}
set.seed(100)

my_data <- select(training,-c(coverage, num_seqs))

inTrain <- createDataPartition(y = my_data$Status, p=0.7, list=FALSE)

training <- my_data[inTrain,]
testing <- my_data[-inTrain,]

```


# Regularized Logistic Regression

## M1: One predictor: sum_len

### Parameters tuning

```{r}
fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T)
```

### Training

```{r}
set.seed(100)
Fit1 <- train(Status ~ ., data = select(training, Status, sum_len), 
                 method = "regLogistic", 
                 trControl = fitControl,
                 verbose = FALSE)
Fit1
```

### Parameters plot

```{r}
ggplot(Fit1)
```

### Coefficients

```{r}
Fit1$finalModel$W
```

## M2: All non-correlated predictors

I remove coverage and num_seqs

### Training

```{r}
set.seed(100)

Fit2 <- train(Status ~ ., data = training, 
                 method = "regLogistic",
                 trControl = fitControl,
                 verbose = FALSE)
Fit2
```

### Parameters plot

```{r}
ggplot(Fit2)
```

### Coefficients

```{r}
Fit2$finalModel$W
```

## M2b: all non-correlated + scaling

```{r}
set.seed(100)

fitControl2 <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10,
                           allowParallel = T)

Fit2b <- train(
  Status ~ .,
  data = training,
  trControl = fitControl2,
  method = "regLogistic",
  preProc = c("center", "scale")
)

Fit2b
```

No difference...

But coefficients are different of course

```{r}
Fit2b$finalModel$W
```

## M2c: sum_len + ROC tuning


```{r}
fitControl3 <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10,
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

set.seed(100)

Fit2c <- train(
  Status ~ .,
  data = select(training, Status, sum_len),
  trControl = fitControl3,
  method = "regLogistic",
  metric="ROC"
)

Fit2c
```


```{r}
Fit2c$finalModel$W
```


## M2d: all non-correlated + ROC tuning

### Tuning & training

```{r}
fitControl4 <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10,
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

set.seed(100)

Fit2d <- train(
  Status ~ .,
  data = training,
  trControl = fitControl4,
  method = "regLogistic",
  metric="ROC"
)

Fit2d
```

```{r}
Fit2d$finalModel$W
```

```{r}
ggplot(Fit2d)
```


# Predict

## Model 1

One predictor (sum_len), accuracy tuning

```{r}

predClasses1 <- predict(Fit1, newdata=testing)

confusionMatrix(data = predClasses1, 
                reference = testing$Status,
                mode="everything",
                positive="incomplete")
```


## Model 2

All predictors, accuracy tuning

```{r}
predClasses2 <- predict(Fit2, newdata=testing)

predProb2 <- predict(Fit2, newdata=testing, type = "prob")
```



```{r}
confusionMatrix(data = predClasses2, 
                reference = testing$Status,
                mode="everything",
                positive="incomplete")
```

## Model 2b

All predictors, scaled, accuracy tuning

```{r}
predClasses2b <- predict(Fit2b, newdata=testing)

confusionMatrix(data = predClasses2b, 
                reference = testing$Status,
                mode="everything",
                positive="incomplete")
```

## Model 2c

sum_len + ROC

```{r}
predClasses2c <- predict(Fit2c, newdata=select(testing,Status, sum_len))

confusionMatrix(data = predClasses2c, 
                reference = testing$Status,
                mode="everything",
                positive="incomplete")
```

## Model 2d

All predictors, ROC tuning

```{r}
predClasses2d <- predict(Fit2d, newdata=testing)

predProbs2d <- predict(Fit2d, newdata=testing, type = "prob")

confusionMatrix(data = predClasses2d, 
                reference = testing$Status,
                mode="everything",
                positive="incomplete")
```

## Model comparison

### sum_len + acc VS all + acc

```{r}
resamps <- resamples(list(sum_len = Fit1, all_acc = Fit2))
summary(resamps)
```

### sum_len + roc VS all + roc

```{r}
resamps2 <- resamples(list(sum_len = Fit2c, all_roc = Fit2d))
summary(resamps2)
```

all predictors + ROC is better

```{r}
xyplot(resamps2, what = "BlandAltman")
```

```{r}
diffs <- diff(resamps2)

summary(diffs)
```

We can not reject H0
