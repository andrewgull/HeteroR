---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(tidyposterior) # for Bayesian ANOVA
#library(embed) # for UMAP
library(finetune) # for win-loss tuning
source("functions.R")

# path for models
models_path <- "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/"
```

# Read and process data

The data sets are the same as in EDA

```{r read data}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf")) %>% 
  filter(strain != "DA63310")

# TWO SCHEMES
hr_testing12 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr12)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr12 )

hr_testing13 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr13)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr13 )

# created during EDA
hr_testing <- read_csv("data/heteroresistance_testing_gr123.csv") 

# USE HR 1+2
data_strain <- data_strain %>% 
  left_join(hr_testing12, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \>4

```{r init processing}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r data split}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Base Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r main recipe}
base_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
base_recipe %>% prep() %>% juice() %>% dim()
```

Target class:

```{r}
base_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```


## Base + YJ transformation

```{r}
base_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## Base + ORQ transformation

```{r}
base_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## PCA

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

Borrowed from 'Tidy modeling with R': Chapter 13.2 'Evaluating the grid'

```{r pca+yj recipe}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>%
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_numeric_predictors())
```

## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r norm recipe}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + Yeo-Johnson transform

development of the approach above, but with decorrelation and no PCA

```{r yj recipe}
ncorr_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + ORQ-transform recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r orq recipe}
ncorr_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## Base + Boruta feature selection

from `colino` package

to use with RF and BT algorithms

fixed on local copy - the original package in the tibble creation has
`name = c("top_p", "threshold"),` instead of
`name = c("top_p", "threshold", "cutoff"),`

the git repository was cloned locally, corrected and the package
installed following <https://kbroman.org/pkg_primer/pages/build.html>

(this was done to employ the RFE steps)

```{r rfe recipe}
if (!require(colino, quietly = TRUE))
  devtools::install_github("andrewgull/colino")

base_boruta_recipe <- base_recipe %>%
  step_select_boruta(all_predictors(), outcome = "resistance")

base_boruta_recipe %>% prep() %>% juice() %>% ncol()
```

## Base + YJ transform + Boruta feature selection

```{r boruta+yj recipe}
base_yj_boruta_recipe <- base_yj_recipe %>% 
  step_select_boruta(all_predictors(), outcome = "resistance")

base_yj_boruta_recipe %>% prep() %>% juice() %>% ncol()
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r cv and metrics}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to tune hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Rule-based prediction

Let's take a rule 'if there is more than 4 beta-lactamases, then the strain is HR' (from EDA)

```{r base rule}
rule_pred <- 
  df_train %>% 
  mutate(pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred)
```

```{r}
conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```


# Penalized LR

By default all models here are LASSO regression models, unless specific
type of penalization is stated.


## Base recipe 

```{r lr norm, fig.height=6, fig.width=6}
lr_base_res <- read_wfset("models_llr.rds", "base_LLR")

autoplot(lr_base_res)
```

### Best models

```{r}
lr_base_res %>% 
  show_best("roc_auc", n = 5) 
```

## Base + YJ recipe

```{r lr yj, fig.height=6, fig.width=6}
lr_base_yj_res <- read_wfset("models_llr.rds", "baseyj_LLR")

autoplot(lr_base_yj_res)
```

### Best models

```{r}
show_best(lr_base_yj_res)
```

```{r}
show_best(lr_base_yj_res, metric = "j_index")
```

The same model is the best

## Base + ORQ recipe

```{r lr orq}
lr_base_orq_res <- read_wfset("models_llr.rds", "baseorq_LLR")

lr_base_orq_res %>% autoplot()
```

### Best models

```{r}
lr_base_orq_res %>%
  show_best("roc_auc", n = 5)
```

## PCA recipe

```{r lr pca}
lr_pca_res <- read_wfset("models_llr.rds", "pca_LLR")

autoplot(lr_pca_res)
```

### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```



## Compare preprocessors

### ROCs overlapped

```{r}
lr_best_res <- list(lr_base_res,
                    lr_base_yj_res,
                    lr_base_orq_res,
                    lr_pca_res)

lr_best_names <-
  c("base",
    "base.yj",
    "base.orq",
    "pca")
```


```{r lr compare roc}
lr_rocs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_roc(x, y))

lr_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, ROC curves")
```

### PR-curves overlapped

```{r lr compare pr}
lr_prs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_pr(x, y))

lr_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, PR curves")
```

### Generate distributions

*post hoc* analysis of resampling results generated by models - via
Bayesian ANOVA

```{r}
lr_comp <-
  res_comp_table(
    res_list = lr_best_res,
    mod_names = lr_best_names
  )

lr_posterior <-
  perf_mod(
    lr_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(lr_posterior)
```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(lr_posterior, seed = 100) 

summary(preproc_diff)
```

Columns `upper` and `lower` represent credible interval for the mean
difference between preprocessing's AUCs.

Positive mean value means that mean of norm \> mean of orq etc.

If CI doesn't include 0, then the difference is significant.

PCA performs significantly worse than the other models.

Base, YJ and ORQ pre-processing recipes don't differ significantly in ROC AUC, but there is a difference in PR curves: base has better PR-curve.

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r mars norm}
mars_ncorr_res <- read_wfset("models_mars_knn.rds", "ncorr_mars")

autoplot(mars_ncorr_res)
```

### Best models

```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

## NCORR + YJ recipe

```{r}
mars_ncoryj_res <- read_wfset("models_mars_knn.rds", "ncorryj_mars")

autoplot(mars_ncoryj_res)
```

### Best models

```{r}
mars_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + ORQ recipe

```{r}
mars_ncorq_res <- read_wfset("models_mars_knn.rds", "ncorrorq_mars")

autoplot(mars_ncorq_res)
```

### Best models

```{r}
mars_ncorq_res %>% show_best("roc_auc", n = 5)
```

## PCA

```{r}
mars_pca_res <- read_wfset("models_mars_knn.rds", "pca_mars")

autoplot(mars_pca_res)
```

```{r}
show_best(mars_pca_res, metric = "roc_auc")
```


# Bagged MARS 

## NCORR + YJ recipe

```{r}
mars_bag_ncoryj_res <- read_wfset("models_mars_knn.rds", "ncorryj_bag_mars")

autoplot(mars_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mars_bag_ncoryj_res)
```

## NCORR + ORQ recipe

```{r}
mars_bag_ncorq_res <- read_wfset("models_mars_knn.rds", "ncorrorq_bag_mars")

autoplot(mars_bag_ncorq_res)
```

```{r}
show_best(mars_bag_ncorq_res, "roc_auc")
```

## PCA

```{r}
mars_bag_pca_res <- read_wfset("models_mars_knn.rds", "pca_bag_mars")

autoplot(mars_bag_pca_res)
```

```{r}
show_best(mars_bag_pca_res)
```


## Compare preprocessors

### ROCs overlapped

```{r}
mars_best_res <- list(
    mars_ncorr_res,
    mars_ncoryj_res,
    mars_ncorq_res,
    mars_pca_res,
    mars_bag_ncoryj_res,
    mars_bag_ncorq_res,
    mars_bag_pca_res
  )
  
mars_best_names <- c(
    "ncorr",
    "ncorr.yj",
    "ncorr.orq",
    "pca",
    "bag.yj",
    "bag.orq",
    "bag.pca"
  )

mars_rocs <-
  map2_dfr(mars_best_res, mars_best_names,
  function(x, y)
    make_roc(x, y))

mars_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MARS, ROC curves")
```

### PR curves overlapped

```{r}
mars_prs <-
  map2_dfr(mars_best_res,
  mars_best_names,
  function(x, y)
    make_pr(x, y))

mars_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MARS, PR curves")
```


## Comparison of the preprocessing steps

```{r}
mars_comp <-
  res_comp_table(
    res_list = mars_best_res,
    mod_names = mars_best_names
  )

mars_posterior <-
  perf_mod(
    mars_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(mars_posterior)
```

```{r}
mars_preproc_diff <- contrast_models(mars_posterior, seed = 100) 

summary(mars_preproc_diff)
```

# Linear support vector machines (lSVM)

```{r}
# models in this file
readRDS(paste0(models_path, "models_svm_mlp.rds")) %>% 
  filter(grepl("lsvm", wflow_id)) %>% 
  pull(wflow_id)
```


## NCORR recipe

```{r lsvm norm}
lsvm_ncorr_res <- read_wfset("models_svm_mlp.rds", "ncorr_lsvm")

autoplot(lsvm_ncorr_res)
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ recipe

```{r}
lsvm_ncoryj_res <- read_wfset("models_svm_mlp.rds", "ncorryj_lsvm")

autoplot(lsvm_ncoryj_res)
```

### Best models

```{r}
lsvm_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + ORQ recipe

```{r}
lsvm_ncorq_res <-  read_wfset("models_svm_mlp.rds", "ncorrorq_lsvm")

autoplot(lsvm_ncorq_res)
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```

## PCA

```{r}
lsvm_pca_res <-  read_wfset("models_svm_mlp.rds", "pca_lsvm")

autoplot(lsvm_pca_res)
```

### Best models

```{r}
show_best(lsvm_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROCs overlapped

```{r}
lsvm_best_res <- list(
    lsvm_ncorr_res,
    lsvm_ncoryj_res,
    lsvm_ncorq_res,
    lsvm_pca_res
  )
  lsvm_best_names <- c(
    "ncorr",
    "ncorr.yj",
    "ncorr.orq",
    "pca"
  )
```


```{r}
lsvm_rocs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_roc(x, y))

lsvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, ROC curves")
```

### PR-curves overlapped

```{r lsvm compare pr}
lsvm_prs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_pr(x, y))

lsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, PR curves")
```

### Distributions

```{r}
lsvm_comp <-
  res_comp_table(
    res_list = lsvm_best_res,
    mod_names = lsvm_best_names
  )

lsvm_posterior <-
  perf_mod(
    lsvm_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(lsvm_posterior)
```

### Difference estimates

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

# Polynomial support vector machines (pSVM)

## NCORR recipe

```{r psvm norm}
psvm_ncorr_res <- read_wfset("models_svm_mlp.rds", "ncorr_psvm")

autoplot(psvm_ncorr_res)
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ

```{r}
psvm_ncoryj_res <- read_wfset("models_svm_mlp.rds", "ncorryj_psvm")

autoplot(psvm_ncoryj_res)
```

### Best models

```{r}
show_best(psvm_ncoryj_res, metric = "roc_auc")
```


## NCORR + ORQ recipe

```{r}
psvm_ncorq_res <- read_wfset("models_svm_mlp.rds", "ncorrorq_psvm")

autoplot(psvm_ncorq_res)
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

## PCA 

```{r}
psvm_pca_res <- read_wfset("models_svm_mlp.rds", "pca_psvm")

autoplot(psvm_pca_res)
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

## Comparison of the preprocessing steps

### ROCs overlapped

```{r}
psvm_best_res <- list(
    psvm_ncorr_res,
    psvm_ncorq_res,
    psvm_ncoryj_res,
    psvm_pca_res
  )

psvm_best_names <- c(
    "ncorr",
    "ncorr.orq",
    "ncorr.yj",
    "pca"
  )
```


```{r}
psvm_rocs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_roc(x, y))

psvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, ROC curves")
```

### PR curves overlapped

```{r}
psvm_prs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_pr(x, y))

psvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, PR curves")
```


### Distributions

```{r}
psvm_comp <-
  res_comp_table(
    res_list = list(
      psvm_ncorr_res,
      psvm_ncorq_res,
      psvm_ncoryj_res,
      psvm_pca_res
    ),
    mod_names = c(
      "ncorr",
      "ncorr.orq",
      "ncorr.yj",
      "pca.yj"
    )
  )

psvm_posterior <-
  perf_mod(
    psvm_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(psvm_posterior)
```

### Estimate the difference

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```

# Radial Basis Function SVM

## NCORR recipe

```{r}
rbf_ncorr_res <- read_wfset("models_svm_mlp.rds", "ncorr_rbfsvm")

autoplot(rbf_ncorr_res)
```

### Best models

```{r}
show_best(rbf_ncorr_res, metric = "roc_auc")
```

## NCORR + YJ recipe

```{r}
rbf_ncoryj_res <- read_wfset("models_svm_mlp.rds", "ncorryj_rbfsvm")

autoplot(rbf_ncoryj_res)
```

### Best models

```{r}
show_best(rbf_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ recipe

```{r}
rbf_ncorq_res <- read_wfset("models_svm_mlp.rds", "ncorrorq_rbfsvm")

autoplot(rbf_ncorq_res)
```

### Best models

```{r}
show_best(rbf_ncorq_res, metric = "roc_auc")
```

## PCA

```{r}
rbf_pca_res <- read_wfset("models_svm_mlp.rds", "pca_rbfsvm")

autoplot(rbf_pca_res)
```


```{r}
show_best(rbf_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROC curves overlapped

```{r}
rsvm_best_res <- list(
    rbf_ncorr_res,
    rbf_ncorq_res,
    rbf_ncoryj_res,
    rbf_pca_res
    
  )

rsvm_best_names <- c(
    "ncorr",
    "ncorr.orq",
    "ncorr.yj",
    "pca"
  )
```


```{r}
rbf_rocs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_roc(x, y))

rbf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, ROC curves")
```

### PR curves overlapped

```{r}
rsvm_prs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_pr(x, y))

rsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, PR curves")
```

### Distributions

```{r}
rbf_comp <-
  res_comp_table(
    res_list = rsvm_best_res,
    mod_names = rsvm_best_names
  )

rbf_posterior <-
  perf_mod(
    rbf_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rbf_posterior)
```

### Difference estimates

```{r}
rbf_preproc_diff <- contrast_models(rbf_posterior, seed = 100) 

summary(rbf_preproc_diff)
```

# k-Nearest Neighbors (KNN)

## NCORR recipe

```{r knn norm}
knn_ncorr_res <- read_wfset("models_mars_knn.rds", "ncorr_knn")

autoplot(knn_ncorr_res)
```

### Best models

```{r}
knn_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ

```{r}
knn_ncoryj_res <- read_wfset("models_mars_knn.rds", "ncorryj_knn")

autoplot(knn_ncoryj_res)
```

### Best models

```{r}
show_best(knn_ncoryj_res)
```

## NCORR + ORQ recipe

```{r}
knn_ncorq_res <- read_wfset("models_mars_knn.rds", "ncorrorq_knn")

autoplot(knn_ncorq_res)
```

### Best models

```{r}
knn_ncorq_res %>% 
  show_best("roc_auc")
```


## PCA

```{r}
knn_pca_res <- read_wfset("models_mars_knn.rds", "pca_knn")

autoplot(knn_pca_res)
```

### Best models

```{r}
knn_pca_res %>% 
  show_best("roc_auc")
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
knn_best_res <- list(knn_ncorr_res,
                     knn_ncorq_res,
                     knn_ncoryj_res,
                     knn_pca_res)
knn_best_names <- c("ncorr",
                    "ncorr.orq",
                    "ncorr.yj",
                    "pca")
```


```{r}
knn_rocs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_roc(x, y))

knn_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, ROC curves")
```

### PR curves overlapped

```{r}
knn_prs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_pr(x, y))

knn_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, PR curves")
```

### Posterior distributions

```{r}
knn_comp <-
  res_comp_table(
    res_list = knn_best_res,
    knn_best_names
  )

knn_posterior <-
  perf_mod(
    knn_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(knn_posterior)
```

### Difference estimates

```{r}
knn_preproc_diff <- contrast_models(knn_posterior, seed = 100) 

summary(knn_preproc_diff)
```

# Random Forest (RF)

## Base recipe

```{r rf base}
rf_base_res <- read_wfset("models_xgb_rf_light.rds", "base_rf")

autoplot(rf_base_res)
```

### Best models

```{r}
rf_base_res %>% 
  show_best("roc_auc", n = 5)
```

## Base + Boruta feature selection

```{r}
rf_base_bor_res <- read_wfset("models_xgb_rf_light.rds", "base_boruta_rf")

autoplot(rf_base_bor_res)
```

```{r}
show_best(rf_base_bor_res)
```

## Comparison of preprocessors

### ROCs overlapped

```{r}
rf_best_res <- list(rf_base_res, rf_base_bor_res)
  
rf_best_names <- c("base", "boruta")
```


```{r}
rf_rocs <-
  map2_dfr(rf_best_res, 
           rf_best_names,
           function(x, y)
             make_roc(x, y))

rf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, ROC curves")
```

### PR curves overlapped

```{r}
rf_prs <-
  map2_dfr(rf_best_res,
           rf_best_names,
           function(x, y)
             make_pr(x, y))

rf_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, PR curves")
```

### Distributions

```{r}
rf_comp <-
  res_comp_table(
    res_list = rf_best_res,
    mod_names = rf_best_names
  )

rf_posterior <-
  perf_mod(
    rf_comp,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rf_posterior)
```

### Difference estimates

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

# Gradient Boosted Trees (GBT)

BT are sensitive to hyper-parameters that's why, Bayesian grid search
should be preferable.

## Base recipe

space search followed by the Bayesian optimization

```{r, fig.width=8, fig.height=6}
bt_base_res <- read_wfset("models_xgb_rf_light.rds", "base_bt_light")

autoplot(bt_base_res)
```

### Best models

```{r}
bt_base_res %>% 
  show_best(metric = "roc_auc")
```


## Base recipe + Bayesian optimization

Bayesian search

```{r, fig.width=8, fig.height=6}
bt_base_bres <- readRDS(paste0(models_path, "base_bt_bres_light.rds"))

bt_base_bres %>% 
  autoplot()
```

### Best models

```{r}
bt_base_bres %>% 
  show_best(metric = "roc_auc") 
```

>Shrinkage (learning rate) is a gradient boosting regularization procedure that helps modify the update rule, which is aided by a parameter known as the learning rate. The use of learning rates below 0.1 produces improvements that are significant in the generalization of a model.

## Base recipe + Boruta feature selection

```{r}
bt_base_bor_res <- read_wfset("models_xgb_rf_light.rds", "base_boruta_bt_light")

autoplot(bt_base_bor_res)
```

### Best models

```{r}
show_best(bt_base_bor_res)
```

## Base recipe + Boruta + Bayesian search

```{r, fig.width=10}
bt_base_bor_bres <- readRDS(paste0(models_path, "base_bt_boruta_bres_light.rds"))

autoplot(bt_base_bor_bres)
```

```{r}
show_best(bt_base_bor_bres)
```

## Comparison of the preprocessors

### ROCs

```{r}
bt_best_res <- list(
    bt_base_res,
    bt_base_bres,
    bt_base_bor_res,
    bt_base_bor_bres
  )

bt_best_names <- c(
    "base",
    "base.bayes",
    "base.bor",
    "base.bor.bayes"
  )

bt_rocs <-
  map2_dfr(bt_best_res,
  bt_best_names,
  function(x, y)
    make_roc(x, y))

bt_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, ROC curves")
```

### PR curves overlapped

```{r}
bt_prs <-
  map2_dfr(bt_best_res,
           bt_best_names,
           function(x, y)
             make_pr(x, y))

bt_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, PR curves")
```

### Posterior distributions

I have excluded models tuned with win-loss approach, since their performance was significantly worse

```{r}
bt_comp <-
  res_comp_table(
    res_list = bt_best_res,
    mod_names = bt_best_names
  )

bt_posterior <-
  perf_mod(
    bt_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(bt_posterior)
```

### Estimating the differences

```{r}
bt_preproc_diff <- contrast_models(bt_posterior, seed = 100) 

summary(bt_preproc_diff) 
```

# Multilayer perceptron (MLP)

aka feed-forward neural network

## NCORR recipe


```{r mlp ncorr}
mlp_ncor_res <- read_wfset("models_svm_mlp.rds", "ncorr_mlp_nnet")

autoplot(mlp_ncor_res)
```

### Best models

```{r}
mlp_ncor_res %>% 
  show_best("roc_auc")
```

### Predictors that remained

```{r}
mlp_cor_threshold <- mlp_ncor_res %>% select_best("roc_auc") %>% pull(corr_tune)

recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = mlp_cor_threshold) %>% 
  prep() %>% 
  juice() %>% 
  colnames()
```

## NCORR + YJ recipe

```{r}
mlp_ncoryj_res <- read_wfset("models_svm_mlp.rds", "ncorryj_mlp_nnet")

mlp_ncoryj_res %>% autoplot()
```

### Best models

```{r}
mlp_ncoryj_res %>% 
  show_best("roc_auc")
```

## NCORR + ORQ

```{r}
mlp_ncorq_res <- read_wfset("models_svm_mlp.rds", "ncorrorq_mlp_nnet")

mlp_ncorq_res %>% autoplot()
```

### Best models

```{r}
mlp_ncorq_res %>% show_best(metric = "roc_auc")
```

## PCA

```{r}
mlp_pca_res <- read_wfset("models_svm_mlp.rds", "pca_mlp_nnet")

mlp_pca_res %>% autoplot()
```

### Best models

```{r}
mlp_pca_res %>% show_best("roc_auc")
```


# Bag MLP

## NCORR

```{r}
mlp_bag_ncorr_res <- read_wfset("models_svm_mlp.rds", "ncorr_bag_mlp")

autoplot(mlp_bag_ncorr_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorr_res)
```

## NCORR + YJ

```{r}
mlp_bag_ncoryj_res <- read_wfset("models_svm_mlp.rds", "ncorryj_bag_mlp")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ

```{r}
mlp_bag_ncorq_res <- read_wfset("models_svm_mlp.rds", "ncorrorq_bag_mlp")

autoplot(mlp_bag_ncorq_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorq_res, metric = "roc_auc")
```

## PCA

```{r}
mlp_bag_pca_res <- read_wfset("models_svm_mlp.rds", "pca_bag_mlp")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_pca_res, metric = "roc_auc")
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
mlp_best_res <- list(
    mlp_ncor_res,
    mlp_ncoryj_res,
    mlp_ncorq_res,
    mlp_pca_res,
    mlp_bag_ncorr_res,
    mlp_bag_ncoryj_res,
    mlp_bag_ncorq_res,
    mlp_pca_res
  )

mlp_best_names <- c(
    "ncorr",
    "ncorr.yj",
    "ncorr.orq",
    "pca",
    "bag.ncorr",
    "bag.ncorr.yj",
    "bag.ncorr.orq",
    "bag.pca"
  )

mlp_rocs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_roc(x, y))

mlp_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, ROC curves")
```

### PR curves overlapped

```{r}
mlp_prs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_pr(x, y))

mlp_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, PR curves")
```

### Posterior distributions

```{r}
mlp_comp <-
  res_comp_table(
    res_list = mlp_best_res,
    mod_names = mlp_best_names
  )

mlp_posterior <-
  perf_mod(
    mlp_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(mlp_posterior)
```

### Difference estimates

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```


# Comparison of models

## ROCs

```{r comparison1}
# you will need these two objects later
best_resamples_list <- list(
    lr_base_orq_res,
    lsvm_ncorr_res,
    psvm_ncorq_res,
    mlp_bag_ncorq_res,
    rf_base_res,
    bt_base_bres
  )
best_resamples_names <- c(
    "LLR",
    "SVM linear",
    "SVM polynomial",
    "MLP",
    "RF",
    "GBT"
  )

# calculate rocs
roc_df <-
  map2_dfr(best_resamples_list,
  best_resamples_names,
  function(x, y)
    make_roc(x, y))

Fig_9A <- roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set2") 

Fig_9A
```

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each
resamples object

```{r}
mod_comparison <- res_comp_table(best_resamples_list,
                                 best_resamples_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

Fig_9B <- autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set2") + 
  theme(plot.margin = margin(t = 23, r = 15, b = 23, l = 15, unit = "pt"))
Fig_9B
```

```{r}
Fig_9AB <- ggarrange(Fig_9A, Fig_9B,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 1, common.legend = TRUE,
                    legend = "bottom")
Fig_9AB
```

## Difference estimates

```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

### Insignificant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper > 0)
```
### Significant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper < 0)
```

```{r}
summary(mod_diff) %>% filter(lower > 0, upper > 0)
```


## Top models

### ROC curves

```{r comparison2}
# calculate rocs
top_models <- list(
    lr_base_res,
    bt_base_bres)

top_names <- c("LR", "GBT")

roc3_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_roc(x, y))
roc3_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("ROC curves, top models")

```

### PR curves

```{r}
pr_top_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_pr(x, y))
pr_top_df %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0,1)) + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("PR curves, top models")
```

### Difference estimates

```{r}
mod_comparison <- res_comp_table(top_models,
                                 top_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set1")
```


```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

GBT is significantly better by 0.014 mean AUC

# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Functions

```{r ff func}
# function to run last fit process
run_last_fit <-
  function(model_name,
           blank_spec,
           tuned_spec,
           recipe,
           train_test_split = data_split,
           return_fit = FALSE) {
    # last workflow
    last_wf <- workflow() %>%
      add_model(blank_spec) %>%
      add_recipe(recipe) %>%
      update_model(tuned_spec)
    
    set.seed(345)
    # final fit
    final_fit <- final_metrics <-
      last_wf %>%
      last_fit(train_test_split)
    # final metrics
    final_metrics <-
      final_fit %>%
      collect_metrics() %>%
      select(.metric, .estimate) %>%
      mutate(model = model_name)
    if (return_fit) {
      return(list(final_metrics, final_fit))
    } else {
      return(list(final_metrics, NULL))
    }
  }

# required by 4 models + 1 if it's ncorr
update_corr_treshold <- function(base_recipe = yj_recipe, step_no, corr_threshold) {
  # update step_corr threshold
  base_recipe$steps[[step_no]] <-
   recipes::update(base_recipe$steps[[step_no]], threshold = corr_threshold)
  
  return(base_recipe)
}

# best models
#top_models <- list(lr_base_res, gbt_base_bres)
best_models <- map(top_models, ~select_best(., "roc_auc"))
```

## LR final

```{r lr ff}
set.seed(800)

# LR specs
lr_spec_blank <- logistic_reg(
  penalty = tune(),
  mixture = 1,
  engine = "glmnet",
  mode = "classification"
)

lr_spec_tuned <-
  logistic_reg(
    penalty = best_models[[1]]$penalty,
    mixture = 1,
    engine = "glmnet",
    mode = "classification"
  )

# LR last fit and metrics
lr_final <-
  run_last_fit("LR",
               lr_spec_blank,
               lr_spec_tuned,
               recipe = base_recipe,
               return_fit = TRUE)

lr_final[[1]]
```



## GBT final

```{r gbt ff}
set.seed(300)

library(bonsai)

# GBT base specs /number 2/
gbt_spec_blank <- boost_tree(
      trees = tune(),
      mtry = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune()) %>%
      set_engine("lightgbm", num.threads = 8) %>%
      set_mode("classification") %>% translate()

gbt_spec_tuned <- boost_tree(
  trees = best_models[[2]]$trees,
  mtry = best_models[[2]]$mtry,
  min_n = best_models[[2]]$min_n,
  tree_depth = best_models[[2]]$tree_depth,
  learn_rate = best_models[[2]]$learn_rate,
  loss_reduction = best_models[[2]]$loss_reduction
) %>%
  set_engine("lightgbm", num.threads = 8) %>%
  set_mode("classification")

# GBT base last fit and metrics
gbt_base_final <-
  run_last_fit(
    "GBT",
    gbt_spec_blank,
    gbt_spec_tuned,
    recipe = base_recipe,
    return_fit = TRUE
  )

gbt_base_final[[1]]
```


## ROC

```{r ff roc}
mod_labels <- c("GBT", "LR")

make_final_roc <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

rocs_final <- map2(list(gbt_base_final[[2]], lr_final[[2]]), mod_labels, ~make_final_roc(.x, .y))

library(patchwork)

(rocs_final[[1]] | rocs_final[[2]])
```

## PR

```{r}
make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

prs_final <- map2(list(gbt_base_final[[2]], lr_final[[2]]), mod_labels, ~make_final_pr(.x, .y))

(prs_final[[1]] | prs_final[[2]])
```


## Confusion Matrix

Probability cut-off = 0.5

```{r ff cm}
make_cm <- function(fit_obj, title="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + ggtitle(title)
}

conf_mat_final <- map2(list(gbt_base_final[[2]], lr_final[[2]]), mod_labels, ~make_cm(.x, .y))

(conf_mat_final[[1]] | conf_mat_final[[2]])
```



## Performance

```{r ff metrics}
get_metrics <- function(fit_obj, label="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  summary() %>% 
  select(-.estimator) %>% 
  mutate(model = label)
}

bind_rows(get_metrics(gbt_base_final[[2]], "GBT"),
          get_metrics(lr_final[[2]], "LLR")) %>%
  pivot_wider(names_from = model, values_from = .estimate)
```

## Probability cutoff adjustment by maximum j-index

```{r ff prob adj func}
maxj <- function(threshold_df) {
  # find max j-index
  max_j_index_threshold <- threshold_df %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
  
  # max_j_index_threshold may be a vector, use its last element
  if (length(max_j_index_threshold) > 1) {
    max_j_index_threshold <-
      max_j_index_threshold[length(max_j_index_threshold)]
  }
  
  return(max_j_index_threshold)
}

get_threshold <- function(fit_obj){
    # collect sens, spec, j-index at various cut-offs
  threshold_data <-
    fit_obj %>%
    collect_predictions() %>%
    threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" |
                               .metric == "spec" ~ "1",
                             TRUE ~ "2"))
  return(threshold_data)
}

senspec_plot <- function(fit_obj, title="") {
  # collect sens, spec, j-index at various cut-offs
  threshold_data <- get_threshold(fit_obj)
  
  # find max j-index
  max_j_index_threshold <- maxj(threshold_data)
  
  # plot metrics v cut-offs
  sens_spec_j_plot <-
    ggplot(threshold_data,
           aes(
             x = .threshold,
             y = .estimate,
             color = .metric,
             alpha = group
           )) +
    geom_line(size = 1) +
    #theme_minimal() +
    #scale_color_viridis_d(end = 0.9) +
    scale_color_brewer(palette = "Set1", guide = guide_legend(title = NULL) ) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(
      xintercept = max_j_index_threshold,
      alpha = .8,
      color = "grey30",
      linetype = "longdash"
    ) +
    labs(x = "Probability",
         y = "Metric Estimate",
         title = title)
  return(sens_spec_j_plot)
}
```

### GTB

```{r}
senspec_plot(gbt_base_final[[2]], "GBT")
```

### LLR

```{r}
Fig_9C <- senspec_plot(lr_final[[2]], "") + theme(legend.position = "left")
Fig_9C
```

### j-index and the probability threshold

```{r}
lr_final[[2]] %>%
  get_threshold() %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  slice_tail()
```

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)


```{r opt cm}
optimize_prediction <- function(fit.obj){
  j_index <- maxj(get_threshold(fit.obj))
  
  optimized <- fit.obj %>%
    collect_predictions() %>%
    mutate(.pred = make_two_class_pred(
      estimate = .pred_HR,
      levels = levels(resistance),
      threshold = j_index
    )) %>%
    select(resistance, contains(".pred"))
  
  return(optimized)
}

cm_optimized_xgb <- optimize_prediction(gbt_base_final[[2]]) %>% 
  conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle("GBT")

cm_optimized_lr <- optimize_prediction(lr_final[[2]]) %>% 
  conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle("LR")

(cm_optimized_xgb | cm_optimized_lr)
```

### Optimized performance

```{r opt perf}
get_metrics2 <- function(fit.obj, label){
  optimize_prediction(fit.obj) %>% 
    conf_mat(truth = resistance, estimate = .pred) %>% 
    summary() %>% 
    mutate(model = label)
}

bind_rows(
  get_metrics2(gbt_base_final[[2]], "GBT"),
  get_metrics2(lr_final[[2]], "LLR")
) %>% 
  pivot_wider(names_from = model, values_from = .estimate) %>% 
  select(-.estimator)
```

# Post-modelling analysis

## Features used by the best model (LR)

### How many predictors were removed

Overall, we have `r ncol(df_train) - 2` predictors.

#### Predictors left after the first `step_nzv()`

```{r n nzv}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  juice() %>% 
  ncol()
```


#### Predictors left after the first and the second `step_nzv()`

```{r n corr}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>% prep() %>% juice() %>% ncol()
```

### Features importance

Non-zero LLR coefficients:

```{r lr coeff}
lr_coeff_df <- lr_final[[2]] %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate)) %>% 
  mutate(odds.ratio = 1/exp(estimate)) %>% 
  select(-penalty)

lr_coeff_df
```

Just 34 features plus Intercept are used by the model.

On the `odds.ratio` column: 

- units in which predictors are measured have been transformed

- apparently, nonHR is outcome 1 here and HR is outcome 0, because increase in `n.beta.lac` decreases chances of outcome 1.


### Predictors decreasing the risk of HR:

```{r feat pos}
lr_coeff_df %>% 
  filter(odds.ratio < 1.0) %>% 
  select(term)
```

### Predictors increasing the risk of HR

```{r feat neg}
lr_coeff_df %>% 
  filter(odds.ratio >= 1.0) %>% 
  select(term)
```

### Plot

```{r, fig.width=8, fig.height=6}
Fig_9D <- lr_coeff_df %>% 
  mutate(effect = if_else(estimate < 0, "increase HR prob.", "decrease HR prob.")) %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(reorder(term, odds.ratio, decreasing = F), odds.ratio)) +
  geom_col(aes(fill=effect)) +
  scale_fill_brewer(palette="Paired", guide = guide_legend(title = NULL)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.9, hjust=0.9, size = 8),legend.position = "bottom") +
  xlab("") + coord_flip()
Fig_9D 
```

```{r,fig.width=7, fig.height=10}
Fig_9CD <- ggarrange(Fig_9C, Fig_9D,
                    labels = c("C", "D"),
                    ncol = 1, nrow = 2)
Fig_9CD
```



# Minimal set of predictors chosen by Boruta

Four predictors were chosen by Boruta algorithms as being significantly more important than the rest

Here we use the same pre-processing as before: yj_recipe with tuned corr. threshold from the best `xgb_yjbor_bres`

```{r min feat set}
set.seed(42)

df_train_bor <-
  base_recipe %>% prep() %>% juice() %>% select(-strain)

bor_pred_selection <- Boruta::Boruta(resistance ~., data = df_train_bor, maxRuns=300)

bor_pred_selection
```

```{r, fig.width=10}
plot(bor_pred_selection)
```

```{r min feat boxplot}
minimal_pred <- bor_pred_selection$ImpHistory %>%
  as_tibble() %>%
  pivot_longer(cols = 1:104,
               names_to = "pred",
               values_to = "value") %>%
  group_by(pred) %>%
  mutate(median = median(value)) 

minimal_pred %>% 
  filter(median > 9) %>%
  ggplot(aes(reorder(pred, median), value)) +
  geom_boxplot(notch = T) +
  geom_violin(alpha = 0.1) +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 0.55,
    hjust = 0.5
  )) +
  xlab("") +
  ylab("Z-score")
```

Top 5 predictors: 

```{r}
minimal_pred_best <- minimal_pred %>% filter(pred == "IS21" |
                                             pred == "ISAS1" |
                                             pred == "TEM.beta.lactamase" |
                                             pred == "n.TEM.1" |
                                             pred == "IS6") %>% select(-median)

kruskal.test(minimal_pred_best$value, minimal_pred_best$pred)
```

## A BT model with the top 5 predictors

```{r}
minimal_pred_best %>% distinct(pred, .keep_all = F) 
```

```{r min recipe}
top_predictors <- 
  minimal_pred_best %>% 
  #filter(median > 10) %>% 
  distinct(pred, .keep_all = F) %>% 
  pull(pred)

data_strain_min <- data_strain %>% 
  select(resistance, all_of(top_predictors))

set.seed(124)

data_split_min <- initial_split(data_strain_min, prop = 0.8, strata = resistance)

df_train_min <- training(data_split_min)
df_test_min <- testing(data_split_min)

cv_folds_min <- vfold_cv(df_train_min, strata = "resistance", v = 10, repeats = 10)

base_min_pred_recipe <- recipe(resistance ~ ., data = df_train_min) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

lgb_spec <- boost_tree(
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune(),
  mode = "classification"
) %>% 
  set_engine(engine = "lightgbm", num.threads = 8) %>% 
  translate()

min_pred_wf <- workflow() %>% 
  add_model(lgb_spec) %>% 
  add_recipe(base_min_pred_recipe)

param_set <- extract_parameter_set_dials(min_pred_wf) %>%
  finalize(x = df_train_min %>% select(-resistance))
```


If you want to train this model again run this chunk:

```{r rf min3 train, eval=FALSE}
lgb_min_res <- min_pred_wf %>%
  tune_grid(
    param_info = param_set,
    grid = 20,
    resamples = cv_folds_min,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

saveRDS(lgb_min_res, paste0(models_path, "lgb_min5_res.rds"))
```

By default this code is used:

```{r rf min3 res}
lgb_min_res <- readRDS(paste0(models_path, "lgb_min5_res.rds"))

autoplot(lgb_min_res)
```


```{r}
show_best(lgb_min_res, metric= "roc_auc")
```

## ROCs

```{r gbt min roc}
autoplot(make_roc(lgb_min_res, "")) +
  ggtitle("GBT, min 5")
```

## Test set

### RF min 3

```{r rf min3 test}
lgb_best_top5 <- select_best(lgb_min_res, metric = "roc_auc")

lgb_min_tuned <- boost_tree(
  mtry = lgb_best_top5$mtry,
  trees = lgb_best_top5$trees,
  min_n = lgb_best_top5$min_n,
  tree_depth = lgb_best_top5$tree_depth,
  learn_rate = lgb_best_top5$learn_rate,
  loss_reduction = lgb_best_top5$loss_reduction,
  sample_size = lgb_best_top5$sample_size,
  stop_iter = lgb_best_top5$stop_iter,
  mode = "classification"
) %>% 
  set_engine(engine = "lightgbm", num.threads = 8) %>% 
  translate()

lgb_min_final <- run_last_fit(
  "LGB top 5",
  lgb_spec,
  lgb_min_tuned,
  recipe = base_min_pred_recipe,
  train_test_split = data_split_min,
  return_fit = TRUE
)

lgb_min_final[[1]]
```

#### Final ROC

```{r}
make_final_roc(lgb_min_final[[2]], "LGB top 5")
```

#### Final confusion matrix

```{r}
make_cm(lgb_min_final[[2]])
```

#### Final metrix

```{r}
get_metrics(lgb_min_final[[2]], "LGB top 5") %>% pivot_wider(names_from = model, values_from = .estimate)
```

#### Final sensitivity/specificity plot

```{r}
senspec_plot(lgb_min_final[[2]], "LGB top 5")
```

## DALEX

You can choose for instance false positives and look what influenced
their classification as HR

```{r, eval=FALSE}
false_positives <- 
  prediction_all_data %>% 
  filter(.pred_class == "HR", 
         .true_class == "nonHR")

false_positives
```

```{r, eval=FALSE}
library(DALEXtra)

explainer_final <- 
  explain_tidymodels(
    final_mod_fit, 
    data = data_strain_proc, 
    y = data_strain$resistance,
    label = "LR",
    verbose = FALSE
  )

data_strain_proc$strain <- data_strain$strain
data_strain_proc$resistance <- data_strain$resistance

obs_interest <- data_strain_proc %>% 
  filter(strain == "DA62984") %>% 
  select(-c(strain, resistance))


final_mod_breakdown <-
  predict_parts(explainer = explainer_final, new_observation = obs_interest)

final_mod_breakdown
```

Most of the predictors don't make any contributions at all.

Also, `contribution` is *the probability of nonHR* (because the last
line 'prediction' equals `1 - .prob_HR` )

```{r, eval=FALSE}
# look at nc.beta.lac.plasmid contribution
data_strain_proc %>% 
  mutate(label = if_else(strain == "DA62984", "DA62984", resistance)) %>% 
  ggplot(aes(label, n.TEM.1)) +
  geom_jitter(aes(color=resistance))
```

## Extracting predictions

Using the final model fit and adjusted probability cut-off

That final recipe:

```{r, eval=FALSE}
final_recipe <- last_wf %>% fit(df_train) %>% extract_recipe()

final_recipe
```

The final model fit

```{r, eval=FALSE}
final_mod_fit <- last_wf %>% fit(df_train) %>% extract_fit_parsnip()
```

```{r, eval=FALSE}
data_strain_proc <- final_recipe %>%
  bake(new_data = data_strain) %>% 
  select(-c(strain, resistance))

data_strain_proc
```

```{r, eval=FALSE}
# I tested this code on df_test and it gives the same confusion matrix as the code above in section 'Final fit' and 'Confusion matrix with prob=0.5'
# So it's the right way to apply predict() to the entire data set to identify false predictions

prediction_all_data <-
  predict(final_mod_fit, new_data = data_strain_proc, type = "prob") %>%
  select(.pred_HR) %>%
  mutate(
    .pred_class = if_else(.pred_HR >= max_j_index_threshold, "HR", "nonHR"),
    .true_class = data_strain$resistance,
    strain = data_strain$strain
  ) %>%
  rename(.prob_HR = .pred_HR)

prediction_all_data
```

## AMR types and false predictions

```{r, eval=FALSE}
# this is for Dan
amr_types_strain <- read_csv("data/amr_types_strain.csv")

prediction_all_data %>%  
  left_join(amr_types_strain, by="strain") %>% 
  write.csv(file="data/amr_types_prediction_prob.csv", row.names = F)
```

```{r, eval=FALSE}
# amr_type_pred <- read_csv("data/amr_types_prediction_prob.csv")

amr_type_pred %>% 
  filter(.pred_class != .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

```{r, eval=FALSE}
amr_type_pred %>% 
  filter(.pred_class == .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```
