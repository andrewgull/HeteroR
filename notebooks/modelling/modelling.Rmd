---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(tidyposterior) # for Bayesian ANOVA
#library(embed) # for UMAP
library(finetune) # for win-loss tuning
source("functions.R")

# path for models
models_path <- "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/"
```

# Read and process data

The data sets are the same as in EDA

```{r}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf")) %>% 
  filter(strain != "DA63310")

# TWO SCHEMES
hr_testing12 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr12)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr12 )

hr_testing13 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr13)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr13 )

# created during EDA
hr_testing <- read_csv("data/heteroresistance_testing_gr123.csv") 

# USE HR 1+2
data_strain <- data_strain %>% 
  left_join(hr_testing12, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \>4

```{r}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  #mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
#data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Basic Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe %>% prep() %>% juice()
```

Target class:

```{r}
main_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```

## PCA + ORQ-normalization

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

For PCA

Does ORQ-normalization `step_orderNorm()` really help?

If you look at EDA, you'll see that ORQ gives better distributions and
less PCs to explain the same amount of variance, than regular
`step_normalize()`. (Same is true for this recipe below)

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  update_role(resistance, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  #step_normalize(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), threshold = .9)
  

pca_recipe %>% prep() %>% juice()
```

## PCA Yeo-Johnson recipe

Borrowed from 'Tidy modeling with R': Chapter 13.2 'Evaluating the grid'

```{r}
pcayj_recipe <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors())

# if I choose, say, 3 PCs, will it compress all predictors into 3 PCs or just pick 3 best out of more PCs?
pcayj_recipe_test <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), num_comp = 3) %>% 
  step_normalize(all_numeric_predictors())

pcayj_recipe_test %>% 
  prep() %>% 
  juice() %>% 
  dim()
# indeed just 3 PCs in total
```


## NCORR Yeo-Johnson recipe

development of the approach above, but with decorrelation and no PCA

```{r}
yj_recipe <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(threshold = tune("corr_tune")) # The step will try to remove the minimum number of columns so that all the resulting absolute correlations are less than this value
```


## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No correlation + ORQ-norm recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r}
ncorq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No corr + Spline basis expansion

to incorporate non-linearity into the class boundary

```{r}
ns_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_ns(all_predictors(), deg_free = tune()) %>% 
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## Recursive Feature Elimination

fixed on local copy - th original package in the tibble creation has
 `name = c("top_p", "threshold"), `
 instead of 
 `name = c("top_p", "threshold", "cutoff"),`
 
 the git repository was cloned locally, corrected and the package installed following 
 https://kbroman.org/pkg_primer/pages/build.html
 
```{r}
if (!require(colino, quietly = TRUE))
  devtools::install_github("andrewgull/colino")

rfe_model <- rand_forest(mode = "classification") %>% 
  set_engine("ranger", num.threads = 8, importance = "impurity" )

rfe_rec <- recipe(resistance ~., data = df_train) %>% 
  step_dummy(all_nominal_predictors())%>% 
  step_select_vip(all_numeric_predictors(), outcome = "resistance", model = rfe_model, threshold = .9) 

rfe_rec %>% 
  prep() %>% 
  juice()
```

# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to une hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Penalized LR

By default all models here are LASSO regression models, unless specific type of penalization is stated.

## NCORR recipe

For LR I used space filling grid, cause Bayesian would cause errors
during resampling and the advantage it gives is marginal.

You can load an `.rds` file if it exists.

```{r, fig.width=6, fig.height=3}
lr_ncorr_res <- readRDS(paste0(models_path, "lr_ncorr_p20.rds"))

autoplot(lr_ncorr_res)
```

### Best models

```{r}
lr_ncorr_res %>% 
  show_best("roc_auc", n = 5) 
```


## NCORQ recipe

New ncorq with dummies after ORQ doesn't make any changes

```{r}
# tuned corr threshold
lr_ncorq_res <- readRDS(paste0(models_path, "lr_ncorq_p30.rds"))

lr_ncorq_res %>% autoplot()
```

### Best models

```{r}
lr_ncorq_res %>%
  show_best("roc_auc", n = 5)
```

## PCA + YJ recipe

Space-filling grid as well, number of components to be tuned

```{r}
lr_pcayj_res <- readRDS(paste0(models_path, "lr_pcayj_p20.rds"))

# there were many errors during tuning process
autoplot(lr_pcayj_res)
```

### Best models

```{r}
lr_pcayj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ recipe

```{r}
lr_ncoryj_res <- readRDS(paste0(models_path, "lr_ncoryj_p20.rds"))

autoplot(lr_ncoryj_res)
```

### Best models

```{r}
show_best(lr_ncoryj_res)
```

## Ridge NCORR + YJ recipe

```{r}
lr0_ncoryj_res <- readRDS(paste0(models_path, "lr0_ncoryj_p20.rds"))

autoplot(lr0_ncoryj_res)
```

### Best models

```{r}
show_best(lr0_ncoryj_res)
```

## Elastic net NCORR + YJ recipe

```{r}
lr_en_ncoryj_res <- readRDS(paste0(models_path, "lr-en_ncoryj_p20.rds"))

autoplot(lr_en_ncoryj_res)
```

### Best models

```{r}
show_best(lr_en_ncoryj_res)
```

## Compare preprocessors

### ROCs overlapped

```{r}
lr_rocs <- map2_dfr(list(lr_ncorr_res, lr_ncoryj_res, lr_pcayj_res, lr0_ncoryj_res, lr_en_ncoryj_res),
                            c("LR ncorr",  "LR ncorr+yj", "LR pca+yj", "LR ridge ncor+yj", "LE el.net"), 
                            function(x, y) make_roc(x, y))

lr_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

### Generate distributions

*post hoc* analysis of resampling results generated by models - via Bayesian ANOVA

```{r}
lr_comp <-
  res_comp_table(
    res_list = list(
      lr_ncorr_res,
      lr_ncoryj_res,
      lr_pcayj_res,
      lr0_ncoryj_res,
      lr_en_ncoryj_res
    ),
    mod_names = c("ncorr", "ncorr.yj", "pca.yj", "ridge", "el.net")
  )

lr_posterior <-
  perf_mod(
    lr_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(lr_posterior)
```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(lr_posterior, seed = 100) 

summary(preproc_diff)
```

Columns `upper` and `lower` represent credible interval for the mean difference between preprocessing's AUCs.

Positive mean value means that mean of norm > mean of orq etc.

If CI doesn't include 0, then the difference is significant.

In this case standard normalization is significantly better than the ordered quantile technique.

The ordered quantile normalization is significantly better than PCA recipe.

LASSO is easier to interpret, so we can choose it (ncorr + yj) over other similar models

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r}
mars_ncorr_res <- readRDS(paste0(models_path, "mars_ncorr_p20.rds"))

autoplot(mars_ncorr_res)
```

This plot suggests that greater number of model terms can give greater AUC and J-index

### Best models

```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

## NCORR + YJ recipe

```{r}
mars_ncoryj_res <- readRDS(paste0(models_path, "mars_ncoryj_p20.rds"))

autoplot(mars_ncoryj_res)
```

### Best models

```{r}
mars_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
mars_ncorq_res <- readRDS(paste0(models_path, "mars_ncorq_p30.rds"))

autoplot(mars_ncorq_res)
```

### Best models

```{r}
mars_ncorq_res %>% show_best("roc_auc", n = 5)
```

## MARS + RFE + YJ recipe

```{r}
mars_yjrfe_res <- readRDS(paste0(models_path, "mars_yjmainrfe_race_p30.rds"))

plot_race(mars_yjrfe_res)
```

### Best models

```{r}
show_best(mars_yjrfe_res)
```

## Bagged MARS + RFE + YJ recipe

```{r}
bag_mars_yjrfe_res <- readRDS(paste0(models_path, "bag_mars_yjmainrfe_race_p30.rds"))

plot_race(bag_mars_yjrfe_res)
```

### Best models

```{r}
show_best(bag_mars_yjrfe_res)
```

## Compare preprocessors

### ROCs overlapped

```{r}
mars_rocs <-
  map2_dfr(list(
    mars_ncorr_res,
    mars_ncoryj_res,
    mars_ncorq_res,
    mars_yjrfe_res,
    bag_mars_yjrfe_res
  ), c(
    "ncorr sfg",
    "ncorr.yj sfg",
    "ncorrq sfg",
    "rfe+yj race",
    "bag rfe.yj race"
  ),
  function(x, y)
    make_roc(x, y))

mars_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

## Comparison of the preprocessing steps

```{r}
mars_comp <-
  res_comp_table(
    res_list = list(
      mars_ncorr_res,
      mars_ncoryj_res,
      mars_ncorq_res,
      mars_yjrfe_res,
      bag_mars_yjrfe_res
    ),
    mod_names = c(
      "ncorr sfg",
      "ncorr.yj sfg",
      "ncorrq sfg",
      "main rfe+yj race",
      "bag rfe+yj race"
    )
  )

mars_posterior <-
  perf_mod(
    mars_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(mars_posterior)
```

```{r}
mars_preproc_diff <- contrast_models(mars_posterior, seed = 100) 

summary(mars_preproc_diff)
```

# Linear support vector machines (lSVM)

## NCORR recipe

```{r}
# this one was obtained via racing
lsvm_ncorr_res <- readRDS(paste0(models_path, "lsvm_ncorr_race_p30.rds"))

plot_race(lsvm_ncorr_res)
```


### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORYJ recipe

```{r}
# trained with win-loss
lsvm_ncoryj_res <- readRDS(paste0(models_path, "lsvm_ncoryj_race_p30.rds"))

plot_race(lsvm_ncoryj_res)
```

### Best models

```{r}
lsvm_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
# obtained via win-loss
lsvm_ncorq_res <- readRDS(paste0(models_path, "lsvm_ncorq_race_p30.rds"))

plot_race(lsvm_ncorq_res)
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```


## RFE + YJ recipe

```{r}
lsvm_yjmainrfe_race <- readRDS(paste0(models_path, "lsvm_yjmainrfe_race_p30.rds"))

plot_race(lsvm_yjmainrfe_race)
```

### Best models

```{r}
show_best(lsvm_yjmainrfe_race)
```

## RFE + YJ recipe Bayesian grid tuning

```{r}
lsvm_yjmainrfe_bayes <- readRDS(paste0(models_path, "lsvm_yjmainrfe_bayes_p30.rds"))

autoplot(lsvm_yjmainrfe_bayes)
```

### Best models

```{r}
show_best(lsvm_yjmainrfe_bayes)
```

## Comparison of the preprocessors

### ROCs

```{r}
lsvm_rocs <- map2_dfr(list(lsvm_ncorr_res, lsvm_ncoryj_res, lsvm_ncorq_res, lsvm_yjmainrfe_race, lsvm_yjmainrfe_bayes),
                            c("lSVM ncorr",  "lSVM ncorr+yj", "lSVM ncorr+orq", "lSVM rfe+yj race", "lSVM rfe+yj bayes"), 
                            function(x, y) make_roc(x, y))

lsvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

### Distributions

```{r}
lsvm_comp <-
  res_comp_table(
    res_list = list(
      lsvm_ncorr_res,
      lsvm_ncoryj_res,
      lsvm_ncorq_res,
      lsvm_yjmainrfe_race,
      lsvm_yjmainrfe_bayes
    ),
    mod_names = c("ncorr",  "ncorr+yj", "ncorr+orq", "rfe+yj race", "rfe+yj bayes")
  )

lsvm_posterior <-
  perf_mod(
    lsvm_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(lsvm_posterior)
```

### Difference estimates

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

# Polynomial support vector machines (pSVM)

## NCORR recipe

```{r}
psvm_ncorr_res <- readRDS(paste0(models_path, "psvm_ncorr_race_p30.rds"))

plot_race(psvm_ncorr_res)
```


### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
psvm_ncorq_res <- readRDS(paste0(models_path, "psvm_ncorq_race_p30.rds"))

plot_race(psvm_ncorq_res)
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ

```{r}
psvm_ncoryj_res <- readRDS(paste0(models_path, "psvm_ncoryj_race_p30.rds"))

plot_race(psvm_ncoryj_res)
```

### Best models

```{r}
show_best(psvm_ncoryj_res)
```

## PCA + YJ recipe

```{r}
psvm_pca_res <- readRDS(paste0(models_path, "psvm_pcayj_race_p30.rds"))

plot_race(psvm_pca_res)
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

## RFE + YJ recipe

```{r}
psvm_yjmainrfe_race <- readRDS(paste0(models_path, "psvm_yjmainrfe_race_p30.rds"))

plot_race(psvm_yjmainrfe_race)
```

### Best models

```{r}
show_best(psvm_yjmainrfe_race)
```

## RFE + YJ recipe Bayesian grid 

```{r}
psvm_yjmainrfe_bayes <- readRDS(paste0(models_path, "psvm_yjmainrfe_bayes_p30.rds"))

autoplot(psvm_yjmainrfe_bayes)
```

### Best models

```{r}
show_best(psvm_yjmainrfe_bayes)
```

## Comparison of the preprocessing steps

### ROCs overlapped

```{r, fig.height=3, fig.width=6}
psvm_rocs <-
  map2_dfr(list(
    psvm_ncorr_res,
    psvm_ncorq_res,
    psvm_ncoryj_res,
    psvm_pca_res,
    psvm_yjmainrfe_race,
    psvm_yjmainrfe_bayes
  ),
  c(
    "ncorr",
    "ncorq",
    "ncorr.yj",
    "pca.yj",
    "ref+yj race",
    "ref+yj bayes"
  ),
  function(x, y)
    make_roc(x, y))

psvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```


### Distributions

```{r}
psvm_comp <-
  res_comp_table(
    res_list = list(
      psvm_ncorr_res,
      psvm_ncorq_res,
      psvm_ncoryj_res,
      psvm_pca_res,
      psvm_yjmainrfe_race,
      psvm_yjmainrfe_bayes
    ),
    mod_names = c(
      "ncorr",
      "ncorq",
      "ncorr.yj",
      "pca.yj",
      "ref+yj race",
      "ref+yj bayes"
    )
  )

psvm_posterior <-
  perf_mod(
    psvm_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(psvm_posterior)
```

### Estimate the difference

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```


# Radial Basis Function SVM

## Main recipe

```{r}
rbf_main_res <- readRDS(paste0(models_path, "rbfsvm_main_race_p30.rds"))

plot_race(rbf_main_res)
```

### Best models

```{r}
show_best(rbf_main_res)
```

## NCORR recipe

```{r}
rbf_ncorr_res <- readRDS(paste0(models_path, "rbfsvm_ncorr_race_p30.rds"))

plot_race(rbf_ncorr_res)
```

### Best models

```{r}
show_best(rbf_ncorr_res)
```

## NCORQ recipe

```{r}
rbf_ncorq_res <- readRDS(paste0(models_path, "rbfsvm_ncorq_race_p30.rds"))

plot_race(rbf_ncorq_res)
```

### Best models

```{r}
show_best(rbf_ncorq_res)
```

## NCORR + YJ

```{r, fig.height=2, fig.width=5}
rbf_ncoryj_res <- readRDS(paste0(models_path, "rbfsvm_ncoryj_race_p30.rds"))

plot_race(rbf_ncoryj_res)
```

### Best models

```{r}
show_best(rbf_ncoryj_res)
```

## RFE + YJ recipe

```{r}
rbfsvm_yjmainrfe_race <- readRDS(paste0(models_path, "rbfsvm_yjmainrfe_race_p30.rds"))

plot_race(rbfsvm_yjmainrfe_race)
```

### Best models

```{r}
show_best(rbfsvm_yjmainrfe_race)
```

## RFE + YJ recipe Bayesian grid

```{r}
rbfsvm_yjmainrfe_bayes <- readRDS(paste0(models_path, "rbfsvm_yjmainrfe_bayes_p30.rds"))

autoplot(rbfsvm_yjmainrfe_bayes)
```

### Best models

```{r}
show_best(rbfsvm_yjmainrfe_bayes)
```

## Comparison of the preprocessors

### ROCs overlapped

```{r, fig.height=3, fig.width=6}
rbf_rocs <-
  map2_dfr(list(
    rbf_main_res,
    rbf_ncorr_res,
    rbf_ncorq_res,
    rbf_ncoryj_res,
    rbfsvm_yjmainrfe_race,
    rbfsvm_yjmainrfe_bayes
  ),
  c(
    "main",
    "ncorr",
    "ncorr.orq",
    "ncorr.yj",
    "rfe+yj race",
    "rfe+yj bayes"
  ),
  function(x, y)
    make_roc(x, y))

rbf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

### Distributions

```{r, fig.height=2, fig.width=5}
rbf_comp <-
  res_comp_table(
    res_list = list(
      rbf_main_res,
      rbf_ncorr_res,
      rbf_ncorq_res,
      rbf_ncoryj_res,
      rbfsvm_yjmainrfe_race,
      rbfsvm_yjmainrfe_bayes
    ),
    mod_names = c(
      "main",
      "ncorr",
      "ncorr.orq",
      "ncorr.yj",
      "rfe+yj race",
      "rfe+yj bayes"
    )
  )

rbf_posterior <-
  perf_mod(
    rbf_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(rbf_posterior)
```


### Difference estimates

```{r}
rbf_preproc_diff <- contrast_models(rbf_posterior, seed = 100) 

summary(rbf_preproc_diff)
```

# K-nearest neighbors (KNN)

## NCORR recipe 

```{r, fig.width=5, fig.height=2}
knn_ncorr_res <- readRDS(paste0(models_path, "knn_ncorr_race_p30.rds"))

plot_race(knn_ncorr_res)
```

### Best models

```{r}
knn_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r, fig.width = 5, fig.height=2}
knn_ncorq_res <- readRDS(paste0(models_path, "knn_ncorq_race_p30.rds"))

plot_race(knn_ncorq_res)
```

### Best models

```{r}
knn_ncorq_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ

```{r, fig.width=5, fig.height=2}
knn_ncoryj_res <- readRDS(paste0(models_path, "knn_ncoryj_race_p30.rds"))

plot_race(knn_ncoryj_res)
```

### Best models

```{r}
show_best(knn_ncoryj_res)
```

## RFE + YJ recipe

```{r}
knn_yjmainrfe_race <- readRDS(paste0(models_path, "knn_yjmainrfe_race_p30.rds"))

plot_race(knn_yjmainrfe_race)
```

### Best models

```{r}
show_best(knn_yjmainrfe_race)
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
knn_rocs <-
  map2_dfr(list(
    knn_ncorr_res,
    knn_ncorq_res,
    knn_ncoryj_res,
    knn_yjmainrfe_race
  ), c("ncorr", "ncorr.orq", "ncorr.yj", "rfe race.yj"),
  function(x, y)
    make_roc(x, y))

knn_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

### Posterior distributions

```{r, fig.width=5, fig.height=2}
knn_comp <-
  res_comp_table(
    res_list = list(
      knn_ncorr_res,
      knn_ncoryj_res,
      knn_ncorq_res,
      knn_yjmainrfe_race
    ),
    c("ncorr", "ncorr.yj", "ncorr.orq", "rfe race.yj")
  )

knn_posterior <-
  perf_mod(
    knn_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(knn_posterior)
```

### Difference estimates

```{r}
knn_preproc_diff <- contrast_models(knn_posterior, seed = 100) 

summary(knn_preproc_diff)
```

# Random Forest (RF)

## Main recipe

```{r}
rf_main_res <- readRDS(paste0(models_path, "rf_main_race_p30.rds"))

plot_race(rf_main_res)
```

### Best models

```{r}
rf_main_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR recipe

```{r}
rf_ncorr_res <- readRDS(paste0(models_path, "rf_ncorr_p20.rds"))

rf_ncorr_res %>% autoplot()
```

### Best models

```{r}
rf_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ

```{r}
rf_ncoryj_res <- readRDS(paste0(models_path, "rf_ncoryj_race_p30.rds"))

plot_race(rf_ncoryj_res)
```

### Best models

```{r}
show_best(rf_ncoryj_res)
```

## RFE + main recipe

```{r}
rf_mainrfe_race <- readRDS(paste0(models_path, "rf_mainrfe_race_p30.rds"))

plot_race(rf_mainrfe_race)
```

### Best models

```{r}
show_best(rf_mainrfe_race)
```

## Comparison of preprocessors

### ROCs

```{r}
rf_rocs <-
  map2_dfr(list(rf_main_res,
                rf_mainrfe_race,
                rf_ncorr_res,
                rf_ncoryj_res), c("main", "main.rfe", "ncorr", "ncorr.yj"),
           function(x, y)
             make_roc(x, y))

rf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```


### Distributions

```{r}
rf_comp <-
  res_comp_table(
    res_list = list(rf_main_res, rf_mainrfe_race, rf_ncorr_res, rf_ncoryj_res),
    mod_names = c("main", "rfe.race", "ncorr", "ncorr.yj")
  )

rf_posterior <-
  perf_mod(
    rf_comp,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rf_posterior)
```

### Difference estimates

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

# Boosted Trees (BT)

BT are sensitive to hyper-parameters that's why, Bayesian grid search should be preferable. 

## Main recipe

```{r}
# win-loss search was used here
xgb_main_res <- readRDS(paste0(models_path, "xgb_main_race_p30.rds"))

plot_race(xgb_main_res)
```

### Best models

```{r}
xgb_main_res %>% 
  show_best(metric = "roc_auc")
```


## NCORR + Bayes search

Bayesian search

```{r}
xgb_ncorr_bres <- readRDS(paste0(models_path, "xgb_ncorr_bayes_p20.rds"))

xgb_ncorr_bres %>% 
  autoplot()
```

### Iterations

```{r}
xgb_ncorr_bres %>% 
  autoplot(type = "performance")
```


### Best models

```{r}
xgb_ncorr_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```


## NCORR + win-loss grid search

```{r}
xgb_ncorr_wl <- readRDS(paste0(models_path, "xgb_ncorr_race_p30.rds"))

plot_race(xgb_ncorr_wl)
```

### Best models

```{r}
xgb_ncorr_wl %>% 
  show_best(metric = "roc_auc")
```

## NCORR + YJ

```{r}
# win-loss search here
xgb_ncoryj_res <- readRDS(paste0(models_path, "xgb_ncoryj_race_p30.rds"))

plot_race(xgb_ncoryj_res)
```

### Best models

```{r}
show_best(xgb_ncoryj_res)
```

## RFE + main recipe

```{r}
xgb_mainrfe_race <- readRDS(paste0(models_path, "bt_mainrfe_race_p30.rds"))

plot_race(xgb_mainrfe_race)
```

### Best models

```{r}
show_best(xgb_mainrfe_race)
```


# RFE + Bayesian search

```{r}
xgb_mainrfe_bayes <- readRDS(paste0(models_path, "bt_mainrfe_bayes_p30.rds"))

autoplot(xgb_mainrfe_bayes)
```

### Best models

```{r}
show_best(xgb_mainrfe_bayes)
```

## Comparison of the preprocessors

### ROCs

```{r}
xgb_rocs <-
  map2_dfr(list(
    xgb_main_res,
    xgb_ncorr_bres,
    xgb_ncorr_wl,
    xgb_ncoryj_res,
    xgb_mainrfe_race,
    xgb_mainrfe_bayes
  ),
  c(
    "XGB main",
    "XGB ncorr b",
    "XGB ncorr wl",
    "XGB ncorr yj",
    "XGB rfe race",
    "XGB rfe bayes"
  ),
  function(x, y)
    make_roc(x, y))

xgb_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```


### Posterior distributions

```{r}
xgb_comp <-
  res_comp_table(
    res_list = list(
      xgb_main_res,
      xgb_ncorr_bres,
      xgb_ncorr_wl,
      xgb_ncoryj_res,
      xgb_mainrfe_race,
      xgb_mainrfe_bayes
    ),
    mod_names = c(
      "XGB main",
      "XGB ncorr b",
      "XGB ncorr wl",
      "XGB ncorr yj",
      "XGB rfe race",
      "XGB rfe bayes"
    )
  )

xgb_posterior <-
  perf_mod(
    xgb_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(xgb_posterior)
```

### Estimating the differences

```{r}
xgb_preproc_diff <- contrast_models(xgb_posterior, seed = 100) 

summary(xgb_preproc_diff)
```

# Multilayer perceptron (MLP NNET)

aka feed-forward neural network

## NCORQ recipe

Applying ORQ-normalization improves MLP (nnet) performance

```{r}
# win-loss here
mlp_ncorq <- readRDS(paste0(models_path, "mlp_nnet_ncorq_race_p30.rds"))

plot_race(mlp_ncorq)
```


### Best models

```{r}
mlp_ncorq %>% 
  show_best("roc_auc")
```


## PCA + YJ recipe

```{r}
mlp_pca <- readRDS(paste0(models_path, "mlp_nnet_pcayj_p20.rds"))

mlp_pca %>% autoplot()
```

### Best models

```{r}
mlp_pca %>% show_best("roc_auc")
```

## NCORR + YJ recipe

```{r}
mlp_ncoryj <- readRDS(paste0(models_path, "mlp_nnet_ncoryj_p20.rds"))

mlp_ncoryj %>% autoplot()
```

### Best ones

```{r}
mlp_ncoryj %>% 
  show_best("roc_auc")
```

## Keras + PCA + YJ

```{r}
mlpk_pca <- readRDS(paste0(models_path, "mlp_keras_pcayj_race_p30.rds"))

mlpk_pca %>% plot_race()
```

### Best models

```{r}
mlpk_pca %>% show_best("roc_auc")
```


## RFE + YJ recipe

```{r}
mlp_yjrfe_race <- readRDS(paste0(models_path, "mlpnnet_yjmainrfe_race_p30.rds"))

plot_race(mlp_yjrfe_race)
```

### Best models

```{r}
show_best(mlp_yjrfe_race)
```

## Bag MLP + RFE + YJ recipe

```{r}
mlp_bag_yjrfe_res <- readRDS(paste0(models_path, "bagmlp_yjmainrfe_race_p30.rds"))

plot_race(mlp_bag_yjrfe_res)
```

### Best models

```{r}
show_best(mlp_bag_yjrfe_res)
```

## Comparison of the preprocessors

### ROCs

```{r}
mlp_rocs <-
  map2_dfr(list(
    mlp_ncoryj,
    mlp_pca,
    mlp_ncorq,
    mlpk_pca,
    mlp_bag_yjrfe_res,
    mlp_yjrfe_race
  ),
  c(
    "MLP ncorr+yj",
    "MLP pca+yj",
    "MLP ncorq",
    "Keras pca+yj",
    "bagMLP rfe yj race",
    "nnetMLP rfe yj race"
  ),
  function(x, y)
    make_roc(x, y))

mlp_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")
```

### Posterior distributions

```{r}
mlp_comp <-
  res_comp_table(
    res_list = list(
      mlp_ncoryj,
      mlp_pca,
      mlp_ncorq,
      mlpk_pca,
      mlp_bag_yjrfe_res,
      mlp_yjrfe_race
    ),
    mod_names = c(
      "MLP ncorr+yj",
      "MLP pca+yj",
      "MLP ncorq",
      "MLP keras pca+yj",
      "bagMLP rfe+yj race",
      "MLP rfe+yj race"
    )
  )

mlp_posterior <-
  perf_mod(
    mlp_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(mlp_posterior)
```

### Difference estimates

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```


# Models stacking

For successful stacking, you need the same set of predictors in every
candidate model.

Models with NOCORR recipe were among the best

Models must be trained using the same set of features!

Each model in the same stack must have same resampling method!



## Make a BRES stack

```{r, eval=FALSE}
library(stacks)

# no corr set
ncorr_stack_bres <-
  stacks() %>% 
  add_candidates(rf_ncorr_bres) %>%
  add_candidates(xgb_ncorr_bres_long) %>% 
  add_candidates(nnet_ncorr_bres)

as_tibble(ncorr_stack_bres)
```

Evaluating the stack

```{r, eval=FALSE}
ncorr_stack_bres %>% 
  blend_predictions(penalty = seq(0.01, 0.4, 0.01)) %>% 
  autoplot()
```

Let's choose penalty

```{r, eval=FALSE}
ncorr_stack_model <- 
  ncorr_stack_bres %>% 
  blend_predictions(penalty = 0.03)
```

```{r, eval=FALSE}
autoplot(ncorr_stack_model, type = "weights")
```

```{r, eval=FALSE}
ncorr_stack_model <- 
  ncorr_stack_model %>% 
  fit_members()

ncorr_stack_model
```

```{r, eval=FALSE}
data_pred <- df_test %>% 
  bind_cols(predict(ncorr_stack_model, ., type = "class"))

npv(data_pred, truth = resistance, .pred_class)
```

```{r, eval=FALSE}
roc_curve(data = data_pred, truth = resistance, .pred_HR_nnet_ncorr_bres_1_18) %>% 
  autoplot()
```

```{r, eval=FALSE}
yardstick::kap(data_pred, truth = resistance, .pred_HR)
```

# Comparison of models

## List of winners

The best model + preprocessor combinations:

- LR: ncorr + yj

- MARS: sucks

- linSVM: ncorr + yj

- polySVM: ncorr + yj

- rbfSVM: ncorr *or* ncorr + yj

- KNN: ncorr + yj (high corr. threshold)

- RF: ncorr + yj (high corr. threshold)

- BT: ncorr(b) *or* ncorr + yj (high corr. threshold)

- MLP: ncorr + yj or PCA + yj (3 PCs high AUC!)

- NB: sucks

## ROCs

```{r, fig.width=8, fig.height=8}
# you will need these two objects later
best_resamples_list <- list(
    lr_ncoryj_res,
    lsvm_ncoryj_res,
    psvm_ncoryj_res,
    rbf_ncoryj_res,
    knn_ncoryj_res,
    rf_ncoryj_res,
    xgb_ncorr_bres,
    xgb_ncoryj_res,
    mlp_pca,
    mlp_ncoryj
  )
best_resamples_names <- c(
    "LR, no corr.+yj",
    "SVM lin, no corr.+yj",
    "SVM poly, no corr.+yj",
    "SVM rbf, no corr.+yj",
    "kNN, no corr.+yj",
    "RF, no corr.+yj",
    "XGB, no corr (b)",
    "XGB, no corr.+yj",
    "MLP, pca+yj",
    "MLP, no corr.+yj"
  )

# calculate rocs
roc_df <-
  map2_dfr(best_resamples_list,
  best_resamples_names,
  function(x, y)
    make_roc(x, y))

roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1")

```

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each resamples object

```{r, fig.width=8, fig.height=4}
mod_comparison <- res_comp_table(best_resamples_list,
                                 best_resamples_names)

mod_posterior <- perf_mod(mod_comparison, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 8)

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set1")
```

## Difference estimates

```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

So, LR, XGB and MLP insignificantly differ from each other

```{r, fig.width=8, fig.height=6}
# calculate rocs

roc3_df <- map2_dfr(list(lr_ncorr_res, xgb_main_bres, mlp_ncorq_bres),
                            c("LR", "XGB", "MLP"), 
                            function(x, y) make_roc(x, y))
roc3_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = -1, option = "H")+
  scale_color_brewer(palette = "Set1")

```

Based on ROC shape I would choose XGB for the final fit - the XGB model reaches sensitivity = 1.0 earlier than other and that's what I need (find all HRs)

# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Model and workflow

```{r}
# tuned recipe
best <- lr_ncoryj_res %>% 
  select_best("roc_auc") # higher corr. threshold - see the recipe first definition

yj_recipe_tuned <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(threshold = best$corr_tune)

# model spec
lr_mod_spec <- logistic_reg(
        penalty = tune(), 
        mixture = 1) %>% 
      set_engine("glmnet") %>% 
  set_mode("classification")

last_mod <-
  logistic_reg(
    penalty = best$penalty
  ) %>% 
  set_engine("glmnet", num.threads = 2) %>% 
  set_mode("classification") 

# workflow
lr_wf <- workflow() %>% 
  add_model(lr_mod_spec) %>% 
  add_recipe(yj_recipe_tuned) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lr_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

## Final model's parameters

```{r, eval=F}
final_fit %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate))
```

## ROC

```{r}
final_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

## Confusion Matrix

Probability cut-off = 0.5

```{r, fig.width=4, fig.height=4}
cm <- final_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

## Probability cutoff adjustment by maximum j-index

```{r, fig.width=8}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  final_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# max_j_index_threshold may be a vector, use its last element
if (length(max_j_index_threshold) > 1){
  max_j_index_threshold <- max_j_index_threshold[length(max_j_index_threshold)]
}

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: LR + YJ"
  )

sens_spec_j_plot
```

What performance will I get using this maximal j-index?

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)

*If MLP PCA:* you might want to use the j-index corresponding to the highest sensitivity

```{r, fig.width=3, fig.height=2}
pred_optimized <- final_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

autoplot(cm_optimized, type = "heatmap")
```

### Optimized performance

```{r}
summary(cm_optimized)
```


# Post-modelling analysis

## Features importance

### How many predictors were removed

```{r}
yj_data_proc <- prep(yj_recipe_tuned, retain = TRUE)

# NZV removed
yj_data_proc$steps[[1]]$removals
```

around 40 predictors were removed

Again, how many non-zero coefficients we have:

```{r}
final_fit %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate))
```

Just 20 (including intercept)

### Predictors importance

```{r, fig.height=6}
final_fit %>% 
  extract_fit_parsnip() %>% 
  vi_model(lambda = best$penalty) %>% 
  filter(Importance != 0) %>% 
  arrange(-abs(Importance))
```

```{r}
final_fit %>% 
  extract_fit_parsnip() %>%
  vip(num_features = 30, include_type = T)
```

This plot doesn't correspond to the table of coefficients above

## Shapley Additive Explanations

Lloyd Shapley (Nobel Prize 2012)

```{r, fig.height=8}
library(fastshap)

# final fit model
final_model_fit <- 
  last_wf %>%
  fit(df_train) %>% 
  pull_workflow_fit()

final_model_fit <- final_fit %>% 
  extract_fit_parsnip()

# Apply the preprocessing steps with prep and juice to the training data
processed_matrix <- prep(yj_recipe_tuned, df_train) %>% 
  juice() %>% 
  select(-c(resistance, strain)) %>% # remove character columns
  as.matrix()

# Compute shapley values 
shap_values <- fastshap::explain(final_model_fit, X = processed_matrix, exact = TRUE)

autoplot(shap_values)
```

### Dependence plot

```{r}
feat <- prep(yj_recipe_tuned, df_train) %>% 
  juice()

autoplot(shap, 
         type = "dependence", 
         feature = "min.AR.len.cen", 
         X = feat,
         smooth = TRUE, 
         color_by = "resistance")
```

### Contributions in each strain

```{r, fig.height=10}
library(patchwork)
p1 <- autoplot(shap, type = "contribution", row_num = 1) +
  ggtitle("DA62886")

p2 <- autoplot(shap, type = "contribution", row_num = 10) +
  ggtitle("DA63030")

p1+p2
```

### Force plot

```{r}
force_plot(object = shap[1,], 
           feature_values = X[1,], 
           display = "html", 
           link = "logit")
```


## Extracting predictions

Using the final model fit and adjusted probability cut-off

That final recipe:

```{r}
final_recipe <- last_wf %>% fit(df_train) %>% extract_recipe()

final_recipe
```

The final model fit

```{r}
final_mod_fit <- last_wf %>% fit(df_train) %>% extract_fit_parsnip()
```


```{r}
data_strain_proc <- final_recipe %>%
  bake(new_data = data_strain) %>% 
  select(-c(strain, resistance))

data_strain_proc
```

```{r}
# I tested this code on df_test and it gives the same confusion matrix as the code above in section 'Final fit' and 'Confusion matrix with prob=0.5'
# So it's the right way to apply predict() to the entire data set to identify false predictions

prediction_all_data <-
  predict(final_mod_fit, new_data = data_strain_proc, type = "prob") %>%
  select(.pred_HR) %>%
  mutate(
    .pred_class = if_else(.pred_HR >= max_j_index_threshold, "HR", "nonHR"),
    .true_class = data_strain$resistance,
    strain = data_strain$strain
  ) %>%
  rename(.prob_HR = .pred_HR)

prediction_all_data
```

## DALEX

You can choose for instance false positives and look what influenced their classification as HR

```{r}
false_positives <- 
  prediction_all_data %>% 
  filter(.pred_class == "HR", 
         .true_class == "nonHR")

false_positives
```

```{r}
library(DALEXtra)

explainer_final <- 
  explain_tidymodels(
    final_mod_fit, 
    data = data_strain_proc, 
    y = data_strain$resistance,
    label = "LR",
    verbose = FALSE
  )

data_strain_proc$strain <- data_strain$strain
data_strain_proc$resistance <- data_strain$resistance

obs_interest <- data_strain_proc %>% 
  filter(strain == "DA62984") %>% 
  select(-c(strain, resistance))


final_mod_breakdown <-
  predict_parts(explainer = explainer_final, new_observation = obs_interest)

final_mod_breakdown
```

Most of the predictors don't make any contributions at all.

Also, `contribution` is *the probability of nonHR* (because the last line 'prediction' equals `1 - .prob_HR` )

```{r}
# look at nc.beta.lac.plasmid contribution
data_strain_proc %>% 
  mutate(label = if_else(strain == "DA62984", "DA62984", resistance)) %>% 
  ggplot(aes(label, n.TEM.1)) +
  geom_jitter(aes(color=resistance))
```


## AMR types and false predictions

```{r, eval=FALSE}
# this is for Dan
amr_types_strain <- read_csv("data/amr_types_strain.csv")

prediction_all_data %>%  
  left_join(amr_types_strain, by="strain") %>% 
  write.csv(file="data/amr_types_prediction_prob.csv", row.names = F)
```

```{r}
# amr_type_pred <- read_csv("data/amr_types_prediction_prob.csv")

amr_type_pred %>% 
  filter(.pred_class != .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

```{r}
amr_type_pred %>% 
  filter(.pred_class == .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

