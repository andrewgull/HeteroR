---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(tidyposterior) # for Bayesian ANOVA
#library(embed) # for UMAP
library(finetune) # for win-loss tuning
source("functions.R")

# path for models
models_path <- "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/"
```

# Read and process data

The data sets are the same as in EDA

```{r read data}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf")) %>% 
  filter(strain != "DA63310")

# TWO SCHEMES
hr_testing12 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr12)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr12 )

hr_testing13 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr13)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr13 )

# created during EDA
hr_testing <- read_csv("data/heteroresistance_testing_gr123.csv") 

# USE HR 1+2
data_strain <- data_strain %>% 
  left_join(hr_testing12, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \>4

```{r init processing}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  #mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
#data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r data split}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Basic Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r main recipe}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe %>% prep() %>% juice()
```

Target class:

```{r}
main_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```

## ORQ-transform + PCA

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

For PCA

Does ORQ-normalization `step_orderNorm()` really help?

If you look at EDA, you'll see that ORQ gives better distributions and
less PCs to explain the same amount of variance, than regular
`step_normalize()`. (Same is true for this recipe below)

```{r pca recipe}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  update_role(resistance, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  #step_normalize(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), threshold = .9)
  

pca_recipe %>% prep() %>% juice()
```

## Yeo-Johnson transorm + PCA

Borrowed from 'Tidy modeling with R': Chapter 13.2 'Evaluating the grid'

```{r pca+yj recipe}
pcayj_recipe <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors())

# if I choose, say, 3 PCs, will it compress all predictors into 3 PCs or just pick 3 best out of more PCs?
pcayj_recipe_test <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), num_comp = 3) %>% 
  step_normalize(all_numeric_predictors())

pcayj_recipe_test %>% 
  prep() %>% 
  juice() %>% 
  dim()
# indeed just 3 PCs in total
```

## No correlation + Yeo-Johnson transform

development of the approach above, but with decorrelation and no PCA

```{r yj recipe}
yj_recipe <- 
  recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune")) # The step will try to remove the minimum number of columns so that all the resulting absolute correlations are less than this value
```

## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r norm recipe}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_predictors(), threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No correlation + ORQ-transform recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r orq recipe}
ncorq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_predictors(), threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No correlation + Spline basis expansion

to incorporate non-linearity into the class boundary

```{r ns recipe}
ns_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_ns(all_predictors(), deg_free = tune()) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## Recursive Feature Elimination

fixed on local copy - th original package in the tibble creation has
`name = c("top_p", "threshold"),` instead of
`name = c("top_p", "threshold", "cutoff"),`

the git repository was cloned locally, corrected and the package
installed following <https://kbroman.org/pkg_primer/pages/build.html>

```{r rfe recipe}
if (!require(colino, quietly = TRUE))
  devtools::install_github("andrewgull/colino")

rfe_model <- rand_forest(mode = "classification") %>% 
  set_engine("ranger", num.threads = 8, importance = "impurity" )

rfe_rec <- recipe(resistance ~., data = df_train) %>% 
  step_dummy(all_nominal_predictors())%>% 
  step_select_vip(all_numeric_predictors(), outcome = "resistance", model = rfe_model, threshold = .9) 

rfe_rec %>% 
  prep() %>% 
  juice()
```

## Boruta features selection

to use with RF and BT algorithms

```{r boruta+main recipe}
# step_select_boruta() from colino
main_boruta_recipe <- main_recipe %>% 
  step_select_boruta(all_predictors(), outcome = "resistance", options = list(maxRuns = 500))

main_boruta_recipe %>% prep() %>% juice() %>% ncol()
```

## Boruta + YJ transfor + NOCORR

```{r boruta+yj recipe}
yj_boruta_recipe <- yj_recipe %>% 
  step_select_boruta(all_predictors(), outcome = "resistance")
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r cv and metrics}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to tune hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Rule-based prediction

Let's take a rule 'if there is more than 4 beta-lactamases, then the strain is HR' (from EDA)

```{r base rule}
rule_pred <- 
  df_train %>% 
  mutate(pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred)
```

```{r}
conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```


# Penalized LR

By default all models here are LASSO regression models, unless specific
type of penalization is stated.


## MAIN recipe 

```{r lr main, fig.height=3, fig.width=6}
lr_main <- readRDS(paste0(models_path, "lr_main_space_p30.rds"))

autoplot(lr_main)
```

### Best models

```{r}
lr_main %>% 
  show_best("roc_auc", n = 5) 
```

## NCORR recipe

For LR I used space filling grid, cause Bayesian would cause errors
during resampling and the advantage it gives is marginal.

You can load an `.rds` file if it exists.

```{r lr norm, fig.height=3, fig.width=6}
lr_ncorr_res <- readRDS(paste0(models_path, "lr_ncorr_p20.rds"))

autoplot(lr_ncorr_res)
```

### Best models

```{r}
lr_ncorr_res %>% 
  show_best("roc_auc", n = 5) 
```

## NCORQ recipe

New ncorq with dummies after ORQ doesn't make any changes

```{r lr orq}
# tuned corr threshold
lr_ncorq_res <- readRDS(paste0(models_path, "lr_ncorq_p30.rds"))

lr_ncorq_res %>% autoplot()
```

### Best models

```{r}
lr_ncorq_res %>%
  show_best("roc_auc", n = 5)
```

## PCA + YJ recipe

Space-filling grid as well, number of components to be tuned

```{r lr pca}
lr_pcayj_res <- readRDS(paste0(models_path, "lr_pcayj_p20.rds"))

# there were many errors during tuning process
autoplot(lr_pcayj_res)
```

### Best models

```{r}
lr_pcayj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ recipe

```{r lr yj}
lr_ncoryj_res <- readRDS(paste0(models_path, "lr_ncoryj_p20_test.rds"))

autoplot(lr_ncoryj_res)
```

### Best models

```{r}
show_best(lr_ncoryj_res)
```

```{r}
show_best(lr_ncoryj_res, metric = "j_index")
```

The same model is the best

## Ridge NCORR + YJ recipe

```{r lrr yj}
lr0_ncoryj_res <- readRDS(paste0(models_path, "lr0_ncoryj_p20.rds"))

autoplot(lr0_ncoryj_res)
```

### Best models

```{r}
show_best(lr0_ncoryj_res)
```

## Elastic net NCORR + YJ recipe

```{r lre yj}
lr_en_ncoryj_res <- readRDS(paste0(models_path, "lr-en_ncoryj_p20.rds"))

autoplot(lr_en_ncoryj_res)
```

### Best models

```{r}
show_best(lr_en_ncoryj_res)
```

## Compare preprocessors

### ROCs overlapped

```{r}
lr_best_res <- list(
    lr_ncorr_res,
    lr_ncoryj_res,
    lr_pcayj_res,
    lr0_ncoryj_res,
    lr_en_ncoryj_res
  )

lr_best_names <-
  c(
    "lasso.ncorr",
    "lasso.ncorr.yj",
    "lasso.pca.yj",
    "ridge.ncorr.yj",
    "elnet.ncorr.yj"
  )
```


```{r lr compare roc}
lr_rocs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_roc(x, y))

lr_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1") +
  ggtitle("LR")
```

### PR-curves overlapped

```{r lr compare pr}
lr_prs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_pr(x, y))

lr_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR")
```

### Generate distributions

*post hoc* analysis of resampling results generated by models - via
Bayesian ANOVA

```{r}
lr_comp <-
  res_comp_table(
    res_list = list(
      lr_ncorr_res,
      lr_ncoryj_res,
      lr_pcayj_res,
      lr0_ncoryj_res,
      lr_en_ncoryj_res
    ),
    mod_names = c("ncorr", "ncorr.yj", "pca.yj", "ridge", "el.net")
  )

lr_posterior <-
  perf_mod(
    lr_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(lr_posterior)
```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(lr_posterior, seed = 100) 

summary(preproc_diff)
```

Columns `upper` and `lower` represent credible interval for the mean
difference between preprocessing's AUCs.

Positive mean value means that mean of norm \> mean of orq etc.

If CI doesn't include 0, then the difference is significant.

In this case standard normalization is significantly better than the
ordered quantile technique.

The ordered quantile normalization is significantly better than PCA
recipe.

LASSO is easier to interpret, so we can choose it (ncorr + yj) over
other similar models

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r mars norm}
mars_ncorr_res <- readRDS(paste0(models_path, "mars_ncorr_p20.rds"))

autoplot(mars_ncorr_res)
```

This plot suggests that greater number of model terms can give greater
AUC and J-index

### Best models

```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

## NCORR + YJ recipe

```{r}
mars_ncoryj_res <- readRDS(paste0(models_path, "mars_ncoryj_p20.rds"))

autoplot(mars_ncoryj_res)
```

### Best models

```{r}
mars_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
mars_ncorq_res <- readRDS(paste0(models_path, "mars_ncorq_p30.rds"))

autoplot(mars_ncorq_res)
```

### Best models

```{r}
mars_ncorq_res %>% show_best("roc_auc", n = 5)
```

## MARS + RFE + YJ recipe

```{r}
mars_yjrfe_res <- readRDS(paste0(models_path, "mars_yjrfe_res_p30.rds"))

plot_race(mars_yjrfe_res)
```

### Best models

```{r}
show_best(mars_yjrfe_res)
```

## Bagged MARS + RFE + YJ recipe

```{r}
bag_mars_yjrfe_res <- readRDS(paste0(models_path, "bag_mars_yjrfe_res_p30.rds"))

plot_race(bag_mars_yjrfe_res)
```

### Best models

```{r}
show_best(bag_mars_yjrfe_res)
```

## Compare preprocessors

### ROCs overlapped

```{r}
mars_rocs <-
  map2_dfr(list(
    mars_ncorr_res,
    mars_ncoryj_res,
    mars_ncorq_res,
    mars_yjrfe_res,
    bag_mars_yjrfe_res
  ), c(
    "ncorr",
    "ncorr.yj",
    "ncorr.orq",
    "rfe.yj",
    "bag.rfe.yj"
  ),
  function(x, y)
    make_roc(x, y))

mars_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = 1, option = "B")
  scale_color_brewer(palette="Set1") +
  ggtitle("MARS")
```

## Comparison of the preprocessing steps

```{r}
mars_comp <-
  res_comp_table(
    res_list = list(
      mars_ncorr_res,
      mars_ncoryj_res,
      mars_ncorq_res,
      mars_yjrfe_res,
      bag_mars_yjrfe_res
    ),
    mod_names = c(
      "ncorr",
      "ncorr.yj",
      "ncorr.orq",
      "rfe.yj",
      "bag.rfe.yj"
    )
  )

mars_posterior <-
  perf_mod(
    mars_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(mars_posterior)
```

```{r}
mars_preproc_diff <- contrast_models(mars_posterior, seed = 100) 

summary(mars_preproc_diff)
```

# Linear support vector machines (lSVM)

## NCORR recipe

```{r lsvm norm}
# this one was obtained via racing
lsvm_ncorr_res <- readRDS(paste0(models_path, "lsvm_ncorr_race_p30.rds"))

plot_race(lsvm_ncorr_res)
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### Predictors that remained

```{r}
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(all_predictors(), threshold =0.59836019) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% prep() %>% juice() %>% ncol()
```

## NCOR + YJ recipe

```{r}
# trained with win-loss
lsvm_ncoryj_res <- readRDS(paste0(models_path, "lsvm_ncoryj_race_p30_test.rds"))

plot_race(lsvm_ncoryj_res)
```

### Best models

```{r}
lsvm_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
# obtained via win-loss
lsvm_ncorq_res <- readRDS(paste0(models_path, "lsvm_ncorq_race_p30.rds"))

plot_race(lsvm_ncorq_res)
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```

## RFE + YJ recipe

```{r}
lsvm_yjrfe_res <- readRDS(paste0(models_path, "lsvm_yjrfe_res_p30.rds"))

plot_race(lsvm_yjrfe_res)
```

### Best models

```{r}
show_best(lsvm_yjrfe_res)
```

## RFE + YJ recipe Bayesian grid tuning

```{r}
lsvm_yjrfe_bres <- readRDS(paste0(models_path, "lsvm_yjrfe_bres_p30.rds"))

autoplot(lsvm_yjrfe_bres)
```

### Best models

```{r}
show_best(lsvm_yjrfe_bres)
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
lsvm_best_res <- list(
    lsvm_ncorr_res,
    lsvm_ncoryj_res,
    lsvm_ncorq_res,
    lsvm_yjrfe_res,
    lsvm_yjrfe_bres
  )
  lsvm_best_names <- c(
    "ncorr",
    "ncorr.yj",
    "ncorr.orq",
    "rfe.yj",
    "rfe.yj.bayes"
  )
```


```{r}
lsvm_rocs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_roc(x, y))

lsvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, ROC curves")
```

### PR-curves overlapped

```{r lsvm compare pr}
lsvm_prs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_pr(x, y))

lsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, PR curves")
```

### Distributions

```{r}
lsvm_comp <-
  res_comp_table(
    res_list = list(
      lsvm_ncorr_res,
      lsvm_ncoryj_res,
      lsvm_ncorq_res,
      lsvm_yjrfe_res,
      lsvm_yjrfe_bres
    ),
    mod_names = c("ncorr",  "ncorr.yj", "ncorr.orq", "rfe.yj.race", "rfe.yj.bayes")
  )

lsvm_posterior <-
  perf_mod(
    lsvm_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(lsvm_posterior)
```

### Difference estimates

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

# Polynomial support vector machines (pSVM)

## NCORR recipe

```{r psvm norm}
psvm_ncorr_res <- readRDS(paste0(models_path, "psvm_ncorr_race_p30.rds"))

plot_race(psvm_ncorr_res)
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
psvm_ncorq_res <- readRDS(paste0(models_path, "psvm_ncorq_race_p30.rds"))

plot_race(psvm_ncorq_res)
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ

```{r}
psvm_ncoryj_res <- readRDS(paste0(models_path, "psvm_ncoryj_race_p30_test.rds"))

plot_race(psvm_ncoryj_res)
```

### Best models

```{r}
show_best(psvm_ncoryj_res)
```

## PCA + YJ recipe

```{r}
psvm_pca_res <- readRDS(paste0(models_path, "psvm_pcayj_race_p30.rds"))

plot_race(psvm_pca_res)
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

## RFE + YJ recipe

```{r}
psvm_yjrfe_res <- readRDS(paste0(models_path, "psvm_yjrfe_res_p30.rds"))

plot_race(psvm_yjrfe_res)
```

### Best models

```{r}
show_best(psvm_yjrfe_res)
```

## RFE + YJ recipe Bayesian grid

```{r}
psvm_yjrfe_bres <- readRDS(paste0(models_path, "psvm_yjrfe_bres_p30.rds"))

autoplot(psvm_yjrfe_bres)
```

### Best models

```{r}
show_best(psvm_yjrfe_bres)
```

## Comparison of the preprocessing steps

### ROCs overlapped

```{r}
psvm_best_res <- list(
    psvm_ncorr_res,
    psvm_ncorq_res,
    psvm_ncoryj_res,
    psvm_pca_res,
    psvm_yjrfe_res,
    psvm_yjrfe_bres
  )

psvm_best_names <- c(
    "ncorr",
    "ncorr.orq",
    "ncorr.yj",
    "pca.yj",
    "rfr.yj.race",
    "rfe.yj.bayes"
  )
```


```{r}
psvm_rocs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_roc(x, y))

psvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, ROC curves")
```

### PR curves overlapped

```{r}
psvm_prs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_pr(x, y))

psvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, PR curves")
```


### Distributions

```{r}
psvm_comp <-
  res_comp_table(
    res_list = list(
      psvm_ncorr_res,
      psvm_ncorq_res,
      psvm_ncoryj_res,
      psvm_pca_res,
      psvm_yjrfe_res,
      psvm_yjrfe_bres
    ),
    mod_names = c(
      "ncorr",
      "ncorr.orq",
      "ncorr.yj",
      "pca.yj",
      "rfe.yj.race",
      "rfe.yj.bayes"
    )
  )

psvm_posterior <-
  perf_mod(
    psvm_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(psvm_posterior)
```

### Estimate the difference

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```

# Radial Basis Function SVM

## Main recipe

```{r rsvm main}
rbf_main_res <- readRDS(paste0(models_path, "rbfsvm_main_race_p30.rds"))

plot_race(rbf_main_res)
```

### Best models

```{r}
show_best(rbf_main_res)
```

## NCORR recipe

```{r}
rbf_ncorr_res <- readRDS(paste0(models_path, "rbfsvm_ncorr_race_p30.rds"))

plot_race(rbf_ncorr_res)
```

### Best models

```{r}
show_best(rbf_ncorr_res)
```

## NCORQ recipe

```{r}
rbf_ncorq_res <- readRDS(paste0(models_path, "rbfsvm_ncorq_race_p30.rds"))

plot_race(rbf_ncorq_res)
```

### Best models

```{r}
show_best(rbf_ncorq_res)
```

## NCORR + YJ

```{r}
rbf_ncoryj_res <- readRDS(paste0(models_path, "rbfsvm_ncoryj_race_p30.rds"))

plot_race(rbf_ncoryj_res)
```

### Best models

```{r}
show_best(rbf_ncoryj_res)
```

## RFE + YJ recipe

```{r}
rbfsvm_yjrfe_res <- readRDS(paste0(models_path, "rbfsvm_yjrfe_res_p30.rds"))

plot_race(rbfsvm_yjrfe_res)
```

### Best models

```{r}
show_best(rbfsvm_yjrfe_res)
```

## RFE + YJ recipe Bayesian grid

```{r}
rbfsvm_yjrfe_bres <- readRDS(paste0(models_path, "rbfsvm_yjrfe_bres_p30.rds"))

autoplot(rbfsvm_yjrfe_bres)
```

### Best models

```{r}
show_best(rbfsvm_yjrfe_bres)
```

## Comparison of the preprocessors

### ROC curves overlapped

```{r}
rsvm_best_res <- list(
    rbf_main_res,
    rbf_ncorr_res,
    rbf_ncorq_res,
    rbf_ncoryj_res,
    rbfsvm_yjrfe_res,
    rbfsvm_yjrfe_bres
  )

rsvm_best_names <- c(
    "main",
    "ncorr",
    "ncorr.orq",
    "ncorr.yj",
    "rfe.yj.race",
    "rfe.yj.bayes"
  )
```


```{r}
rbf_rocs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_roc(x, y))

rbf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, ROC curves")
```

### PR curves overlapped

```{r}
rsvm_prs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_pr(x, y))

rsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, PR curves")
```

### Distributions

```{r}
rbf_comp <-
  res_comp_table(
    res_list = rsvm_best_res,
    mod_names = rsvm_best_names
  )

rbf_posterior <-
  perf_mod(
    rbf_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(rbf_posterior)
```

### Difference estimates

```{r}
rbf_preproc_diff <- contrast_models(rbf_posterior, seed = 100) 

summary(rbf_preproc_diff)
```

# k-Nearest Neighbors (KNN)

## NCORR recipe

```{r knn norm}
knn_ncorr_res <- readRDS(paste0(models_path, "knn_ncorr_race_p30.rds"))

plot_race(knn_ncorr_res)
```

### Best models

```{r}
knn_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORQ recipe

```{r}
knn_ncorq_res <- readRDS(paste0(models_path, "knn_ncorq_race_p30.rds"))

plot_race(knn_ncorq_res)
```

### Best models

```{r}
knn_ncorq_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ

```{r}
knn_ncoryj_res <- readRDS(paste0(models_path, "knn_ncoryj_race_p30.rds"))

plot_race(knn_ncoryj_res)
```

### Best models

```{r}
show_best(knn_ncoryj_res)
```

## RFE + YJ recipe

```{r}
knn_yjrfe_res <- readRDS(paste0(models_path, "knn_yjrfe_res_p30.rds"))

plot_race(knn_yjrfe_res)
```

### Best models

```{r}
show_best(knn_yjrfe_res)
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
knn_best_res <- list(knn_ncorr_res,
                     knn_ncorq_res,
                     knn_ncoryj_res,
                     knn_yjrfe_res)
knn_best_names <- c("ncorr",
                    "ncorr.orq",
                    "ncorr.yj",
                    "rfe.yj")
```


```{r}
knn_rocs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_roc(x, y))

knn_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, ROC curves")
```

### PR curves overlapped

```{r}
knn_prs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_pr(x, y))

knn_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, PR curves")
```

### Posterior distributions

```{r}
knn_comp <-
  res_comp_table(
    res_list = list(
      knn_ncorr_res,
      knn_ncoryj_res,
      knn_ncorq_res,
      knn_yjrfe_res
    ),
    c("ncorr", "ncorr.yj", "ncorr.orq", "rfe.yj")
  )

knn_posterior <-
  perf_mod(
    knn_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(knn_posterior)
```

### Difference estimates

```{r}
knn_preproc_diff <- contrast_models(knn_posterior, seed = 100) 

summary(knn_preproc_diff)
```

# Random Forest (RF)

## Main recipe

```{r rf main}
rf_main_res <- readRDS(paste0(models_path, "rf_main_race_p30.rds"))

plot_race(rf_main_res)
```

### Best models

```{r}
rf_main_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR recipe

```{r}
rf_ncorr_res <- readRDS(paste0(models_path, "rf_ncorr_p20.rds"))

rf_ncorr_res %>% autoplot()
```

### Best models

```{r}
rf_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ

```{r}
rf_ncoryj_res <- readRDS(paste0(models_path, "rf_ncoryj_race_p30.rds"))

plot_race(rf_ncoryj_res)
```

### Best models

```{r}
show_best(rf_ncoryj_res)
```

## RFE + main recipe

```{r}
rf_mainrfe_res <- readRDS(paste0(models_path, "rf_mainrfe_res_p30.rds"))

plot_race(rf_mainrfe_res)
```

### Best models

```{r}
show_best(rf_mainrfe_res)
```

## Boruta feature selection

main recipe

```{r}
rf_mainbor_res <- readRDS(paste0(models_path, "rf_mainbor_p30.rds"))

autoplot(rf_mainbor_res)
```

```{r}
show_best(rf_mainbor_res)
```

yj transform and decorr

```{r}
rf_yjbor_res <- readRDS(paste0(models_path, "rf_yjbor_p30.rds"))

autoplot(rf_yjbor_res)
```

```{r}
show_best(rf_yjbor_res)
```


## Comparison of preprocessors

### ROCs overlapped

```{r}
rf_best_res <- list(rf_main_res,
                rf_mainrfe_res,
                rf_ncorr_res,
                rf_ncoryj_res,
                rf_mainbor_res,
                rf_yjbor_res)
  
rf_best_names <- c("main", 
             "main.rfe", 
             "ncorr", 
             "ncorr.yj",
             "main.boruta",
             "ncorr.yj.bor")
```


```{r}
rf_rocs <-
  map2_dfr(rf_best_res, 
           rf_best_names,
           function(x, y)
             make_roc(x, y))

rf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("RF")
```

### PR curves overlapped

```{r}
rf_prs <-
  map2_dfr(rf_best_res,
           rf_best_names,
           function(x, y)
             make_pr(x, y))

rf_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("RF, PR curves")
```

### Distributions

```{r}
rf_comp <-
  res_comp_table(
    res_list = rf_best_res,
    mod_names = rf_best_names
  )

rf_posterior <-
  perf_mod(
    rf_comp,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rf_posterior)
```

### Difference estimates

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

# Gradient Boosted Trees (GBT)

BT are sensitive to hyper-parameters that's why, Bayesian grid search
should be preferable.

## Main recipe + win-loss search

```{r xgb main}
# win-loss search was used here
xgb_main_wl <- readRDS(paste0(models_path, "xgb_main_race_p30.rds"))

plot_race(xgb_main_wl)
```

### Best models

```{r}
xgb_main_wl %>% 
  show_best(metric = "roc_auc")
```

## NCORR + Bayes search

Bayesian search

```{r}
xgb_ncorr_bres <- readRDS(paste0(models_path, "xgb_ncorr_bres_p30_test.rds"))

xgb_ncorr_bres %>% 
  autoplot()
```

### Best models

```{r}
xgb_ncorr_bres %>% 
  show_best(metric = "roc_auc", n = 5) 
```

>Shrinkage (learning rate) is a gradient boosting regularization procedure that helps modify the update rule, which is aided by a parameter known as the learning rate. The use of learning rates below 0.1 produces improvements that are significant in the generalization of a model.

## RFE + main recipe + Bayesian search

```{r}
xgb_mainrfe_bayes <- readRDS(paste0(models_path, "bt_mainrfe_bres_p30.rds"))

autoplot(xgb_mainrfe_bayes)
```

### Best models

```{r}
show_best(xgb_mainrfe_bayes)
```

### Predictors that remained

```{r}
rfe_model <- rand_forest(mode = "classification") %>%
  set_engine("ranger", num.threads = 8, importance = "impurity")

recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>%
  step_select_vip(
    all_numeric_predictors(),
    outcome = "resistance",
    model = rfe_model,
    threshold = .9
  ) %>% prep() %>% juice() %>% colnames()
```

## Main recipe + Boruta + Bayesian search

```{r}
xgb_main_bor_bres <- readRDS(paste0(models_path, "bt_mainbor_bres_p30.rds"))

autoplot(xgb_main_bor_bres)
```

```{r}
show_best(xgb_main_bor_bres)
```

## Boruta + YJ-transform + Bayesian search

```{r}
xgb_yjbor_bres <- readRDS(paste0(models_path, "xgb_yjbor_bres_p30.rds"))

autoplot(xgb_yjbor_bres)
```

```{r}
show_best(xgb_yjbor_bres)
```


## Comparison of the preprocessors

### ROCs

```{r}
xgb_best_res <- list(
    xgb_main_wl,
    xgb_ncorr_bres,
    xgb_mainrfe_bayes,
    xgb_main_bor_bres,
    xgb_yjbor_bres
  )

xgb_best_names <- c(
    "main",
    "ncorr.bayes",
    "main.rfe.bayes",
    "main.bor.bayes",
    "ncorr.yj.bor"
  )

xgb_rocs <-
  map2_dfr(xgb_best_res,
  xgb_best_names,
  function(x, y)
    make_roc(x, y))

xgb_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("XGB, ROC curves")
```

### PR curves overlapped

```{r}
xgb_prs <-
  map2_dfr(xgb_best_res,
           xgb_best_names,
           function(x, y)
             make_pr(x, y))

xgb_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("XGB, PR curves")
```

### Posterior distributions

I have excluded models tuned with win-loss approach, since their performance was significantly worse

```{r}
xgb_comp <-
  res_comp_table(
    res_list = xgb_best_res,
    mod_names = xgb_best_names
  )

xgb_posterior <-
  perf_mod(
    xgb_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(xgb_posterior)
```

### Estimating the differences

```{r}
xgb_preproc_diff <- contrast_models(xgb_posterior, seed = 100) 

summary(xgb_preproc_diff) 
```

# Multilayer perceptron (MLP NNET)

aka feed-forward neural network

## NCORQ recipe

Applying ORQ-normalization improves MLP (nnet) performance

```{r mlp orq}
# win-loss here
mlp_ncorq_wl <- readRDS(paste0(models_path, "mlp_nnet_ncorq_race_p30.rds"))

plot_race(mlp_ncorq_wl)
```

### Best models

```{r}
mlp_ncorq_wl %>% 
  show_best("roc_auc")
```

### Predictors that remained

```{r}
recipe(resistance ~ ., data = df_train) %>%
     update_role(strain, new_role = "ID") %>% 
     step_nzv(all_predictors()) %>%
     step_orderNorm(all_numeric_predictors()) %>%
     step_dummy(all_nominal_predictors()) %>%
     step_corr(all_predictors(), threshold = 0.5860753) %>%
     step_smote(resistance, over_ratio = 1, seed = 100) %>% prep() %>% juice() %>%  colnames()
```

## PCA + YJ recipe

```{r}
mlp_pca <- readRDS(paste0(models_path, "mlp_nnet_pcayj_p20.rds"))

mlp_pca %>% autoplot()
```

### Best models

```{r}
mlp_pca %>% show_best("roc_auc")
```

## NCORR + YJ recipe

```{r}
mlp_ncoryj_res <- readRDS(paste0(models_path, "mlp_ncoryj_p20_test.rds"))

mlp_ncoryj_res %>% autoplot()
```

### Best ones

```{r}
mlp_ncoryj_res %>% 
  show_best("roc_auc")
```

## Keras + PCA + YJ

```{r}
mlpk_pca <- readRDS(paste0(models_path, "mlp_keras_pcayj_race_p30.rds"))

mlpk_pca %>% plot_race()
```

### Best models

```{r}
mlpk_pca %>% show_best("roc_auc")
```

## RFE + YJ recipe

```{r}
mlp_yjrfe_wl <- readRDS(paste0(models_path, "mlp_yjrfe_res_p30.rds"))

plot_race(mlp_yjrfe_wl)
```

### Best models

```{r}
show_best(mlp_yjrfe_wl)
```

## Bag MLP + RFE + YJ recipe

```{r}
mlp_bag_yjrfe_wl <- readRDS(paste0(models_path, "bag_mlp_yjrfe_res_p30.rds"))

plot_race(mlp_bag_yjrfe_wl)
```

### Best models

```{r}
show_best(mlp_bag_yjrfe_wl)
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
mlp_best_res <- list(
    mlp_ncoryj_res,
    mlp_pca,
    mlp_ncorq_wl,
    mlp_bag_yjrfe_wl,
    mlp_yjrfe_wl
  )

mlp_best_names <- c(
    "ncorr.yj",
    "pca.yj",
    "ncorr.orq",
    "bag.rfe.yj.race",
    "rfe.yj.race"
  )

mlp_rocs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_roc(x, y))

mlp_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP/FFNN, ROC curves")
```

### PR curves overlapped

```{r}
mlp_prs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_pr(x, y))

mlp_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP/FFNN, PR curves")
```

### Posterior distributions

```{r}
mlp_comp <-
  res_comp_table(
    res_list = mlp_best_res,
    mod_names = mlp_best_names
  )

mlp_posterior <-
  perf_mod(
    mlp_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 2
  )

autoplot(mlp_posterior)
```

### Difference estimates

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```

# Models stacking

For successful stacking, you need the same set of predictors in every
candidate model.

Models with NOCORR recipe were among the best

Models must be trained using the same set of features!

Each model in the same stack must have same resampling method!

## Make a BRES stack

```{r stacking, eval=FALSE}
library(stacks)

# no corr set
ncorr_stack_bres <-
  stacks() %>% 
  add_candidates(rf_ncorr_bres) %>%
  add_candidates(xgb_ncorr_bres_long) %>% 
  add_candidates(nnet_ncorr_bres)

as_tibble(ncorr_stack_bres)
```

Evaluating the stack

```{r, eval=FALSE}
ncorr_stack_bres %>% 
  blend_predictions(penalty = seq(0.01, 0.4, 0.01)) %>% 
  autoplot()
```

Let's choose penalty

```{r, eval=FALSE}
ncorr_stack_model <- 
  ncorr_stack_bres %>% 
  blend_predictions(penalty = 0.03)
```

```{r, eval=FALSE}
autoplot(ncorr_stack_model, type = "weights")
```

```{r, eval=FALSE}
ncorr_stack_model <- 
  ncorr_stack_model %>% 
  fit_members()

ncorr_stack_model
```

```{r, eval=FALSE}
data_pred <- df_test %>% 
  bind_cols(predict(ncorr_stack_model, ., type = "class"))

npv(data_pred, truth = resistance, .pred_class)
```

```{r, eval=FALSE}
roc_curve(data = data_pred, truth = resistance, .pred_HR_nnet_ncorr_bres_1_18) %>% 
  autoplot()
```

```{r, eval=FALSE}
yardstick::kap(data_pred, truth = resistance, .pred_HR)
```

# Comparison of models

## List of winners

The best model + preprocessor combinations:

-   LRL: ncorr+yj

-   MARS: bag+rfe+yj

-   lSVM: any ncorr; rfe are probably less overfitted

-   pSVM: any ncorr; rfe are probably less overfitted

-   rbfSVM: main (likely is overfitted); rfe bayes or any ncorr

-   kNN: rfe yj

-   RF: main or ncorr, but rfe is not overfitted

-   XGB: ncorr bayes or boruta + yj; the rest are very close to each other

-   MLP: ncorr yj and orq; the rest are very close to each other

## ROCs

```{r comparison1}
# you will need these two objects later
best_resamples_list <- list(
    lr_ncoryj_res,
    lr_ncorr_res,
    lsvm_ncoryj_res,
    psvm_ncoryj_res,
    rbf_ncoryj_res,
    knn_yjrfe_res,
    rf_ncoryj_res,
    xgb_ncorr_bres,
    xgb_yjbor_bres,
    mlp_pca,
    mlp_bag_yjrfe_wl
  )
best_resamples_names <- c(
    "LR yj",
    "LR norm",
    "SVM lin",
    "SVM poly",
    "SVM rbf",
    "kNN",
    "RF",
    "XGB, ncorr",
    "XGB, yj+boruta",
    "MLP, pca.yj",
    "MLP, bag.ncorr.yj"
  )

# calculate rocs
roc_df <-
  map2_dfr(best_resamples_list,
  best_resamples_names,
  function(x, y)
    make_roc(x, y))

roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(direction = 1, option = "B") +
  #scale_color_brewer(palette="Set1") +
  ggtitle("All the best models")

```

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each
resamples object

```{r}
mod_comparison <- res_comp_table(best_resamples_list,
                                 best_resamples_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set3") +
  theme_dark()
```

## Difference estimates

```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

Insignificant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper > 0)
```

LR is insignificantly better than XGBs and SVMs

## Top models

```{r comparison2}
# calculate rocs
top_models <- list(
    lr_ncoryj_res,
    psvm_ncoryj_res,
    lsvm_ncoryj_res,
    mlp_bag_yjrfe_wl,
    xgb_ncorr_bres,
    xgb_yjbor_bres)

top_names <- c("LR", "pSVM", "lSVM", "MLP", "XGB ncorr", "XGB yj boruta")

roc3_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_roc(x, y))
roc3_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("ROC curves, top models")

```

Based on ROC shape I would choose XGB for the final fit - the XGB model
reaches sensitivity = 1.0 earlier than other and that's what I need
(find all HRs)

```{r}
pr3_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_pr(x, y))
pr3_df %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0,1)) + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("PR curves, top models")
```



```{r}
mod_comparison <- res_comp_table(top_models,
                                 top_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set3")
```


```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```


# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Functions

```{r ff func}
# function to run last fit process
run_last_fit <-
  function(model_name,
           blank_spec,
           tuned_spec,
           recipe,
           train_test_split = data_split,
           return_fit = FALSE) {
    # last workflow
    last_wf <- workflow() %>%
      add_model(blank_spec) %>%
      add_recipe(recipe) %>%
      update_model(tuned_spec)
    
    set.seed(345)
    # final fit
    final_fit <- final_metrics <-
      last_wf %>%
      last_fit(train_test_split)
    # final metrics
    final_metrics <-
      final_fit %>%
      collect_metrics() %>%
      select(.metric, .estimate) %>%
      mutate(model = model_name)
    if (return_fit) {
      return(list(final_metrics, final_fit))
    } else {
      return(list(final_metrics, NULL))
    }
  }

# required by 4 models + 1 if it's ncorr
update_corr_treshold <- function(base_recipe = yj_recipe, step_no, corr_threshold) {
  # update step_corr threshold
  base_recipe$steps[[step_no]] <-
   recipes::update(base_recipe$steps[[step_no]], threshold = corr_threshold)
  
  return(base_recipe)
}

# required by 1
main_rfe_recipe <- main_recipe %>%
  step_select_vip(
    all_numeric_predictors(),
    outcome = "resistance",
    model = rfe_model,
    threshold = .9
  )

# best models
best_models <- map(top_models, ~select_best(., "roc_auc"))
```

## LR final

```{r lr ff}
# LR specs
lr_spec_blank <- logistic_reg(
  penalty = tune(),
  mixture = 1,
  engine = "glmnet",
  mode = "classification"
)

lr_spec_tuned <-
  logistic_reg(
    penalty = best_models[[1]]$penalty,
    mixture = 1,
    engine = "glmnet",
    mode = "classification"
  )
# LR last fit and metrics
yj_recipe_tuned_lr <-
  update_corr_treshold(step_no = 6, corr_threshold = best_models[[1]]$corr_tune)

lr_final <-
  run_last_fit("LR",
               lr_spec_blank,
               lr_spec_tuned,
               recipe = yj_recipe_tuned_lr,
               return_fit = TRUE)

lr_final[[1]]
```

## MLP final

```{r mlp ff}
# MLP specs
mlp_spec_blank <-
  mlp(hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet", num.threads = 4)

mlp_spec_tuned <-
  mlp(hidden_units = best_models[[4]]$hidden_units,
      penalty = best_models[[4]]$penalty,
      epochs = best_models[[4]]$epochs) %>%
  set_mode("classification") %>%
  set_engine("nnet", num.threads = 4)
# MLP last fit and metrics
yj_recipe_tuned_mlp <-
  update_corr_treshold(step_no = 6, corr_threshold = best_models[[4]]$corr_tune)

run_last_fit("MLP", mlp_spec_blank, mlp_spec_tuned, recipe = yj_recipe_tuned_mlp)
```


## pSVM final

```{r psvm ff}
# polySVM specs /number 2/
psvm_spec_blank <- svm_poly(
  cost = tune(),
  degree = tune(),
  scale_factor = tune(),
  margin = NULL
) %>%
  set_mode("classification") %>%
  set_engine("kernlab", num.threads = 4)

psvm_spec_tuned <- svm_poly(
  cost = best_models[[2]]$cost,
  degree = best_models[[2]]$degree,
  scale_factor = best_models[[2]]$scale_factor,
  margin = NULL
) %>%
  set_mode("classification") %>%
  set_engine("kernlab", num.threads = 4)
# polySVM last fit and metrics
yj_recipe_tuned_psvm <-
  update_corr_treshold(step_no = 6, corr_threshold = best_models[[2]]$corr_tune)

psvm_final <-
  run_last_fit(
    "polySVM",
    psvm_spec_blank,
    psvm_spec_tuned,
    recipe = yj_recipe_tuned_psvm,
    return_fit = TRUE
  )

psvm_final[[1]]
```


## lSVM final

```{r lsvm ff}
# linSVM specs
lsvm_spec_blank <- svm_linear(cost = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

lsvm_spec_tuned <- svm_linear(cost = best_models[[3]]$cost) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
# linSVM last fit and metrics
yj_recipe_tuned_lsvm <-
  update_corr_treshold(step_no = 6, corr_threshold = best_models[[3]]$corr_tune)

run_last_fit("linSVM", lsvm_spec_blank, lsvm_spec_tuned, recipe = yj_recipe_tuned_lsvm)
```


## XGB ncorr final

```{r xgb1 ff}
best_learn_rate_xgb <-
  xgb_ncorr_bres %>% 
  show_best('roc_auc', n = 10) %>% 
  filter(learn_rate < 0.1) %>% 
  select(
    mtry,
    min_n,
    tree_depth,
    learn_rate,
    loss_reduction,
    sample_size,
    stop_iter,
    corr_tune,
    .config
  )

# XGB ncorr specs /number 5/
xgb_spec_blank <- boost_tree(
  trees = 50,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune()
) %>%
  set_engine("xgboost", num.threads = 4) %>%
  set_mode("classification")

xgb_ncorr_spec_tuned <- boost_tree(
  trees = 50,
  mtry = best_models[[5]]$mtry,
  min_n = best_models[[5]]$min_n,
  tree_depth = best_models[[5]]$tree_depth,
  learn_rate = best_models[[5]]$learn_rate,
  loss_reduction = best_models[[5]]$loss_reduction,
  sample_size = best_models[[5]]$sample_size,
  stop_iter = best_models[[5]]$stop_iter
) %>%
  set_engine("xgboost", num.threads = 4) %>%
  set_mode("classification")

# xgb_ncorr_spec_tuned <- boost_tree(
#   trees = 50,
#   mtry = best_learn_rate_xgb$mtry,
#   min_n = best_learn_rate_xgb$min_n,
#   tree_depth = best_learn_rate_xgb$tree_depth,
#   learn_rate = best_learn_rate_xgb$learn_rate,
#   loss_reduction = best_learn_rate_xgb$loss_reduction,
#   sample_size = best_learn_rate_xgb$sample_size,
#   stop_iter = best_learn_rate_xgb$stop_iter
# ) %>%
#   set_engine("xgboost", num.threads = 4) %>%
#   set_mode("classification")

# XGB nocorr last fit and metrics
ncorr_recipe_tuned_xgb <-
  update_corr_treshold(
    base_recipe = ncorr_recipe,
    step_no = 4,
    corr_threshold = best_models[[5]]$corr_tune
  )

# ncorr_recipe_tuned_xgb <-
#   update_corr_treshold(
#     base_recipe = ncorr_recipe,
#     step_no = 4,
#     corr_threshold = best_learn_rate_xgb$corr_tune
#   )

xgb_ncorr_final <-
  run_last_fit(
    "XGB.ncorr",
    xgb_spec_blank,
    xgb_ncorr_spec_tuned,
    recipe = ncorr_recipe_tuned_xgb,
    return_fit = TRUE
  )

xgb_ncorr_final[[1]]
```


## XGB YJ Boruta final

```{r xgb2 ff}
# XGB yjbor specs /number 6/
xgb_yjbor_spec_tuned <- boost_tree(
  trees = 50,
  mtry = best_models[[6]]$mtry,
  min_n = best_models[[6]]$min_n,
  tree_depth = best_models[[6]]$tree_depth,
  learn_rate = best_models[[6]]$learn_rate,
  loss_reduction = best_models[[6]]$loss_reduction,
  sample_size = best_models[[6]]$sample_size,
  stop_iter = best_models[[6]]$stop_iter
) %>%
  set_engine("xgboost", num.threads = 4) %>%
  set_mode("classification")

ncoryj_bor_recipe_tuned_xgb <-
  update_corr_treshold(
    base_recipe = yj_boruta_recipe,
    step_no = 6,
    corr_threshold = best_models[[6]]$corr_tune
  )

# XGB yjbor last fit and metrics
xgb_yjbor_final <- run_last_fit(
  "XGB.yjbor",
  xgb_spec_blank,
  xgb_yjbor_spec_tuned,
  recipe = ncoryj_bor_recipe_tuned_xgb,
  return_fit = TRUE
)

xgb_yjbor_final[[1]]
```

## ROC

```{r ff roc}
mod_labels <- c("XGB ncorr", "LLR")

make_final_roc <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

rocs_final <- map2(list(xgb_yjbor_final[[2]], lr_final[[2]]), mod_labels, ~make_final_roc(.x, .y))

library(patchwork)

(rocs_final[[1]] | rocs_final[[2]])
```

## PR

```{r}
make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

prs_final <- map2(list(xgb_ncorr_final[[2]], lr_final[[2]]), mod_labels, ~make_final_pr(.x, .y))

(prs_final[[1]] | prs_final[[2]])
```


## Confusion Matrix

Probability cut-off = 0.5

```{r ff cm}
make_cm <- function(fit_obj, title="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + ggtitle(title)
}

conf_mat_final <- map2(list(xgb_ncorr_final[[2]], lr_final[[2]]), mod_labels, ~make_cm(.x, .y))

(conf_mat_final[[1]] | conf_mat_final[[2]])
```



## Performance

```{r ff metrics}
get_metrics <- function(fit_obj, label="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  summary() %>% 
  select(-.estimator) %>% 
  mutate(model = label)
}

bind_rows(get_metrics(xgb_ncorr_final[[2]], "XGB ncorr"),
          get_metrics(lr_final[[2]], "LLR")) %>%
  pivot_wider(names_from = model, values_from = .estimate)
```

## Probability cutoff adjustment by maximum j-index

```{r ff prob adj func}
maxj <- function(threshold_df) {
  # find max j-index
  max_j_index_threshold <- threshold_df %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
  
  # max_j_index_threshold may be a vector, use its last element
  if (length(max_j_index_threshold) > 1) {
    max_j_index_threshold <-
      max_j_index_threshold[length(max_j_index_threshold)]
  }
  
  return(max_j_index_threshold)
}

get_threshold <- function(fit_obj){
    # collect sens, spec, j-index at various cut-offs
  threshold_data <-
    fit_obj %>%
    collect_predictions() %>%
    threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" |
                               .metric == "spec" ~ "1",
                             TRUE ~ "2"))
  return(threshold_data)
}

senspec_plot <- function(fit_obj, title="") {
  # collect sens, spec, j-index at various cut-offs
  threshold_data <- get_threshold(fit_obj)
  
  # find max j-index
  max_j_index_threshold <- maxj(threshold_data)
  
  # plot metrics v cut-offs
  sens_spec_j_plot <-
    ggplot(threshold_data,
           aes(
             x = .threshold,
             y = .estimate,
             color = .metric,
             alpha = group
           )) +
    geom_line(size = 1) +
    #theme_minimal() +
    #scale_color_viridis_d(end = 0.9) +
    scale_color_brewer(palette = "Set1") +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(
      xintercept = max_j_index_threshold,
      alpha = .8,
      color = "grey30",
      linetype = "longdash"
    ) +
    labs(x = "Probability",
         y = "Metric Estimate",
         title = title)
  return(sens_spec_j_plot)
}
```

XGB

```{r}
senspec_plot(xgb_ncorr_final[[2]], "XGB ncorr")
```

LR

```{r}
senspec_plot(lr_final[[2]], "LLR")
```

j-index and the probability threshold

```{r}
lr_final[[2]] %>%
  get_threshold() %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  slice_tail()
```

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)


```{r opt cm}
optimize_prediction <- function(fit.obj){
  j_index <- maxj(get_threshold(fit.obj))
  
  optimized <- fit.obj %>%
    collect_predictions() %>%
    mutate(.pred = make_two_class_pred(
      estimate = .pred_HR,
      levels = levels(resistance),
      threshold = j_index
    )) %>%
    select(resistance, contains(".pred"))
  
  return(optimized)
}

cm_optimized_xgb <- optimize_prediction(xgb_ncorr_final[[2]]) %>% 
  conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle("XGB ncorr")

cm_optimized_lr <- optimize_prediction(lr_final[[2]]) %>% 
  conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle("LLR")

(cm_optimized_xgb | cm_optimized_lr)
```

### Optimized performance

```{r opt perf}
get_metrics2 <- function(fit.obj, label){
  optimize_prediction(fit.obj) %>% 
    conf_mat(truth = resistance, estimate = .pred) %>% 
    summary() %>% 
    mutate(model = label)
}

bind_rows(
  get_metrics2(xgb_yjbor_final[[2]], "XGB yjbor"),
  get_metrics2(xgb_ncorr_final[[2]], "XGB ncorr"),
  get_metrics2(lr_final[[2]], "LLR")
) %>% 
  pivot_wider(names_from = model, values_from = .estimate) %>% 
  select(-.estimator)
```

# Post-modelling analysis

## Features used by the best model

### How many predictors were removed

Overall, we have `r ncol(df_train) - 2` predictors.

#### Predictors left after applying `step_nzv()`

```{r n nzv}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  juice() %>% 
  ncol()
```


#### Predictors left after applying both `step_nzv()` and `step_corr()`

```{r n corr}
# NB: strain and resistance are still there
yj_recipe_tuned_lr %>% prep() %>% juice() %>% ncol()
```

Correlation threshold in the best LLR is `r best_models[[1]]$corr_tune`

### Features importance

Non-zero LLR coefficients:

```{r lr coeff}
lr_coeff_df <- lr_final[[2]] %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate)) %>% 
  mutate(odds.ratio = exp(estimate)) %>% 
  select(-penalty)

lr_coeff_df
```

Just 19 features plus Intercept are used by the model.

On the `odds.ratio` column: 

- units in which predictors are measured have been transformed

- apparently, nonHR is outcome 1 here and HR is outcome 0, because increase in `n.beta.lac` decreases chances of outcome 1.


### Predictors increasing the risk of HR:

```{r feat pos}
lr_coeff_df %>% 
  filter(odds.ratio < 1.0) %>% 
  select(term)
```

### Predictors decreasing the risk of HR

```{r feat neg}
lr_coeff_df %>% 
  filter(odds.ratio >= 1.0) %>% 
  select(term)
```

### Plot

```{r}
lr_coeff_df %>% 
  mutate(effect = if_else(estimate < 0, "negative", "positive")) %>% 
  ggplot(aes(reorder(term, odds.ratio, decreasing = T), odds.ratio)) +
  geom_col(aes(fill=effect)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.9, hjust=0.9, size = 14)) +
  xlab("")
```

# Minimal set of predictors chosen by Boruta

Four predictors were chosen by Boruta algorithms as being significantly more important than the rest

Here we use the same pre-processing as before: yj_recipe with tuned corr. threshold from the best `xgb_yjbor_bres`

```{r min feat set}
set.seed(42)

ncoryj_recipe_tuned_xgb <-
  update_corr_treshold(
    base_recipe = yj_recipe,
    step_no = 6,
    corr_threshold = best_models[[6]]$corr_tune
  )

df_train_yj <-
  ncoryj_recipe_tuned_xgb %>% prep() %>% juice() %>% select(-strain)

bor_pred_selection <- Boruta::Boruta(resistance ~., data = df_train_yj, maxRuns=200)

bor_pred_selection
```

```{r, fig.width=10}
plot(bor_pred_selection)
```

```{r min feat boxplot, fig.width=10}
minimal_pred <- bor_pred_selection$ImpHistory %>%
  as_tibble() %>%
  pivot_longer(cols = 1:77,
               names_to = "pred",
               values_to = "value") %>%
  group_by(pred) %>%
  mutate(median = median(value)) 

minimal_pred %>% 
  filter(median > 9.4) %>%
  ggplot(aes(reorder(pred, median), value)) +
  geom_boxplot(notch = T) +
  geom_violin(alpha = 0.1) +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 0.55,
    hjust = 0.5
  )) +
  xlab("") +
  ylab("Z-score")
```

```{r}
minimal_pred_best <- minimal_pred %>% filter(median > 11.5) %>% select(-median)

kruskal.test(minimal_pred_best$value, minimal_pred_best$pred)
```

```{r}
minimal_pred_best <- minimal_pred %>% filter(median > 13.5) %>% select(-median)

kruskal.test(minimal_pred_best$value, minimal_pred_best$pred)
```

n.TEM.1 and IS6 differ non-significantly

n.rep.plasmid.TEM and min_distance_is_gene differ significantly


Why to choose the top 3?

Their 'boxes' overlap.

## An RF model with the top 3 predictors

```{r}
minimal_pred %>% distinct(pred, .keep_all = T) %>% arrange(-median)
```

```{r min recipe}
top_predictors <- 
  minimal_pred %>% 
  #filter(median > 10) %>% 
  distinct(pred, .keep_all = T) %>% 
  arrange(-median) %>% 
  head(n = 3) %>% 
  pull(pred)

data_strain_min <- data_strain %>% select(resistance, all_of(top_predictors))

set.seed(124)

data_split_min <- initial_split(data_strain_min, prop = 0.8, strata = resistance)

df_train_min <- training(data_split_min)
df_test_min <- testing(data_split_min)

cv_folds_min <- vfold_cv(df_train_min, strata = "resistance", v = 10, repeats = 10)

min_set_recipe <- recipe(resistance ~ ., data = df_train_min) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

rf_spec <-
  rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine('ranger', num.threads = 6) %>%
  set_mode('classification')

min_pred_wf <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(min_set_recipe)

param_set <- extract_parameter_set_dials(min_pred_wf) %>%
  finalize(x = df_train_min %>% select(-resistance))
```


If you need to train this model again run this chunk:

```{r rf min3 train, eval=FALSE}
rf_min_res <- min_pred_wf %>%
  tune_grid(
    param_info = param_set,
    grid = 20,
    resamples = cv_folds_min,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

saveRDS(rf_min_res, paste0(models_path, "rf_min3_res.rds"))
```

By default this chunk is used:

```{r rf min3 res}
rf_min_res <- readRDS(paste0(models_path, "rf_min3_res.rds"))
autoplot(rf_min_res)
```


```{r}
show_best(rf_min_res, metric= "roc_auc")
```

## An XGB model with the top 3 predictors

same top 3 predictors from Boruta selection

```{r xgb min3}
xgb_min3_bres <- readRDS(paste0(models_path, "xgb_min3_yj_bres_p30.rds"))

autoplot(xgb_min3_bres)
```

```{r}
show_best(xgb_min3_bres)
```

## ROCs

```{r min3 rocs}
top_models <- list(
    lr_ncoryj_res,
    rf_min_res,
    xgb_min3_bres)

top_names <- c("LR", "RF top 3", "XGB top 3")

roc_min_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_roc(x, y))
roc_min_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  #scale_color_viridis_d(direction = -1, option = "H")+
  scale_color_brewer(palette = "Set1") +
  ggtitle("")
```

## Distribution comparison

```{r}
mod_comparison <- res_comp_table(top_models,
                                 top_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set1") 
```


```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```


## Test set

### RF min 3

```{r rf min3 test}
rf_best_top3 <- select_best(rf_min_res, metric = "roc_auc")

rf_min_tuned <-
  rand_forest(mtry = rf_best_top3$mtry, 
              min_n = rf_best_top3$min_n) %>%
  set_engine('ranger', num.threads = 2) %>%
  set_mode('classification')

rf_min_final <- run_last_fit(
  "RF top 3",
  rf_spec,
  rf_min_tuned,
  recipe = min_set_recipe,
  train_test_split = data_split_min,
  return_fit = TRUE
)

rf_min_final[[1]]
```

#### Final ROC

```{r}
make_final_roc(rf_min_final[[2]], "RF top 3")
```

#### Final confusion matrix

```{r}
make_cm(rf_min_final[[2]])
```

#### Final metrix

```{r}
get_metrics(rf_min_final[[2]], "RF top 3") %>% pivot_wider(names_from = model, values_from = .estimate)
```

#### Final sensitivity/specificity plot

```{r}
senspec_plot(rf_min_final[[2]], "RF top 3")
```

This one lacks high sensitivity.

### XGB min 3

```{r xgb min3 test}
boost_tree_xgboost_spec <-
  boost_tree(
    tree_depth = tune(),
    trees = tune(),
    learn_rate = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    stop_iter = tune()
  ) %>%
  set_engine('xgboost', num.threads = 4) %>%
  set_mode('classification')



xgb_best_top3 <- select_best(xgb_min3_bres, metric = "roc_auc")

xgb_min_tuned <-
  boost_tree(
    trees = 50,
    min_n = xgb_best_top3$min_n,
    tree_depth = xgb_best_top3$tree_depth,
    learn_rate = xgb_best_top3$learn_rate,
    loss_reduction = xgb_best_top3$loss_reduction,
    sample_size = xgb_best_top3$sample_size,
    stop_iter = xgb_best_top3$stop_iter
  ) %>%
  set_engine('xgboost', num.threads = 2) %>%
  set_mode('classification')

xgb_min_final <- run_last_fit(
  "XGB top 3",
  boost_tree_xgboost_spec,
  xgb_min_tuned,
  recipe = min_set_recipe,
  train_test_split = data_split_min,
  return_fit = TRUE
)

xgb_min_final[[1]]
```

#### Final ROC

```{r}
make_final_roc(xgb_min_final[[2]], "xgb top 3")
```

#### Final confusion matrix

```{r}
make_cm(xgb_min_final[[2]])
```

#### Final metrix

```{r}
get_metrics(xgb_min_final[[2]], "xgb top 3") %>% pivot_wider(names_from = model, values_from = .estimate)
```

#### Final sensitivity/specificity plot

```{r}
senspec_plot(xgb_min_final[[2]], "xgb top 3")
```

## DALEX

You can choose for instance false positives and look what influenced
their classification as HR

```{r, eval=FALSE}
false_positives <- 
  prediction_all_data %>% 
  filter(.pred_class == "HR", 
         .true_class == "nonHR")

false_positives
```

```{r, eval=FALSE}
library(DALEXtra)

explainer_final <- 
  explain_tidymodels(
    final_mod_fit, 
    data = data_strain_proc, 
    y = data_strain$resistance,
    label = "LR",
    verbose = FALSE
  )

data_strain_proc$strain <- data_strain$strain
data_strain_proc$resistance <- data_strain$resistance

obs_interest <- data_strain_proc %>% 
  filter(strain == "DA62984") %>% 
  select(-c(strain, resistance))


final_mod_breakdown <-
  predict_parts(explainer = explainer_final, new_observation = obs_interest)

final_mod_breakdown
```

Most of the predictors don't make any contributions at all.

Also, `contribution` is *the probability of nonHR* (because the last
line 'prediction' equals `1 - .prob_HR` )

```{r, eval=FALSE}
# look at nc.beta.lac.plasmid contribution
data_strain_proc %>% 
  mutate(label = if_else(strain == "DA62984", "DA62984", resistance)) %>% 
  ggplot(aes(label, n.TEM.1)) +
  geom_jitter(aes(color=resistance))
```

## Extracting predictions

Using the final model fit and adjusted probability cut-off

That final recipe:

```{r, eval=FALSE}
final_recipe <- last_wf %>% fit(df_train) %>% extract_recipe()

final_recipe
```

The final model fit

```{r, eval=FALSE}
final_mod_fit <- last_wf %>% fit(df_train) %>% extract_fit_parsnip()
```

```{r, eval=FALSE}
data_strain_proc <- final_recipe %>%
  bake(new_data = data_strain) %>% 
  select(-c(strain, resistance))

data_strain_proc
```

```{r, eval=FALSE}
# I tested this code on df_test and it gives the same confusion matrix as the code above in section 'Final fit' and 'Confusion matrix with prob=0.5'
# So it's the right way to apply predict() to the entire data set to identify false predictions

prediction_all_data <-
  predict(final_mod_fit, new_data = data_strain_proc, type = "prob") %>%
  select(.pred_HR) %>%
  mutate(
    .pred_class = if_else(.pred_HR >= max_j_index_threshold, "HR", "nonHR"),
    .true_class = data_strain$resistance,
    strain = data_strain$strain
  ) %>%
  rename(.prob_HR = .pred_HR)

prediction_all_data
```

## AMR types and false predictions

```{r, eval=FALSE}
# this is for Dan
amr_types_strain <- read_csv("data/amr_types_strain.csv")

prediction_all_data %>%  
  left_join(amr_types_strain, by="strain") %>% 
  write.csv(file="data/amr_types_prediction_prob.csv", row.names = F)
```

```{r, eval=FALSE}
# amr_type_pred <- read_csv("data/amr_types_prediction_prob.csv")

amr_type_pred %>% 
  filter(.pred_class != .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

```{r, eval=FALSE}
amr_type_pred %>% 
  filter(.pred_class == .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```
