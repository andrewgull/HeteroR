---
title: "Modelling"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = T)
library(tidyverse)
library(caret)
library(car)
```

# All models are wrong, but some are useful.

[George Box](https://r4ds.had.co.nz/model-basics.html)

# Read the data

Data set `features_amp_strain` has been made previously in `feature_engineering.Rmd`

Here I add BL genes counts as ordered factor and as binary features (if BL > 3)

```{r}
feat_amp <- read_csv("tables/features_amp_strain.csv", na = "NA")
feat_amp <- rename(feat_amp, n.plasmids="n_plasmids") %>% relocate(n.plasmids, .before="n.rep.total")

# create dummy from original target
#feat_amp$resistance.dummy <- ifelse(feat_amp$resistance=="R", 1, 0)
#feat_amp <- relocate(feat_amp, resistance.dummy, .before="n.beta.lac")

# BL counts as an ordered factor
#feat_amp$n.beta.lac.f <- factor(feat_amp$n.beta.lac, ordered = T, levels = c(1,2,3,4,5,6))
#feat_amp <- relocate(feat_amp, n.beta.lac.f, .before="n.plasmids")

# resistance as factor
feat_amp$resistance <- as.factor(feat_amp$resistance)

# BL counted as binary > 3
feat_amp$n.beta.lac.3 <- ifelse(feat_amp$n.beta.lac > 3, 1, 0)
feat_amp <- relocate(feat_amp, n.beta.lac.3, .before="n.plasmids")

# remove AB and strain
feat_amp <- feat_amp %>% select(-c(strain, AB))

feat_amp
```

# Descriptive stats

```{r}
skimr::skim(feat_amp)
```

There are 3 strains with NAs, we can replace NA s with 0s

# Data imputation

`caret::preProcess()` doesn't have method for replacing with zeros...

```{r}
# make it by hand
feat_amp$n.rep.tot.cen <- ifelse(is.na(feat_amp$n.rep.tot.cen), 0, feat_amp$n.rep.tot.cen)

feat_amp$n.rep.tot.non.cen <- ifelse(is.na(feat_amp$n.rep.tot.non.cen), 0, feat_amp$n.rep.tot.non.cen)

feat_amp$med.tot.rep.len <- ifelse(is.na(feat_amp$med.tot.rep.len), 0, feat_amp$med.tot.rep.len)

feat_amp$med.AR.len.cen <- ifelse(is.na(feat_amp$med.AR.len.cen), 0, feat_amp$med.AR.len.cen)

feat_amp$ampC.med.tot.rep.len <- ifelse(is.na(feat_amp$ampC.med.tot.rep.len), 0, feat_amp$ampC.med.tot.rep.len)
```


# Pairs plot

```{r}
names(feat_amp)
```


One more time

```{r, fig.width=18, fig.height=18, warning=F, message=F}
# use GGally

GGally::ggpairs(feat_amp, 
        columns=c(3:41), 
        aes(color=resistance, alpha=0.2, dotsize=0.02), 
        upper = list(continuous = GGally::wrap("cor", size = 2.5)),
        diag=list(continuous ="barDiag"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")
```

# Correlated predictors

## Corr plot

```{r, fig.width=12, fig.height=12}
cor.matrix <- cor(feat_amp[,c(4:41)], use = "pairwise.complete.obs")
corrplot::corrplot(cor.matrix, hclust.method = "ward")
```

## Identifying and removing correlated features

Summary of pairwise correlations between numeric predictors

```{r}
# exclude resistance, n.beta.lac.3
feat_amp_proc <- select(feat_amp, -c(strain, AB, resistance, n.beta.lac.3))

descrCor <- cor(feat_amp_proc)
summary(descrCor[upper.tri(descrCor)])
```

```{r, fig.width=4, fig.height=4}
boxplot(descrCor[upper.tri(descrCor)])
```

Remove those predictors that have correlation coefficient greater than 0.75

Check the summary of the new filtered correlations

```{r}
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

feat_amp_proc <- feat_amp_proc[,-highlyCorDescr]

# check new correlations
descrCor2 <- cor(feat_amp_proc)
summary(descrCor2[upper.tri(descrCor2)])
```

Add `resistance` and `n.beta.lac.3` columns to filtered columns

```{r}
feat_amp_proc$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_proc$resistance <- feat_amp$resistance
```


I will use non-correlated predictors with LR

With RF and XGB I will use all predictors + automatic feature selection

# Split the data set

## Non-correlated predictors

```{r}
set.seed(100)

in_train_nocorr <- createDataPartition(y = feat_amp_proc$resistance, p=0.8, list=FALSE)


training_nocorr <- feat_amp_proc[in_train_nocorr,]
testing_nocorr <- feat_amp_proc[-in_train_nocorr,]
```

## All predictors

```{r}
set.seed(100)

in_train_all <- createDataPartition(y = feat_amp$resistance, p=0.8, list=FALSE)


training_all <- feat_amp[in_train_all,]
testing_all <- feat_amp[-in_train_all,]
```


# Regularized Logistic Regression

## Hand picked features

```{r}
feat_selected <- feat_amp %>% select(c(resistance, n.beta.lac.3, n.plasmids, med.rep.len, med.tot.rep.len, max.rep.len, n.rep.total, med.AR.len.cen))

# NAs in `med.tot.rep.len` and in `med.AR.len.cen` should be replaced with 0s
feat_selected$med.tot.rep.len <- ifelse(is.na(feat_selected$med.tot.rep.len), 0, feat_selected$med.tot.rep.len)
feat_selected$med.AR.len.cen <- ifelse(is.na(feat_selected$med.AR.len.cen), 0, feat_selected$med.AR.len.cen)

summary(feat_selected)
```

### Split hand picked dataset

```{r}
set.seed(100)

in_train_hand <- createDataPartition(y = feat_selected$resistance, p=0.8, list=FALSE)


training_hand <- feat_selected[in_train_hand,]
testing_hand <- feat_selected[-in_train_hand,]
```

### Params tuning

```{r}
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fitControl_acc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T)
```

### Training and validation

Same model (3a) that won in "Statistical exploration" (see below and the html file compiled earlier)

#### ROC-optimized

```{r}
set.seed(100)

fit_LR_hand_roc <- train(resistance ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len, data = training_hand, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
fit_LR_hand_roc
```

**Coefficients**

```{r}
fit_LR_hand_roc$finalModel$W
```

#### Accuracy-optimized

```{r}
set.seed(100)

fit_LR_hand_acc <- train(resistance ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len, data = training_hand, 
                 method = "regLogistic", 
                 trControl = fitControl_acc,
                 verbose = FALSE,
                 metric="Accuracy")
fit_LR_hand_acc
```


**Coefficients**

```{r}
fit_LR_hand_acc$finalModel$W
```

### Testing

#### ROC-optimized

```{r}

predClasses_roc <- predict(fit_LR_hand_roc, newdata=testing_hand)

confusionMatrix(data = predClasses_roc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")
```

Not bad and similar to Model 3a

#### Accuracy-optimized

```{r}

predClasses_acc <- predict(fit_LR_hand_acc, newdata=testing_hand)

confusionMatrix(data = predClasses_acc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")
```

This model performs slightly worse than the ROC-optimized one

## Non-correlated features

Since LR doesn't have in-built feature selection, let's use *filter* methods (less prone to over-fitting and more computationally efficient than *wrapper* methods)

### Prepare the data set

remove strain, AB

```{r, eval=F}
feat_amp <- select(feat_amp, -c(strain, AB))

skimr::skim_to_wide(feat_amp)
```

### Training and validation

```{r}
set.seed(100)

# fitControl is ROC optimized
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fit_LR_nocorr_roc <- train(resistance ~ ., data = training_nocorr, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_LR_nocorr_roc
```

It's better then the model with hand picked predictors (ROC)

### Testing

```{r}

predClasses_roc <- predict(fit_LR_nocorr_roc, newdata=testing_nocorr)

confusionMatrix(data = predClasses_roc, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")
```

Overall, all non-correlated predictors model is better than the hand-picked one

## Recursive feature selection

```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:5, 10, 15, 21)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE,
                   allowParallel = T)

# training[, 2:21] - 2:21 because I want to exclude n.beta.lac and leave only its binary counterpart n.beta.lac.3
lmProfile <- rfe(x=training[, 2:21], y=training$resistance,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile
```

## LR on top 5 best features


### Data

Leave only the top 5 features from test and train data sets

```{r}
training_top5 <- select(training, c(resistance, n.beta.lac.3, max.rep.len.non.cen, med.AR.len.100, n.rep.100.non.cen, med.AR.len.non.cen.100))

testing_top5 <- select(testing, c(resistance, n.beta.lac.3, max.rep.len.non.cen, med.AR.len.100, n.rep.100.non.cen, med.AR.len.non.cen.100))
```


### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

Fit3 <- train(resistance ~ ., data = training_top5, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit3
```

### Testing

```{r}

predClasses <- predict(Fit3, newdata=testing_top5)

confusionMatrix(data = predClasses, 
                reference = testing_top5$resistance,
                mode="everything",
                positive="R")
```


## LR on 15 best features

including top 5

### Data set

```{r}
training_top15 <- select(training, resistance, lmProfile$optVariables)

testing_top15 <- select(testing, resistance, lmProfile$optVariables)
```


### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

Fit4 <- train(resistance ~ ., data = training_top15, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit4
```

### Testing

```{r}

predClasses <- predict(Fit4, newdata=testing_top15)

confusionMatrix(data = predClasses, 
                reference = testing_top15$resistance,
                mode="everything",
                positive="R")
```

## RLR conclusion

On validation data set Fit2 (all non-correlated features) and Fit3 (top 5) are the best two models

# Random Forest

Same `fitControl_roc` as earlier

on all predictors because RF has automatic feature selection

## Params tuning

```{r}
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```


## Training and validation

```{r}
set.seed(100)

fit_RF <- train(resistance ~ ., data = training_all, 
                 method = "rf", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_RF
```

## Chosen predictors

```{r}
predictors(fit_RF)
```

Every predictor was used

## Testing

```{r}
predClasses <- predict(fit_RF, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

Much better than any RLR

# SVM

Same `fitControl_roc` as earlier

on all non-correlated predictors

## Training and validation

```{r}
set.seed(100)

fit_SVM <- train(resistance ~ ., data = training_all, 
                 method = "svmRadial", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_SVM
```

## Testing

```{r}
predClasses <- predict(fit_SVM, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

# xgBoost

DART variant

## Training and validation

```{r}
set.seed(100)

fit_xgbDART <- train(resistance ~ ., data = training_all, 
                 method = "xgbDART", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)
fit_xgbDART
```

### Best model

```{r}
filter(fit_xgbDART$results, nrounds == 150, max_depth == 1, eta == 0.4, gamma == 0, subsample == 1, colsample_bytree == 0.6, rate_drop == 0.01, skip_drop == 0.95, min_child_weight == 1)
```


## Testing


```{r}
predClasses <- predict(fit_xgbDART, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

## Tune grid

Fixed grid parameters, is much faster of course.

The params are borrowed from [Kaggle](https://www.kaggle.com/code/nagsdata/simple-r-xgboost-caret-kernel/script)

N.B. It's `xgbTree`!

```{r}
trctrl <- trainControl(method = "cv", number = 5)

# Takes a long to time to run in kaggle
#tune_grid <- expand.grid(nrounds=c(100,200,300,400), 
#                         max_depth = c(3:7),
#                         eta = c(0.05, 1),
#                         gamma = c(0.01),
#                         colsample_bytree = c(0.75),
#                         subsample = c(0.50),
#                         min_child_weight = c(0))

# Tested the above setting in local machine
tune_grid <- expand.grid(nrounds = 200,
                        max_depth = 5,
                        eta = 0.05,
                        gamma = 0.01,
                        colsample_bytree = 0.75,
                        min_child_weight = 0,
                        subsample = 0.5)

fit_xgbTree <- train(resistance ~., data = training[2:22], method = "xgbTree",
                trControl=fitControl_roc,
                tuneGrid = tune_grid,
                tuneLength = 10,
                metric="ROC")

fit_xgbTree
```



# Model comparison

```{r}
#, SVM=FitSVM, XGB.DART=fit_xgbDART, XGB.Tree=fit_xgbTree
models_compare <- resamples(list(RLR_hand=fit_LR_hand_roc, RLR_nocorr=fit_LR_nocorr_roc, RF=fit_RF, SVM=fit_SVM))

summary(models_compare)
```

```{r}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
```

```{r}
dotplot(models_compare, scales=scales)
```


`xgbDART` with hyper-parameters search is the best (as expected)


# Variable importance

It is not available for RLR and SVM

## RF

```{r}
imp_vars_rf <- varImp(fit_RF)

plot(imp_vars_rf, main="Variable Importance with RF")
```

## xgBoost

```{r}
imp_vars_xgb <- varImp(fit_xgbDART)

plot(imp_vars_xgb, main="Variable Importance with XGB")
```

# Save the workspace

```{r, eval=TRUE}
save.image(f="/home/andrei/Data/HeteroR/notebooks/modelling.RData")
```
