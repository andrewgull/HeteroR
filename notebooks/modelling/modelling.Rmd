---
title: "Modelling"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = T)
library(tidyverse)
library(caret)
#library(car)
library(parallel)
library(doParallel)
```

# All models are wrong, but some are useful.

[George Box](https://r4ds.had.co.nz/model-basics.html)

# Read the data

Data set `features_amp_strain` has been made previously in `feature_engineering.Rmd`

Here I add BL genes counts as ordered factor and as binary features (if BL > 3)

```{r}
feat_amp <- read_csv("tables/features_amp_strain.csv", na = "NA")
feat_amp <- rename(feat_amp, n.plasmids="n_plasmids") %>% relocate(n.plasmids, .before="n.rep.total")

# create dummy from original target
#feat_amp$resistance.dummy <- ifelse(feat_amp$resistance=="R", 1, 0)
#feat_amp <- relocate(feat_amp, resistance.dummy, .before="n.beta.lac")

# BL counts as an ordered factor
#feat_amp$n.beta.lac.f <- factor(feat_amp$n.beta.lac, ordered = T, levels = c(1,2,3,4,5,6))
#feat_amp <- relocate(feat_amp, n.beta.lac.f, .before="n.plasmids")

# resistance as factor
feat_amp$resistance <- as.factor(feat_amp$resistance)

# BL counted as binary > 3
feat_amp$n.beta.lac.3 <- ifelse(feat_amp$n.beta.lac > 3, 1, 0)
feat_amp <- relocate(feat_amp, n.beta.lac.3, .before="n.plasmids")

# remove AB and strain
feat_amp <- feat_amp %>% select(-c(strain, AB))

feat_amp
```

# Descriptive stats

```{r}
skimr::skim(feat_amp)
```

There are 3 strains with NAs, we can replace NA s with 0s

# Data imputation

`caret::preProcess()` doesn't have method for replacing with zeros...

```{r}
# make it by hand
feat_amp$n.rep.tot.cen <- ifelse(is.na(feat_amp$n.rep.tot.cen), 0, feat_amp$n.rep.tot.cen)

feat_amp$n.rep.tot.non.cen <- ifelse(is.na(feat_amp$n.rep.tot.non.cen), 0, feat_amp$n.rep.tot.non.cen)

feat_amp$med.tot.rep.len <- ifelse(is.na(feat_amp$med.tot.rep.len), 0, feat_amp$med.tot.rep.len)

feat_amp$med.AR.len.cen <- ifelse(is.na(feat_amp$med.AR.len.cen), 0, feat_amp$med.AR.len.cen)

feat_amp$ampC.med.tot.rep.len <- ifelse(is.na(feat_amp$ampC.med.tot.rep.len), 0, feat_amp$ampC.med.tot.rep.len)
```

# Data normalization

scale and center numeric features (real numeric + count, not dummies!)

```{r}
feat_amp_scaled <- as_tibble(scale(feat_amp[,c(2,4:39)]))

feat_amp_scaled$resistance <- feat_amp$resistance
feat_amp_scaled$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_scaled <- relocate(feat_amp_scaled, n.beta.lac.3, .before="n.beta.lac")
feat_amp_scaled <- relocate(feat_amp_scaled, resistance, .before="n.beta.lac.3")
```


# Pairs plot

```{r}
names(feat_amp)
```


One more time

```{r, fig.width=18, fig.height=18, warning=F, message=F}
# use GGally

GGally::ggpairs(feat_amp_scaled, 
        columns=c(1:39), 
        aes(color=resistance, alpha=0.2, dotsize=0.02), 
        upper = list(continuous = GGally::wrap("cor", size = 2.5)),
        diag=list(continuous ="barDiag"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")
```

# Correlated predictors

## Corr plot

```{r, fig.width=12, fig.height=12}
cor_matrix <- cor(feat_amp[,c(2:39)], use = "pairwise.complete.obs")
corrplot::corrplot(cor_matrix, type="upper", tl.col = "black")
```

```{r, fig.width=12, fig.height=12}
cor_matrix_scaled <- cor(feat_amp[,c(2:39)], use = "pairwise.complete.obs")
corrplot::corrplot(cor_matrix_scaled, type="upper", method = "number", number.cex=0.8, number.digits=1, tl.col = "black")
```


## Identifying and removing correlated features

Summary of pairwise correlations between numeric predictors - on scaled data

```{r}
# exclude resistance, n.beta.lac.3
feat_amp_proc <- select(feat_amp_scaled, -c(resistance, n.beta.lac.3))

descr_cor <- cor(feat_amp_proc)
summary(descr_cor[upper.tri(descr_cor)])
```

```{r, fig.width=4, fig.height=4}
boxplot(descr_cor[upper.tri(descr_cor)])
```

Remove those predictors that have correlation coefficient greater than 0.75

Check the summary of the new filtered correlations

```{r}
highly_cor <- findCorrelation(descr_cor, cutoff = .75)

feat_amp_proc <- feat_amp_proc[,-highly_cor]

# check new correlations
descr_cor2 <- cor(feat_amp_proc)
summary(descr_cor2[upper.tri(descr_cor2)])
```

Add `resistance` and `n.beta.lac.3` columns to filtered columns

```{r}
feat_amp_proc$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_proc$resistance <- feat_amp$resistance
```


I will use non-correlated predictors with LR

With RF and XGB I will use all predictors + automatic feature selection

# Split the data set

## Data set with non-correlated predictors

```{r}
set.seed(100)

in_train_nocorr <- createDataPartition(y = feat_amp_proc$resistance, p=0.75, list=FALSE)


training_nocorr <- feat_amp_proc[in_train_nocorr,]
testing_nocorr <- feat_amp_proc[-in_train_nocorr,]

# to avoid slowness of caret
x_nocorr <- as.matrix(training_nocorr[,-26])
y_nocorr <- training_nocorr$resistance

```

## Data set with all predictors

```{r}
set.seed(100)

in_train_all <- createDataPartition(y = feat_amp_scaled$resistance, p=0.75, list=FALSE)


training_all <- feat_amp_scaled[in_train_all,]
testing_all <- feat_amp_scaled[-in_train_all,]

# to avoid slowness of caret
x_all <- as.matrix(training_all[,-1])  # except for resistance
y_all <- training_all$resistance

```

# Feature importance

### Box plots

```{r, fig.width=15, fig.height=12}
featurePlot(x = x_all, 
            y = y_all, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

### Density

```{r, fig.width=15, fig.height=12}
featurePlot(x = x_all, 
            y = y_all, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

Skewed predictors may be candidates for transformation (log)

## Best predictors

I will use them later along with the set of all predictors

```{r}
pred_best <- c("resistance", "n.beta.lac.3", "med.dist.oriC", "n.genes.plus.strand", "n.genes.plasmids", "med.AR.len.cen", "max.rep.len", "n.rep.500", "med.rep.len.100")
```

## Data set with the best predictors

```{r}
training_best <- training_all %>% select(pred_best)
testing_best <- testing_all %>% select(pred_best)

x_best <- as.matrix(training_best[,-1])
y_best <- training_best$resistance
```


# Regularized Logistic Regression

## Hand picked features

```{r}
feat_selected <- feat_amp_scaled %>% 
  select(c(resistance, n.beta.lac.3, n.plasmids, med.rep.len, med.tot.rep.len, max.rep.len, n.rep.total, med.AR.len.cen, med.dist.oriC, n.genes.plasmids))

# NAs in `med.tot.rep.len` and in `med.AR.len.cen` should be replaced with 0s
feat_selected$med.tot.rep.len <- ifelse(is.na(feat_selected$med.tot.rep.len), 0, feat_selected$med.tot.rep.len)
feat_selected$med.AR.len.cen <- ifelse(is.na(feat_selected$med.AR.len.cen), 0, feat_selected$med.AR.len.cen)

summary(feat_selected)
```

### Split hand picked dataset

```{r}
set.seed(100)

in_train_hand <- createDataPartition(y = feat_selected$resistance, p=0.75, list=FALSE)


training_hand <- feat_selected[in_train_hand,]
testing_hand <- feat_selected[-in_train_hand,]

# to avoid slowness of caret
x_hand <- as.matrix(training_hand[,-1])
y_hand <- training_hand$resistance

```

### Params tuning

```{r}
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fitControl_acc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T)
```

### Training and validation

Same model (3a) that won in "Statistical exploration" (see below and the html file compiled earlier)

#### ROC-optimized

```{r}
set.seed(100)

fit_LR_hand_roc <- train(x = x_hand, 
                         y = y_hand,
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
fit_LR_hand_roc
```

**Resamples**

```{r}
fit_LR_hand_roc$resample %>% arrange(-ROC)
```

**confusion matrix on train**

```{r}
confusionMatrix.train(fit_LR_hand_roc)
```


#### Accuracy-optimized

```{r}
set.seed(100)

fit_LR_hand_acc <- train(x = x_hand,
                         y = y_hand, 
                 method = "regLogistic", 
                 trControl = fitControl_acc,
                 verbose = FALSE,
                 metric="Accuracy")
fit_LR_hand_acc
```


### Testing

#### ROC-optimized

```{r}

predClasses_roc <- predict(fit_LR_hand_roc, newdata=testing_hand)

confusionMatrix(data = predClasses_roc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")
```

Not bad and similar to Model 3a

#### Accuracy-optimized

```{r}

predClasses_acc <- predict(fit_LR_hand_acc, newdata=testing_hand)

confusionMatrix(data = predClasses_acc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")
```

This model performs a bit worse than ROC-optimized one

## Non-correlated features

Since LR doesn't have in-built feature selection, let's use *filter* methods (less prone to over-fitting and more computationally efficient than *wrapper* methods)

### Training and validation

```{r}
set.seed(100)

# fitControl is ROC optimized
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fit_LR_nocorr_roc <- train(x = x_nocorr,
                           y = y_nocorr,
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_LR_nocorr_roc
```

It's better then the model with hand picked predictors (ROC)

**Resamples**

```{r}
fit_LR_nocorr_roc$resample %>% arrange(-ROC)
```

**confusion matrix on train**

```{r}
confusionMatrix.train(fit_LR_nocorr_roc)
```

### Testing

```{r}

predClasses_roc <- predict(fit_LR_nocorr_roc, newdata=testing_nocorr)

confusionMatrix(data = predClasses_roc, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")
```

Overall, all non-correlated predictors model is better than the hand-picked one



## LR on automatically selected features

### Recursive feature selection

```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:38)

rfe_ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE,
                   allowParallel = T, 
                   number = 10, 
                   p = 0.75)

# training[, -2] - because I want to exclude n.beta.lac and leave only its binary counterpart n.beta.lac.3
lm_profile <- rfe(x = x_all[,-2], 
                 y = y_all,
                 sizes = subsets,
                 rfeControl = rfe_ctrl, 
                 metric = "RMSE")

lm_profile
```

### Data

Leave only the top 5 features from test and train data sets

```{r}
top5 <- c("min.dist.oriC", "n.beta.lac.3", "n.genes.plasmids", "max.rep.len.cen", "med.AR.len.100")

training_top5 <- select(training_all, c(resistance, top5))

testing_top5 <- select(testing_all, c(resistance, top5))


x_top5 <- as.matrix(training_top5[,-1])
y_top5 <- training_top5$resistance
```


### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

fit_LR_top5_roc <- train(x = x_top5,
                         y = y_top5,
                         method = "regLogistic", 
                         trControl = fitControl_roc,
                         verbose = FALSE,
                         metric="ROC")
fit_LR_top5_roc
```

**Resamples**

```{r}
fit_LR_top5_roc$resample %>% arrange(-ROC)
```

**Confusion matrix on train**

```{r}
confusionMatrix.train(fit_LR_top5_roc)
```


### Testing

```{r}

predClasses <- predict(fit_LR_top5_roc, newdata=testing_top5)

confusionMatrix(data = predClasses, 
                reference = testing_top5$resistance,
                mode="everything",
                positive="R")
```

## RLR conclusion

On validation data set Fit2 (all non-correlated features) and Fit3 (top 5) are the best two models

# Random Forest

Same `fitControl_roc` as earlier

on all predictors because RF has automatic feature selection

## Params tuning

```{r}
fit_ctrl_rf_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

#mtry <- sqrt(ncol(training_all[,2:39]))
#tunegrid <- expand.grid(.mtry=mtry)
```

## Data set with all predictors

### Training and validation

```{r}
set.seed(100)

fit_RF <- train(x = x_all,
                y = y_all,  
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF
```

### Predictors importance

```{r}
imp_vars_rf <- varImp(fit_RF)

plot(imp_vars_rf, main="Variable Importance with RF")
```


### Testing

```{r}
predClasses <- predict(fit_RF, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

### Training and validation

```{r}
set.seed(100)

fit_RF_best <- train(x_best, y_best, 
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF_best
```

### Predictors importance

```{r}
imp_vars_rf <- varImp(fit_RF_best)

plot(imp_vars_rf, main="Variable Importance with RF")
```



### Testing

```{r}
predClasses <- predict(fit_RF_best, newdata=testing_best)

confusionMatrix(data = predClasses, 
                reference = testing_best$resistance,
                mode="everything",
                positive="R")
```


# SVM

Same `fitControl_roc` as earlier

on all non-correlated predictors

## Training and validation

```{r}
set.seed(100)

fit_SVM <- train(resistance ~ ., data = training_all, 
                 method = "svmRadial", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_SVM
```

## Testing

```{r}
predClasses <- predict(fit_SVM, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

# xgBoost

DART variant

## Params tuning

```{r}
fit_ctrl_xgb_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 5, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```


## Data set with all predictors

### Training and validation

```{r}
set.seed(100)

fit_XGB <- train(x = x_all, y = y_all, 
                 method = "xgbDART", 
                 trControl = fit_ctrl_xgb_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)
fit_XGB$bestTune
```

#### Best model

```{r}
filter(fit_XGB$results, nrounds == 150, max_depth == 1, eta == 0.3, gamma == 0, subsample == 0.75, colsample_bytree == 0.6, rate_drop == 0.01, skip_drop == 0.95, min_child_weight == 1)
```

### Features importance

```{r}
imp_vars_xgb <- varImp(fit_XGB)

plot(imp_vars_xgb, main="Variable Importance with XGB")
```

### Testing


```{r}
predClasses <- predict(fit_XGB, newdata=testing_all)

confusionMatrix(data = predClasses, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")
```

## Data set with best predictors

### Training and validation

Scaled and centered train data set!

```{r}
set.seed(100)

x_best_scaled <- scale(x_best)

fit_XGB_best <- train(x=x_best_scaled, y=y_best, 
                 method = "xgbDART", 
                 trControl = fit_ctrl_xgb_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)

fit_XGB_best$bestTune
```


### Features importance


```{r}
imp_vars_xgb <- varImp(fit_XGB_best)

plot(imp_vars_xgb, main="Variable Importance with XGB")
```

### Testing

```{r}
predClasses <- predict(fit_XGB_best, newdata=testing_best)

confusionMatrix(data = predClasses, 
                reference = testing_best$resistance,
                mode="everything",
                positive="R")
```


## Tune grid

Fixed grid parameters, is much faster of course.

The params are borrowed from [Kaggle](https://www.kaggle.com/code/nagsdata/simple-r-xgboost-caret-kernel/script)

N.B. It's `xgbTree`!

```{r, eval=F}
trctrl <- trainControl(method = "cv", number = 5)

# Takes a long to time to run in kaggle
#tune_grid <- expand.grid(nrounds=c(100,200,300,400), 
#                         max_depth = c(3:7),
#                         eta = c(0.05, 1),
#                         gamma = c(0.01),
#                         colsample_bytree = c(0.75),
#                         subsample = c(0.50),
#                         min_child_weight = c(0))

# Tested the above setting in local machine
tune_grid <- expand.grid(nrounds = 200,
                        max_depth = 5,
                        eta = 0.05,
                        gamma = 0.01,
                        colsample_bytree = 0.75,
                        min_child_weight = 0,
                        subsample = 0.5)

fit_xgbTree <- train(resistance ~., data = training[2:22], method = "xgbTree",
                trControl=fitControl_roc,
                tuneGrid = tune_grid,
                tuneLength = 10,
                metric="ROC")

fit_xgbTree
```


# Model comparison

## During training and validation

This resampling data come from training and validation, so they reflect model search not the average model performance.

```{r}
#, SVM=FitSVM, XGB.DART=fit_xgbDART, XGB.Tree=fit_xgbTree
models_compare <- resamples(list(RLR_hand=fit_LR_hand_roc, RLR_nocorr=fit_LR_nocorr_roc, RF=fit_RF, SVM=fit_SVM))

summary(models_compare)
```

```{r}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
```

```{r}
models_compare_xgb <- resamples(list(XGB=fit_XGB, XGB.best=fit_XGB_best))

summary(models_compare_xgb)

```



```{r}
bwplot(models_compare_xgb, scales=scales)
```


`xgbDART` with hyper-parameters search is the best (as expected)


## ROC and AUC

### XGB

```{r}
library(ROCR)

#pred_prob_xgb <- predict(fit_XGB, type="prob")

# on testing data
pred_prob_xgb <- predict.train(fit_XGB, newdata = testing_all, type = "prob")

pred_roc_xgb <- prediction(predictions = pred_prob_xgb$S, labels = testing_all$resistance)

perf_roc_xgb <- performance(pred_roc_xgb, measure="tpr", x.measure = "fpr")

auc_xgb <- round(unlist(slot(performance(pred_roc_xgb, measure = "auc"), "y.values")), 3)
```


```{r}
# requires a separate code chunk

plot(perf_roc_xgb, main = "XGB ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_xgb))
```


### RF

```{r}
pred_prob_rf <- predict.train(fit_RF, newdata = testing_all, type="prob")

pred_roc_rf <- prediction(predictions = pred_prob_rf$S, labels = testing_all$resistance)

perf_roc_rf <- performance(pred_roc_rf, measure="tpr", x.measure = "fpr")

auc_rf <- round(unlist(slot(performance(pred_roc_rf, measure = "auc"), "y.values")), 3)

```

```{r}
# requires a separate code chunk

plot(perf_roc_rf, main = "RF ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_rf))
```


# Save the workspace

```{r, eval=TRUE}
save.image(f="/home/andrei/Data/HeteroR/notebooks/modelling.RData")
```
