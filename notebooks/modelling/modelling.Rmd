---
title: "Modelling"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F)
library(tidyverse)
library(caret)
library(car)
library(GGally)
```

# All models are wrong, but some are useful.

[George Box](https://r4ds.had.co.nz/model-basics.html)

# Read the data

Data set `features_amp_strain` has been made previously in `feature_engineering.Rmd`

Here I add BL genes counts as oredered factor and as binary features (if BL > 3)

```{r}
feat_amp <- read_csv("tables/features_amp_strain.csv")
feat_amp <- rename(feat_amp, n.plasmids="n_plasmids") %>% relocate(n.plasmids, .before="n.rep.total")

# create dummy from original target
feat_amp$resistance.dummy <- ifelse(feat_amp$resistance=="R", 1, 0)
feat_amp <- relocate(feat_amp, resistance.dummy, .before="n.beta.lac")

# BL counts as an ordered factor
feat_amp$n.beta.lac.f <- factor(feat_amp$n.beta.lac, ordered = T, levels = c(1,2,3,4,5,6))
feat_amp <- relocate(feat_amp, n.beta.lac.f, .before="n.plasmids")

# resistance as factor
feat_amp$resistance <- as.factor(feat_amp$resistance)

# BL counted as binary > 3
feat_amp$n.beta.lac.3 <- ifelse(feat_amp$n.beta.lac > 3, 1, 0)
feat_amp <- relocate(feat_amp, n.beta.lac.3, .before="n.plasmids")

feat_amp
```

# Pairs plot

One more time

```{r, fig.width=15, fig.height=10, warning=F, message=F}
# use GGally

ggpairs(feat_amp, 
        columns=c(4,5,6,7,9,10,15,16,22,23,25,27,28), 
        aes(color=resistance, alpha=0.2, dotsize=0.02), 
        upper = list(continuous = wrap("cor", size = 2.5)),
        diag=list(continuous ="barDiag"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")
```

# Regularized Logistic Regression

## Hand picked features

```{r}
feat_selected <- feat_amp %>% select(c(resistance, n.beta.lac.3, n.plasmids, med.rep.len, med.tot.rep.len, max.rep.len, n.rep.total, med.AR.len.cen))

# NAs in `med.tot.rep.len` and in `med.AR.len.cen` should be replaced with 0s
feat_selected$med.tot.rep.len <- ifelse(is.na(feat_selected$med.tot.rep.len), 0, feat_selected$med.tot.rep.len)
feat_selected$med.AR.len.cen <- ifelse(is.na(feat_selected$med.AR.len.cen), 0, feat_selected$med.AR.len.cen)

summary(feat_selected)
```

### Split the data set

```{r}
set.seed(100)

inTrain <- createDataPartition(y = feat_selected$resistance, p=0.7, list=FALSE)


training <- feat_selected[inTrain,]
testing <- feat_selected[-inTrain,]
```

### Params tuning

```{r}
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fitControl_acc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T)
```

### Training

Same model (3a) that won in "Statistical exploration" (see below)

#### ROC-optimized

```{r}
set.seed(100)

Fit1 <- train(resistance ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len, data = training, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit1
```

**Coefficients**

```{r}
Fit1$finalModel$W
```

#### Accuracy-optimized

```{r}
set.seed(100)

Fit1_acc <- train(resistance ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len, data = training, 
                 method = "regLogistic", 
                 trControl = fitControl_acc,
                 verbose = FALSE,
                 metric="Accuracy")
Fit1_acc
```


**Coefficients**

```{r}
Fit1_acc$finalModel$W
```

### Validation

#### ROC-optimized

```{r}

predClasses <- predict(Fit1, newdata=testing)

confusionMatrix(data = predClasses, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```

Not bad and similar to Model 3a

#### Accuracy-optimized

```{r}

predClasses_acc <- predict(Fit1_acc, newdata=testing)

confusionMatrix(data = predClasses_acc, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```

This model performs slightly worse than ROC-optimized one

## Automatic feature selection

Since LR doesn't have in-built feature selection, let's use *filter* methods (less prone to over-fitting and more computationally efficient than *wrapper* methods)

### Prepare the data set

remove strain, AB and resistance.dummy

```{r}
feat_amp <- select(feat_amp, -c(strain, AB, resistance.dummy))

skimr::skim_to_wide(feat_amp)
```

#### Data imputation

In this case all NAs can be replaced by 0s

`caret::preProcess()` doesn't have method for replacing with zeros...

```{r}
# make it by hand
feat_amp$n.rep.tot.cen <- ifelse(is.na(feat_amp$n.rep.tot.cen), 0, feat_amp$n.rep.tot.cen)

feat_amp$n.rep.tot.non.cen <- ifelse(is.na(feat_amp$n.rep.tot.non.cen), 0, feat_amp$n.rep.tot.non.cen)

feat_amp$med.tot.rep.len <- ifelse(is.na(feat_amp$med.tot.rep.len), 0, feat_amp$med.tot.rep.len)

feat_amp$med.AR.len.cen <- ifelse(is.na(feat_amp$med.AR.len.cen), 0, feat_amp$med.AR.len.cen)

feat_amp$ampC.med.tot.rep.len <- ifelse(is.na(feat_amp$ampC.med.tot.rep.len), 0, feat_amp$ampC.med.tot.rep.len)
```


### Correlated predictors

Summary of pairwise correlations between numeric predictors

```{r}
# exclude resistance, n.beta.lac.3, n.beta.lac.f
feat_amp_proc <- feat_amp[, c(2, 5:36)]

descrCor <- cor(feat_amp_proc)
summary(descrCor[upper.tri(descrCor)])
```

Remove those predictors that have correlation coefficient greater than 0.75

Check the summary of the new filtered correlations

```{r}
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

feat_amp_proc <- feat_amp_proc[,-highlyCorDescr]

# check new correlations
descrCor2 <- cor(feat_amp_proc)
summary(descrCor2[upper.tri(descrCor2)])
```

Add `resistance` and `n.beta.lac.3` columns to filtered columns

```{r}
feat_amp_proc$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_proc$resistance <- feat_amp$resistance
```

### Split the data set

```{r}
set.seed(101)

inTrain <- createDataPartition(feat_amp_proc$resistance, p=0.8, list=FALSE)


training <- feat_amp_proc[inTrain,]
testing <- feat_amp_proc[-inTrain,]
```

### Feature importance

**Box plots**

```{r}
featurePlot(x = training[, 1:20], 
            y = training$resistance, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

**Density curves**

```{r}
featurePlot(x = training[, 1:20], 
            y = training$resistance, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

### LR model on all non-correlated predictors

I remove only `n.beta.lac` and leave `n.beta.lac.3`

#### Training

```{r}
set.seed(100)

# fitControl is ROC optimized

Fit2 <- train(resistance ~ ., data = training[, c(2:22)], 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit2
```

It is better then Fit1 (ROC)

#### Validation

```{r}

predClasses <- predict(Fit2, newdata=testing)

confusionMatrix(data = predClasses, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```

Overall, Fit2 is better than Fit1

### Recursive feature selection


```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:5, 10, 15, 21)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE,
                   allowParallel = T)

# training[, 2:21] - 2:21 because I want to exclude n.beta.lac and leave only its binary counterpart n.beta.lac.3
lmProfile <- rfe(x=training[, 2:21], y=training$resistance,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile
```

### LR on top 5 best features


#### Data

Leave only the top 5 features from test and train data sets

```{r}
training_top5 <- select(training, c(resistance, n.beta.lac.3, max.rep.len.non.cen, med.AR.len.100, n.rep.100.non.cen, med.AR.len.non.cen.100))

testing_top5 <- select(testing, c(resistance, n.beta.lac.3, max.rep.len.non.cen, med.AR.len.100, n.rep.100.non.cen, med.AR.len.non.cen.100))
```


#### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

Fit3 <- train(resistance ~ ., data = training_top5, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit3
```

#### Validation

```{r}

predClasses <- predict(Fit3, newdata=testing_top5)

confusionMatrix(data = predClasses, 
                reference = testing_top5$resistance,
                mode="everything",
                positive="R")
```


### LR on 15 best features

including top 5

#### Data set

```{r}
training_top15 <- select(training, resistance, lmProfile$optVariables)

testing_top15 <- select(testing, resistance, lmProfile$optVariables)
```


#### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

Fit4 <- train(resistance ~ ., data = training_top15, 
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
Fit4
```

#### Validation

```{r}

predClasses <- predict(Fit4, newdata=testing_top15)

confusionMatrix(data = predClasses, 
                reference = testing_top15$resistance,
                mode="everything",
                positive="R")
```

## RLR conclusion

On validation data set Fit2 (all non-correlated features) and Fit3 (top 5) are the best two models

# Random Forest

Same `fitControl_roc` as earlier

on all non-correlated predictors

## Training

```{r}
#FitRF <- train(resistance ~ ., data=training_top5, method='rf', tuneLength=5, trControl = fitControl)
set.seed(100)

FitRF <- train(resistance ~ ., data = training[, c(2:22)], 
                 method = "rf", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

FitRF
```

## Validation

```{r}
predClasses <- predict(FitRF, newdata=testing)

confusionMatrix(data = predClasses, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```

Much better than RLR

# SVM

Same `fitControl_roc` as earlier

on all non-correlated predictors

## Training

```{r}
set.seed(100)

FitSVM <- train(resistance ~ ., data = training[, c(2:22)], 
                 method = "svmRadial", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

FitSVM

```

## Validation

```{r}
predClasses <- predict(FitSVM, newdata=testing)

confusionMatrix(data = predClasses, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```

## xgBoost Dart

### Training

```{r}
set.seed(100)

fit_xgbDART <- train(resistance ~ ., data = training[, c(2:22)], 
                 method = "xgbDART", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
fit_xgbDART
```


### Validation


```{r}
predClasses <- predict(fit_xgbDART, newdata=testing)

confusionMatrix(data = predClasses, 
                reference = testing$resistance,
                mode="everything",
                positive="R")
```


# Model comparison

```{r}
models_compare <- resamples(list(RLR1=Fit2, RLR2=Fit3, RF=FitRF, SVM=FitSVM, XGB=fit_xgbDART))

summary(models_compare)
```

```{r}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
```

`xgBoost` is the best (as expected)


# Variable importance

It is not available for RLR and SVM

## RF

```{r}
imp_vars_rf <- varImp(FitRF)

plot(imp_vars_rf, main="Variable Importance with RF")
```

## xgBoost

```{r}
imp_vars_xgb <- varImp(fit_xgbDART)

plot(imp_vars_xgb, main="Variable Importance with XGB")
```


----

# Statistical exploration of predictors (no caret)

## Select predictors

Selected variables (based on plots in `EDA.Rmd`)

```{r, eval=FALSE}
feat_selected <- feat_amp %>% select(c(strain, resistance, resistance_dummy, n.beta.lac, n.beta.lac.f, n.plasmids, med.rep.len, med.tot.rep.len, max.rep.len, n.rep.total, med.AR.len.cen))

summary(feat_selected)
```

Response (resistance) classes are almost equally represented in the data set.

```{r, fig.width=10, fig.height=10}
ggpairs(feat_selected, 
        columns=c(3:10), 
        aes(color=resistance, alpha=0.2, dotsize=0.02), 
        upper = list(continuous = wrap("cor", size = 2.5)),
        diag=list(continuous ="barDiag"))+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")
```

## Model 0: one constant predictor (null model)

```{r}
fit0 <- glm(formula = resistance_dummy ~ 1, data=feat_amp, family = binomial(link="logit"))

summary(fit0)
```


## Model 1: add BL-genes

Let's try to build a simple model and then run its diagnostics.

We assume that beta-lactamases is the main predictor of resistance (HR?). Let's check it out.

### BL genes number as discrete variable

```{r}
fit1a <- lm(formula = resistance_dummy ~ n.beta.lac, data=feat_amp, family = binomial(link="logit"))

summary(fit1a)

```

Credibility intervals:

```{r}
confint(fit1a)
```

Looks quite good: significant slope and intercept, F-statistic and CIs

### BL genes count as ordered factor

```{r}
# n.beta.lac+n.plasmids+med.rep.len+med.tot.rep.len+max.rep.len+n.rep.total+med.AR.len.cen
#prop.R <- (feat_amp %>% filter(resistance=="R") %>% nrow())/nrow(feat_amp)

fit1b <- glm(formula = resistance_dummy ~ n.beta.lac.f, data=feat_amp, family = binomial(link="logit"))

summary(fit1b)

```

None of the predictors is significant

### Recoding BL count to binary

But we know that chances a strain to be susceptible decrease sharply if number of BL genes in the strain is greater than 3

Let's code BL number in one binary variable 'greater than 3: yes or no'

```{r}
feat_amp$n.beta.lac.3 <- ifelse(feat_amp$n.beta.lac > 3, 1, 0)
feat_amp <- relocate(feat_amp, n.beta.lac.3, .before="n.plasmids")
```


```{r}
fit1c <- glm(formula = resistance_dummy ~ n.beta.lac.3, data=feat_amp, family = binomial(link="logit"))

summary(fit1c)
```

It looks much better!

### Models 0 and 1b comparison test

```{r}
anova(fit0, fit1c, test = "Chisq")
```

fit1c is better than fit0: 
  - residual deviance is lower, 
  - models are significantly different

## Model 2: add AR size

median AR size of repeat pairs should work well according to EDA.

### 2a: median AR size

```{r}
fit2a <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len, data = feat_amp, family = binomial(link = "logit"))

summary(fit2a)
```

```{r}
anova(fit0, fit1c, fit2a, test = "Chisq")
```

### 2b: median AR size of repeat pairs spanning gene center

for repeats spanning center it might be better but there are NAs. Replace them with 0

```{r}
feat_amp %>% select(resistance_dummy, n.beta.lac.f, med.AR.len.cen) %>% filter(is.na(med.AR.len.cen))
```

```{r}
feat_amp$med.AR.len.cen <- ifelse(is.na(feat_amp$med.AR.len.cen), 0, feat_amp$med.AR.len.cen)
```

```{r}
fit2b <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen, data = feat_amp, family = binomial(link = "logit"))

summary(fit2b)
```

```{r}
anova(fit0, fit1c, fit2b, test = "Chisq")
```

### 2c: median AR size of repeat pairs min 100 bp long

```{r}
fit2c <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.100, data = feat_amp, family = binomial(link = "logit"))

anova(fit2b, fit2c)
```

### 2d: median AR size of repeat pars min 100 bp long and spanning gene center

```{r}
fit2d <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen.100, data = feat_amp, family = binomial(link = "logit"))

anova(fit2b, fit2d)
```

filtered AR do not make the model better, plus they are correlated with med.AR.len.cen

```{r}
ggplot(feat_amp, aes(med.AR.len.cen, med.AR.len.cen.100))+
  geom_point(aes(color=resistance))
```


## Model 3: add maximum repeat length

### 3a: max len for all repeats

```{r}
fit3a <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len, data = feat_amp, family = binomial(link = "logit"))

summary(fit3a)
```

```{r}
anova(fit2b, fit3a, test = "Chisq")
```

It gets better!

### 3b: for repeats spanning center

```{r}
fit3b <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len.cen, data = feat_amp, family = binomial(link = "logit"))

summary(fit3b)
```

```{r}
anova(fit2b, fit3a, fit3b, test = "Chisq")
```

max.rep.len.cen doesn't decrease Res.Dev and the corresponding model is not signif. different.

## Model 4: add repeat counts

### 4a: total number of repeat pairs

```{r}
fit4a <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len + n.rep.total, data = feat_amp, family = binomial(link = "logit"))

summary(fit4a)
```

```{r}
anova(fit3a, fit4a, test="Chisq")
```

No better

### 4b: repeats not spanning center

```{r}
feat_amp$n.rep.tot.cen <- ifelse(is.na(feat_amp$n.rep.tot.cen), 0, feat_amp$n.rep.tot.cen)

fit4b <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len + n.rep.tot.cen, data = feat_amp, family = binomial(link = "logit"))

summary(fit4b)
```

```{r}
anova(fit3a, fit4b, test="Chisq")
```

Not that much different...

Model 3a is the best so far

## Model 5: add number of plasmids

```{r}
fit5 <- glm(formula = resistance_dummy ~ n.beta.lac.3 + med.AR.len.cen + max.rep.len + n.plasmids, data = feat_amp, family = binomial(link = "logit"))

summary(fit5)
```

Nope

## The winner is Model 3a

$resistance = 3.218 + 5.271n.beta.lac.3 - (2.109e-05)med.AR.len.cen + (1.870e-03)max.rep.len$

```{r}
confint(fit3a)
```


### Validation

#### TPR v FPR

```{r}
require(ROCR)

# predict prob
resistance.prob <- predict(object = fit3a, type = "response")
resistance.pred <- prediction(resistance.prob, feat_amp$resistance_dummy)

# calculate TPR and FPR
resistance.perf <- performance(resistance.pred ,"tpr","fpr")
plot(resistance.perf, colorize=T , xlab='false positive', ylab='true positive')
```

#### Precision, Recall, F-score

```{r, eval=FALSE}
precision <- posPredValue(resistance.pred, feat_amp$resistance_dummy, positive="1")
recall <- sensitivity(resistance.pred, y, positive="1")

F1 <- (2 * precision * recall) / (precision + recall)
```


#### AUC

```{r}
auc  <- performance(resistance.pred, measure = "auc")
accuracy <- performance(resistance.pred, measure = "acc")

print(c("AUC", auc@y.values[[1]]))
```

#### Accuracy

```{r}
plot(accuracy@x.values[[1]], accuracy@y.values[[1]], xlab=accuracy@x.name, ylab=accuracy@y.name)
```

Maximum accuracy `r max(accuracy@y.values[[1]])`

#### Spec, sens, acc

```{r}
perf3  <- performance(resistance.pred, x.measure = "cutoff", measure = "spec")
perf4  <- performance(resistance.pred, x.measure = "cutoff", measure = "sens")
perf5  <- performance(resistance.pred, x.measure = "cutoff", measure = "acc")

plot(perf3, col = "red", lwd =2, xlab='prob', ylab='spec')
plot(add=T, perf4 , col = "steelblue", lwd =2)
plot(add=T, perf5, lwd =2)

legend(x = 0.6,y = 0.3, c("spec", "sens", "acc"), 
       lty = 1, col =c('red', 'steelblue', 'black'), bty = 'n', cex = 1, lwd = 2)
# determine x coordinate manually!
#abline(v= 0.165, lwd = 2, lty=6)
```

### Diagnostics

```{r,  fig.width=6, fig.height=4, eval=F}
qqPlot(fit3a, labels=feat_amp$strain, id.method="identify", simulate=T, main="Q-Q plot")

```



```{r}
durbinWatsonTest(fit3a)
```

```{r}
crPlots(fit3a)
```

```{r, eval=F}
ncvTest(fit3a)
```


------------------------------------------------------------------------

# Save the workspace

```{r, eval=TRUE}
save.image(f="/home/andrei/Data/HeteroR/notebooks/modelling.RData")
```
