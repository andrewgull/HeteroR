---
title: "Modelling"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = T)
library(tidyverse)
library(caret)
library(ROCR)
```

# All models are wrong, but some are useful.

[George Box](https://r4ds.had.co.nz/model-basics.html)

# Read the data

Data set `features_amp_strain` has been made previously in `feature_engineering.Rmd`

Here I add BL genes counts as ordered factor and as binary features (if BL > 3)

```{r, cache=FALSE}
feat_amp <- read_csv("tables/features_amp_strain.csv", na = "NA")
feat_amp <- feat_amp %>% relocate(n.plasmids, .before="n.rep.total") %>% rename(n.ampC="ampC")

# create dummy from original target
#feat_amp$resistance.dummy <- ifelse(feat_amp$resistance=="R", 1, 0)
#feat_amp <- relocate(feat_amp, resistance.dummy, .before="n.beta.lac")

# BL counts as an ordered factor
#feat_amp$n.beta.lac.f <- factor(feat_amp$n.beta.lac, ordered = T, levels = c(1,2,3,4,5,6))
#feat_amp <- relocate(feat_amp, n.beta.lac.f, .before="n.plasmids")

# resistance as factor
feat_amp$resistance <- as.factor(feat_amp$resistance)

# BL counted as binary > 3
feat_amp$n.beta.lac.3 <- ifelse(feat_amp$n.beta.lac > 3, 1, 0)
feat_amp <- relocate(feat_amp, n.beta.lac.3, .before="n.plasmids")

# remove AB and strain
feat_amp <- feat_amp %>% select(-c(strain, AB))

feat_amp
```

# Descriptive stats

```{r}
skimr::skim(feat_amp)
```

There are 3 strains with NAs, we can replace NA s with 0s

# Data imputation

`caret::preProcess()` doesn't have method for replacing with zeros...

```{r, cache=FALSE}
# make it by hand
feat_amp$n.rep.tot.cen <- ifelse(is.na(feat_amp$n.rep.tot.cen), 0, feat_amp$n.rep.tot.cen)

feat_amp$n.rep.tot.non.cen <- ifelse(is.na(feat_amp$n.rep.tot.non.cen), 0, feat_amp$n.rep.tot.non.cen)

feat_amp$med.tot.rep.len <- ifelse(is.na(feat_amp$med.tot.rep.len), 0, feat_amp$med.tot.rep.len)

feat_amp$med.AR.len.cen <- ifelse(is.na(feat_amp$med.AR.len.cen), 0, feat_amp$med.AR.len.cen)

feat_amp$ampC.med.tot.rep.len <- ifelse(is.na(feat_amp$ampC.med.tot.rep.len), 0, feat_amp$ampC.med.tot.rep.len)
```

# Data normalization

scale and center numeric features (real numeric + count, not dummies!)

```{r, cache=FALSE}
end <- length(feat_amp)
feat_amp_scaled <- as_tibble(scale(feat_amp[,c(2,3,5:end)]))

feat_amp_scaled$resistance <- feat_amp$resistance
feat_amp_scaled$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_scaled <- relocate(feat_amp_scaled, n.beta.lac.3, .before="n.beta.lac")
feat_amp_scaled <- relocate(feat_amp_scaled, resistance, .before="n.beta.lac.3")
```


# Correlated predictors

## Corr plot

```{r, fig.width=12, fig.height=12, cache=FALSE}
cor_matrix <- cor(feat_amp[,c(2:end)], use = "pairwise.complete.obs")
corrplot::corrplot(cor_matrix, type="upper", tl.col = "black")
```

```{r, fig.width=12, fig.height=12, cache=FALSE}
cor_matrix_scaled <- cor(feat_amp[,c(2:end)], use = "pairwise.complete.obs")
corrplot::corrplot(cor_matrix_scaled, type="upper", method = "number", number.cex=0.8, number.digits=1, tl.col = "black")
```


## Identifying and removing correlated features

### Summary of pairwise correlations between numeric predictors (on scaled data)

```{r, cache=FALSE}
# exclude resistance, n.beta.lac.3
feat_amp_proc <- select(feat_amp_scaled, -c(resistance, n.beta.lac.3))

descr_cor <- cor(feat_amp_proc)
summary(descr_cor[upper.tri(descr_cor)])
```

### Variation among the correlation coefficients

```{r, fig.width=4, fig.height=4, cache=FALSE}
boxplot(descr_cor[upper.tri(descr_cor)])
```

Remove those predictors that have correlation coefficient greater than 0.75

### Summary of the new filtered correlations

```{r, cache=FALSE}
highly_cor <- findCorrelation(descr_cor, cutoff = .75)

feat_amp_proc <- feat_amp_proc[,-highly_cor]

# check new correlations
descr_cor2 <- cor(feat_amp_proc)
summary(descr_cor2[upper.tri(descr_cor2)])
```

### Variation among filtered correlation coefficients

```{r, fig.width=4, fig.height=4, cache=FALSE}
boxplot(descr_cor2[upper.tri(descr_cor2)])
```


Add `resistance` and `n.beta.lac.3` columns to filtered columns

```{r, cache=FALSE}
feat_amp_proc$n.beta.lac.3 <- feat_amp$n.beta.lac.3

feat_amp_proc$resistance <- feat_amp$resistance
```


## Conclusion

I will use non-correlated predictors with LR

With RF and XGB I will use all predictors + automatic feature selection


# Split the data set

## Data set with non-correlated predictors

```{r, cache=F}
set.seed(100)

in_train_nocorr <- createDataPartition(y = feat_amp_proc$resistance, p=0.75, list=FALSE)


training_nocorr <- feat_amp_proc[in_train_nocorr,]
testing_nocorr <- feat_amp_proc[-in_train_nocorr,]

# to avoid slowness of caret
x_nocorr <- as.matrix(training_nocorr[, -27])  # 27th column is 'resistance'
y_nocorr <- training_nocorr$resistance

```

## Data set with all predictors

```{r}
set.seed(100)

in_train_all <- createDataPartition(y = feat_amp_scaled$resistance, p=0.75, list=FALSE)


training_all <- feat_amp_scaled[in_train_all,]
testing_all <- feat_amp_scaled[-in_train_all,]

# to avoid slowness of caret
x_all <- as.matrix(training_all[,-1])  # except for resistance
y_all <- training_all$resistance

```

# Feature importance

### Box plots

```{r, fig.width=15, fig.height=12}
featurePlot(x = x_all, 
            y = y_all, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

### Density

```{r, fig.width=15, fig.height=12}
featurePlot(x = x_all, 
            y = y_all, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

Skewed predictors may be candidates for transformation (log)

## Best predictors

I will use them later along with the set of all predictors

```{r}
pred_best <- c("resistance", "n.beta.lac.3", "n.ampC", "med.dist.oriC", "n.genes.plus.strand", "n.genes.plasmids", "med.AR.len.cen", "max.rep.len", "n.rep.500", "med.rep.len.100")
```

## Data set with the best predictors

```{r}
training_best <- training_all %>% select(pred_best)
testing_best <- testing_all %>% select(pred_best)

x_best <- as.matrix(training_best[,-1])
y_best <- training_best$resistance
```


# Regularized Logistic Regression

## Hand picked features

I picked these features based on EDA

```{r}
feat_selected <- feat_amp_scaled %>% 
  select(c(resistance, n.beta.lac.3, n.ampC, n.plasmids, med.rep.len, med.tot.rep.len, max.rep.len, n.rep.total, med.AR.len.cen, med.dist.oriC, n.genes.plasmids))

# NAs in `med.tot.rep.len` and in `med.AR.len.cen` should be replaced with 0s
feat_selected$med.tot.rep.len <- ifelse(is.na(feat_selected$med.tot.rep.len), 0, feat_selected$med.tot.rep.len)
feat_selected$med.AR.len.cen <- ifelse(is.na(feat_selected$med.AR.len.cen), 0, feat_selected$med.AR.len.cen)

```

### Split hand picked dataset

```{r}
set.seed(100)

in_train_hand <- createDataPartition(y = feat_selected$resistance, p=0.75, list=FALSE)


training_hand <- feat_selected[in_train_hand,]
testing_hand <- feat_selected[-in_train_hand,]

# to avoid slowness of caret
x_hand <- as.matrix(training_hand[,-1])
y_hand <- training_hand$resistance

```

### Params tuning

```{r}
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fitControl_acc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T)
```

### Training and validation

Same model (3a) that won in "Statistical exploration" (see below and the html file compiled earlier)

#### ROC-optimized

```{r}
set.seed(100)

fit_LR_hand_roc <- train(x = x_hand, 
                         y = y_hand,
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")
fit_LR_hand_roc
```

**Resamples**

```{r}
fit_LR_hand_roc$resample %>% arrange(-ROC)
```

**confusion matrix on train**

```{r}
confusionMatrix.train(fit_LR_hand_roc)
```


#### Accuracy-optimized

```{r}
set.seed(100)

fit_LR_hand_acc <- train(x = x_hand,
                         y = y_hand, 
                 method = "regLogistic", 
                 trControl = fitControl_acc,
                 verbose = FALSE,
                 metric="Accuracy")
fit_LR_hand_acc
```


### Testing

#### ROC-optimized

```{r}

predClasses_RLR_hand_roc <- predict(fit_LR_hand_roc, newdata=testing_hand)

cm_RLR_hand_roc <- confusionMatrix(data = predClasses_RLR_hand_roc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")

cm_RLR_hand_roc
```

Not bad and similar to Model 3a

#### Accuracy-optimized

```{r}

predClasses_RLR_hand_acc <- predict(fit_LR_hand_acc, newdata=testing_hand)

cm_RLR_hand_acc <- confusionMatrix(data = predClasses_RLR_hand_acc, 
                reference = testing_hand$resistance,
                mode="everything",
                positive="R")

cm_RLR_hand_acc
```

This model performs a bit worse than ROC-optimized one

## Non-correlated predictors

Predictors chosen above in `Correlated predictors` section.

### Training and validation

```{r}
set.seed(100)

# fitControl is ROC optimized
fitControl_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

fit_LR_nocorr_roc <- train(x = x_nocorr,
                           y = y_nocorr,
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_LR_nocorr_roc
```

It's better then the model with hand picked predictors (ROC)


### Testing

```{r}

predClasses_RLR_nocorr_roc <- predict(fit_LR_nocorr_roc, newdata=testing_nocorr)

cm_RLR_nocorr_roc <- confusionMatrix(data = predClasses_RLR_nocorr_roc, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")

cm_RLR_nocorr_roc
```

Overall, all non-correlated predictors model is better than the hand-picked one

## Non-correlated predictors without n.beta.lac

...but with `n.beta.lac.3`

### Training and validation

```{r}
# remove n.beta.lac - column number 1
fit_LR_nocorr_roc2 <- train(x = x_nocorr[, -1],
                           y = y_nocorr,
                 method = "regLogistic", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_LR_nocorr_roc2
```

### Testing

```{r}
predClasses_RLR_nocorr_roc2 <- predict(fit_LR_nocorr_roc2, newdata=testing_nocorr[, -1])

cm_RLR_nocorr_roc2 <- confusionMatrix(data = predClasses_RLR_nocorr_roc2, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")

cm_RLR_nocorr_roc2
```

Apparently, `n.beta.lac` is very importnat in conjunction with other predictors

## LR on automatically selected features

### Recursive feature selection

```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:38)

rfe_ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE,
                   allowParallel = T, 
                   number = 10, 
                   p = 0.75)

# training[, -2] - because I want to exclude n.beta.lac and leave only its binary counterpart n.beta.lac.3
lm_profile <- rfe(x = x_all[,-2], 
                 y = y_all,
                 sizes = subsets,
                 rfeControl = rfe_ctrl, 
                 metric = "RMSE")

lm_profile
```

### Data

Leave only the top 5 features from test and train data sets

```{r}
top5 <- c("min.dist.oriC", "n.beta.lac.3", "n.genes.plasmids", "max.rep.len.cen", "med.AR.len.100")

training_top5 <- select(training_all, c(resistance, top5))

testing_top5 <- select(testing_all, c(resistance, top5))


x_top5 <- as.matrix(training_top5[,-1])
y_top5 <- training_top5$resistance
```


### Training

```{r}
# I use fitControl_roc from earlier

set.seed(100)

fit_LR_top5_roc <- train(x = x_top5,
                         y = y_top5,
                         method = "regLogistic", 
                         trControl = fitControl_roc,
                         verbose = FALSE,
                         metric="ROC")
fit_LR_top5_roc
```


### Testing

```{r}

predClasses_RLR_top5_roc <- predict(fit_LR_top5_roc, newdata=testing_top5)

cm_RLR_top5_roc <- confusionMatrix(data = predClasses_RLR_top5_roc, 
                reference = testing_top5$resistance,
                mode="everything",
                positive="R")

cm_RLR_top5_roc
```

## RLR conclusion

On testing data set, model with all non-correlated features is the best model.

# Random Forest

Same `fitControl_roc` as earlier

on all predictors because RF has automatic feature selection

## Params tuning

```{r}
fit_ctrl_rf_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)

#mtry <- sqrt(ncol(training_all[,2:39]))
#tunegrid <- expand.grid(.mtry=mtry)
```

## Data set with all predictors

### Training and validation

```{r}
set.seed(100)

fit_RF <- train(x = x_all,
                y = y_all,  
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF
```

### Predictors importance

```{r, fig.width=8, fig.height=6}
imp_vars_rf_all <- varImp(fit_RF)

plot(imp_vars_rf_all, main="Variable Importance with RF (all)")
```


### Testing

```{r}
predClasses_RF_all <- predict(fit_RF, newdata=testing_all)

cm_RF_all <- confusionMatrix(data = predClasses_RF_all, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")

cm_RF_all
```

## Data set with all predictors, except n.ampC (old style)

### Training and validation

```{r}
set.seed(100)

# without n.ampC
fit_RF_no_n.ampC <- train(x = x_all[, -3],
                y = y_all,  
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF_no_n.ampC
```


### Feature importance

```{r, fig.width=8, fig.height=6}
imp_vars_rf_no_n.ampC <- varImp(fit_RF_no_n.ampC)

plot(imp_vars_rf_no_n.ampC, main="Variable Importance with RF (no n.ampC)")

```


### Testing

```{r}
# without n.ampC
predClasses_RF_no_n.ampC <- predict(fit_RF_no_n.ampC, newdata=testing_all[, -4])

cm_RF_no_n.ampC <- confusionMatrix(data = predClasses_RF_no_n.ampC, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")

cm_RF_no_n.ampC
```

RF is better without `n.ampC`


## Data set with the best predictors

### Training and validation

```{r}
set.seed(100)

fit_RF_best <- train(x_best, y_best, 
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF_best
```

### Predictors importance

```{r}
imp_vars_rf_best <- varImp(fit_RF_best)

plot(imp_vars_rf_best, main="Variable Importance with RF (manual best)")
```

Again, `n.ampC` doesn't look important at all.

### Testing

```{r}
predClasses_RF_best <- predict(fit_RF_best, newdata=testing_best)

cm_RF_best <- confusionMatrix(data = predClasses_RF_best, 
                reference = testing_best$resistance,
                mode="everything",
                positive="R")

cm_RF_best
```

## Data set with non-correlated predictors

### Training and validation

```{r}
set.seed(100)

fit_RF_nocorr <- train(x_nocorr, y_nocorr, 
                 method = "rf", 
                 trControl = fit_ctrl_rf_roc,
                 #tuneGrid=tunegrid,
                 verbose = FALSE,
                 metric="ROC")

fit_RF_nocorr
```


### Predictors importance

```{r}
imp_vars_rf_nocorr <- varImp(fit_RF_nocorr)

plot(imp_vars_rf_nocorr, main="Variable Importance with RF (non-correlated)")
```


### Testing

```{r}
predClasses_RF_nocorr <- predict(fit_RF_nocorr, newdata=testing_nocorr)

cm_RF_nocorr <- confusionMatrix(data = predClasses_RF_nocorr, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")

cm_RF_nocorr
```


# SVM

Same `fitControl_roc` as earlier

## All non-correlated predictors

### Training and validation

```{r}
set.seed(100)

fit_SVM_all <- train(resistance ~ ., data = training_all,
                 method = "svmRadial", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_SVM_all
```

### Testing

```{r}
predClasses_SVM_all <- predict(fit_SVM_all, newdata=testing_all)

cm_SVM_all <- confusionMatrix(data = predClasses_SVM_all, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")

cm_SVM_all
```

## Non-correlated predictors

### Training and validation

```{r}
set.seed(100)

fit_SVM_nocorr <- train(resistance ~ ., data = training_nocorr, 
                 method = "svmRadial", 
                 trControl = fitControl_roc,
                 verbose = FALSE,
                 metric="ROC")

fit_SVM_nocorr
```


### Testing

```{r}
predClasses_SVM_nocorr <- predict(fit_SVM_nocorr, newdata=testing_nocorr)

cm_SVM_nocorr <- confusionMatrix(data = predClasses_SVM_nocorr, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")

cm_SVM_nocorr
```


# xgBoost

DART variant

## Params tuning

5 repeats instead of 10, to make things faster

```{r}
fit_ctrl_xgb_roc <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           repeats = 5, 
                           allowParallel = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```


## Data set with all predictors

### Training and validation

```{r}
set.seed(100)

fit_XGB_all <- train(x = x_all, y = y_all, 
                 method = "xgbDART", 
                 trControl = fit_ctrl_xgb_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)
fit_XGB_all$bestTune
```

#### Best model

```{r}
filter(fit_XGB_all$results, nrounds == 150, max_depth == 1, eta == 0.3, gamma == 0, subsample == 0.75, colsample_bytree == 0.6, rate_drop == 0.01, skip_drop == 0.95, min_child_weight == 1)
```

### Features importance

```{r}
imp_vars_xgb_all <- varImp(fit_XGB_all)

plot(imp_vars_xgb_all, main="Variable Importance with XGB (all)")
```

### Testing

```{r}
predClasses_XGB_all <- predict(fit_XGB_all, newdata=testing_all)

cm_XGB_all <- confusionMatrix(data = predClasses_XGB_all, 
                reference = testing_all$resistance,
                mode="everything",
                positive="R")

cm_XGB_all
```

## Data set with non-correlated predictors

### Training and validation

```{r}
set.seed(100)

fit_XGB_nocorr <- train(x = x_nocorr, y = y_nocorr, 
                 method = "xgbDART", 
                 trControl = fit_ctrl_xgb_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)

fit_XGB_nocorr$bestTune
```

### Variable importance

```{r}
imp_vars_xgb_nocorr <- varImp(fit_XGB_nocorr)

plot(imp_vars_xgb_nocorr, main="Variable Importance with XGB (nocorr)")
```


### Testing

```{r}
predClasses_XGB_nocorr <- predict(fit_XGB_nocorr, newdata=testing_nocorr)

cm_XGB_nocorr <- confusionMatrix(data = predClasses_XGB_nocorr, 
                reference = testing_nocorr$resistance,
                mode="everything",
                positive="R")

cm_XGB_nocorr
```


## Data set with best predictors

### Training and validation

Scaled and centered train data set!

```{r}
set.seed(100)

#x_best_scaled <- scale(x_best)

fit_XGB_best <- train(x=x_best, y=y_best, 
                 method = "xgbDART", 
                 trControl = fit_ctrl_xgb_roc,
                 verbose = FALSE,
                 metric="ROC",
                 verbosity=0)

fit_XGB_best$bestTune
```


### Features importance


```{r}
imp_vars_xgb_best <- varImp(fit_XGB_best)

plot(imp_vars_xgb_best, main="Variable Importance with XGB (best)")
```

### Testing

```{r}
predClasses_XGB_best <- predict(fit_XGB_best, newdata=testing_best)

cm_XGB_best <- confusionMatrix(data = predClasses_XGB_best, 
                reference = testing_best$resistance,
                mode="everything",
                positive="R")

cm_XGB_best
```

# Model comparison

## Performance metrics on testing data set

The table will be written to file `tables/metrics.csv`

```{r, eval=T}
# take results from confusionMatrix objects
cm_list <- list(cm_RLR_hand_roc, cm_RLR_nocorr_roc, cm_RLR_nocorr_roc2, cm_RLR_top5_roc, cm_RF_all, cm_RF_no_n.ampC, cm_RF_best, cm_RF_nocorr, cm_SVM_all, cm_SVM_nocorr, cm_XGB_all, cm_XGB_best, cm_XGB_nocorr)


perf_metrics <- as.data.frame(
  sapply(cm_list, function(cm){
    round(c(cm$overall[1], cm$overall[2], cm$byClass[1], cm$byClass[2], cm$byClass[7]), 3)
  })
)

# calculate AUC

# quite stupid way
fit_list <- list(fit_LR_hand_roc, fit_LR_nocorr_roc, fit_LR_nocorr_roc2, fit_LR_top5_roc, fit_RF, fit_RF_no_n.ampC, fit_RF_best, fit_RF_nocorr, fit_SVM_all, fit_SVM_nocorr, fit_XGB_all, fit_XGB_best, fit_XGB_nocorr)

testing_list <- list(testing_hand, testing_nocorr, testing_nocorr[, -1], testing_top5, testing_all, testing_all[, -4], testing_best, testing_nocorr, testing_all, testing_nocorr, testing_all, testing_best, testing_nocorr)

# iterate over both lists
auc_vec <- sapply(c(1:length(fit_list)), function(i){
  #print(i)
  f <- fit_list[[i]]
  t <- testing_list[[i]]
  
  pred_prob <- predict.train(f, newdata = t, type = "prob")
  pred_roc <- ROCR::prediction(predictions = pred_prob$S, labels = t$resistance)
  auc <- round(unlist(slot(ROCR::performance(pred_roc, measure = "auc"), "y.values")), 3)
  
  return(auc)
})

# make it 1 row
auc_df <- as.data.frame(t(data.frame(AUC=auc_vec)))
# add to other metrics
perf_metrics <- bind_rows(perf_metrics, auc_df)
# make Metric column
perf_metrics$Metric <- row.names(perf_metrics)
perf_metrics <- as_tibble(perf_metrics) %>% relocate(Metric, .before = V1)
# make other columns names
colnames(perf_metrics) <- c("Metric", "RLR.hand", "RLR.nocorr", "RLR.nocorr.2", "RLR.top5.auto", "RF.all", "RF.all.no.n.ampC", "RF.best", "RF.nocorr", "SVM.all", "SVM.nocorr", "XGB.all", "XGB.best", "XGB.nocorr")
# write to file
write.csv(perf_metrics, "tables/metrics.csv", row.names = FALSE)

# show
perf_metrics

```


## During training and validation

This resampling data comes from training and validation, so they reflect model search not the average model performance.

```{r}
models_compare <- resamples(list(
  RLR_hand = fit_LR_hand_roc, 
  RLR_nocorr = fit_LR_nocorr_roc, 
  RLR_nocorr2 = fit_LR_nocorr_roc2, 
  RLR_top5 = fit_LR_top5_roc, 
  RF_all = fit_RF, 
  RF_best = fit_RF_best, 
  RF_noAMPC = fit_RF_no_n.ampC, 
  RF_nocorr = fit_RF_nocorr,
  SVM_all = fit_SVM_all, 
  SVM_nocorr = fit_SVM_nocorr
  ))

summary(models_compare)
```

```{r, fig.width=8, fig.height=6}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
```

XGB should be compared separately because of difrent number of resampling rounds

```{r}
models_compare_xgb <- resamples(list(
  XGB_all = fit_XGB_all, 
  XGB_best = fit_XGB_best,
  XGB_nocorr = fit_XGB_nocorr
))

summary(models_compare_xgb)
```



```{r, fig.width=8, fig.height=4}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare_xgb, scales=scales)
```



`xgbDART` and `RLR.nocorr` are the best (as expected)


## ROC and AUC

Compare ROC shape of the best models

Function to get ROC

```{r}
get_roc <- function(fit.obj, testing.df){
  pred_prob <- predict.train(fit.obj, newdata = testing.df, type="prob")
  pred_roc <- prediction(predictions = pred_prob$S, labels = testing.df$resistance)
  perf_roc <- performance(pred_roc, measure="tpr", x.measure = "fpr")
  return(perf_roc)
}
```


### RLR.nocorr

```{r}
# calculate ROC
perf_roc_rlr <- get_roc(fit_LR_nocorr_roc, testing_nocorr)

# take AUC from perf_metrics
auc_rlr <- unlist(perf_metrics %>% filter(Metric=="AUC") %>% select(RLR.nocorr))

# plot
plot(perf_roc_rlr, main = "RLR ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_rlr))
```


### XGB.all

```{r}
# calculate ROC
perf_roc_xgb <- get_roc(fit_XGB_all, testing_all)

# take AUC from perf_metrics
auc_xgb <- unlist(perf_metrics %>% filter(Metric=="AUC") %>% select(XGB.all))

# plot
plot(perf_roc_xgb, main = "XGB.all ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_xgb))
```

### XGB.nocorr

```{r}
# calculate ROC
perf_roc_xgb_nocorr <- get_roc(fit_XGB_nocorr, testing_nocorr)

# take AUC from perf_metrics
auc_xgb_nocorr <- unlist(perf_metrics %>% filter(Metric=="AUC") %>% select(XGB.nocorr))

# plot
plot(perf_roc_xgb_nocorr, main = "XGB.nocorr ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_xgb_nocorr))
```

### RF.nocorr

```{r}
# calculate ROC
perf_roc_rf <- get_roc(fit_RF_nocorr, testing_nocorr)

auc_rf <- unlist(perf_metrics %>% filter(Metric=="AUC") %>% select(RF.nocorr))

# plot
plot(perf_roc_rf, main = "RF ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_rf))
```

### SVM.nocorr

```{r}
# calculate ROC
perf_roc_svm <- get_roc(fit_SVM_nocorr, testing_nocorr)

auc_svm <- unlist(perf_metrics %>% filter(Metric=="AUC") %>% select(SVM.nocorr))

# plot
plot(perf_roc_svm, main = "SVM ROC curve", col = "steelblue", lwd = 3)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)
legend(x = 0.8, y = 0.3, legend = paste0("AUC = ", auc_svm))
```


# Save the workspace

```{r, eval=TRUE}
save.image(f="/home/andrei/Data/HeteroR/notebooks/modelling.RData")
```
