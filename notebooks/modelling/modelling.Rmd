---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
#library(embed) # for UMAP
library(finetune)

# path for models
models_path <- "~/Data/HeteroR/notebooks/models/scheme12/"
models_path_cv5 <- "~/Data/HeteroR/notebooks/models/scheme123/folds5/"
```

# Read and process data

The data sets are the same as in EDA

```{r}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf"))

# TWO SCHEMES
hr_testing12 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr12)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr12 )

hr_testing13 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr13)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr13 )

# created during EDA
hr_testing <- read_csv("data/heteroresistance_testing_gr123.csv") 

# USE HR 1+2+3
data_strain <- data_strain %>% 
  left_join(hr_testing12, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \> 3 and \>4

```{r}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.3, n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Basic Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe %>% prep() %>% juice()
```

Target class:

```{r}
main_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```

## PCA + ORQ-normalization

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

For PCA

Does ORQ-normalization `step_orderNorm()` really help?

If you look at EDA, you'll see that ORQ gives better distributions and
less PCs to explain the same amount of variance, than regular
`step_normalize()`. (Same is true for this recipe below)

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  update_role(resistance, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  #step_normalize(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_pca(all_predictors(), threshold = .9)
  

pca_recipe %>% prep() %>% juice()
```

## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No correlation + ORQ-norm recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r}
ncorq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No corr + Spline basis expansion

to incorporate non-linearity into the class boundary

```{r}
ns_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_ns(all_predictors(), deg_free = tune()) %>% 
  step_corr(threshold = tune("corr_tune")) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to une hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Functions to simplify things

To make model creation less repetitive

```{r}

## MAKE MODEL SPECIFICATION ##

set_model <- function(mod, cores) {
  if (mod == "lr") {
    my_mod <- logistic_reg(
        penalty = tune(), 
        mixture = 1) %>% 
      set_engine("glmnet")
  } else if (mod == "mars") {
    my_mod <- mars(
      mode = "classification",
      engine = "earth",
      num_terms = tune(),
      prod_degree = tune(),
      prune_method = "backward") %>% 
      translate()
  } else if(mod == "svm") {
    my_mod <- svm_linear(
        cost = tune()) %>% # margin - for regression only
      set_mode("classification") %>%
      set_engine("kernlab")  # default
  } else if(mod == "rf") {
    my_mod <- rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
      set_engine("ranger", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "bt") {
    my_mod <- boost_tree(
      trees = 50, 
      mtry = tune(), 
      min_n = tune(), 
      tree_depth = tune(), 
      learn_rate = tune(), 
      loss_reduction = tune(), 
      sample_size = tune(), 
      stop_iter = tune()) %>% 
      set_engine("xgboost", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "knn") {
    my_mod <- nearest_neighbor(
      neighbors = tune(),
      weight_func = tune(),
      dist_power = tune()) %>%
      set_engine("kknn") %>%
      set_mode("classification")
  } else if (mod == "nnet"){
    my_mod <-
      mlp(hidden_units = tune(), 
          penalty = tune(), 
          epochs = tune()) %>%
      set_mode("classification") %>%
      set_engine("nnet", num.threads = cores)
  }
  return(my_mod)
}

## SET WORKFLOW ##

set_wf <- function(mod, rec, cores){
  # mod: model type, one of: lr, knn, mars, svm, rf, bt
  # rec: recipe object (one of: main, ncorr, pca, umap)
  # rec must be in GlobalEnv
  
  if (rec == "main"){
    rc <- main_recipe
  } else if (rec == "ncorr"){
    rc <- ncorr_recipe
  } else if (rec == "pca") {
    rc <- pca_recipe
  } else if (rec == "umap") {
    rc <- umap_recipe
  } else {
    print("ERROR! Undefined recipe!")
  }

  wf <- workflow() %>% 
    add_model(set_model(mod = mod, cores = cores)) %>% 
    add_recipe(rc)
  
  return(wf)
}

## MK ROC OBJECT ##
# for autoplot()

make_roc <- function(mod_res, title){
  
  mod_best <- mod_res %>%
    select_best(metric = "roc_auc")
  
  mod_auc <- mod_res %>% 
    collect_predictions(parameters = mod_best) %>% 
    roc_curve(resistance, .pred_HR) %>% 
    mutate(model = title)
  
  return(mod_auc)
}

# Also, there is a function 'run_models.R' to run resampling from terminal
```

# Tune race - TEST

```{r}
library(finetune)

test_model <- logistic_reg(penalty = tune(),
                           mixture = 1) %>%
  set_engine("glmnet")

test_wf <- workflow() %>%
  add_model(test_model) %>%
  add_recipe(ncorr_recipe)

grid_wl <- test_wf %>%
  tune_race_win_loss(
    resamples = cv_folds,
    grid = 20,
    control = control_race(verbose_elim = TRUE, save_pred = TRUE, burn_in = 50)
  )

show_best(grid_wl, metric = "roc_auc")
```

```{r}
plot_race(grid_wl)
```

```{r}
make_roc(grid_wl, "") %>% autoplot()
```

```{r}
bres_test <- test_wf %>% 
  tune_bayes(
    resamples = cv_folds,
    param_info = extract_parameter_set_dials(test_wf),
    initial = grid_wl,
    iter = 30,
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 10, 
                            verbose = FALSE, 
                            save_pred = TRUE, 
                            save_workflow = TRUE))

bres_test %>% show_best("roc_auc")
```

```{r}
make_roc(bres_test, "") %>% autoplot()
```

# Penalized LR

## UMAP

If you want to use UMAP instead of real features, then you should not
use supervised UMAP.

Also, (supervised) UMAP recipe dumps core for whatever reason

## NCORR recipe

For LR I used space filling grid, cause Bayesian would cause errors
during resampling and the advantage it gives is marginal.

You can load an `.rds` file if it exists.

```{r, fig.width=10}
# correlation threshold was tuned as well
lr_ncorr_res <- readRDS(paste0(models_path, "lr_ncorr_rs20_corr_tune.rds"))

autoplot(lr_ncorr_res)
```

### ROC of resamples

```{r}
lr_ncorr_res %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Gain curve

```{r}
lr_ncorr_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```


### Best models

```{r}
lr_ncorr_res %>% 
  show_best("roc_auc", n = 5) 
```

### Best ROC

```{r}
make_roc(lr_ncorr_res, "") %>% 
  autoplot() + ggtitle("LR (norm)")
```

## NCORQ recipe

New ncorq with dummies after ORQ doesn't make any changes

```{r}
# tuned corr theshold
lr_ncorq_res <- readRDS(paste0(models_path, "lr_ncorq_rs20_corr_tune.rds"))

lr_ncorq_res %>% autoplot()
```

### Gain curve

```{r}
lr_ncorq_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

The gain curve looks better than the one with regular 'no corr' recipe

### Best models

```{r}
lr_ncorq_res %>%
  show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(lr_ncorq_res, "") %>% 
  autoplot() + ggtitle("LR (ORQ)")
```

Very close, but not better than regular 'no corr' recipe

## PCA recipe

Space-filling grid as well, 0.9 variance is set to be kept during PCA


```{r, fig.width = 10}
lr_pca_res <- readRDS(paste0(models_path, "lr_pca_rs20.rds"))


autoplot(lr_pca_res)
```

### Gain curve

```{r}
lr_pca_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

Much better gain curve so far

### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(lr_pca_res, "") %>% 
  autoplot() + ggtitle("LR (PCA)")
```



## NS recipe

```{r}
lr_ns <- logistic_reg(penalty = tune(),
                      mixture = 1) %>%
  set_engine("glmnet")

ns_wf <- workflow() %>% 
  add_model(lr_ns) %>% 
  add_recipe(ns_recipe)
  
ns_res <- ns_wf %>%
  tune_grid(
    grid = 20,
    resamples = cv_folds,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

ns_res_wl <- ns_wf %>%
  tune_race_win_loss(
    resamples = cv_folds,
    grid = 20,
    control = control_race(verbose_elim = TRUE, save_pred = TRUE, burn_in = 10)
  )
```


## Recursive Feature Elimination

Doesn't work

```{r}
# devtools::install_github("stevenpawley/recipeselectors")
library(recipeselectors)

rfe_model <- rand_forest(mode = "classification") %>% 
  set_engine("ranger", num.threads = 8)

rfe_rec <- recipe(resistance ~., data = df_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_select_vip(all_numeric_predictors(), outcomme = "resistance", model = rfe_model, threshold = .9) 

rfe_rec %>% 
  prep() %>% 
  juice()
```

## Compare the best models

*post hoc* analysis of resampling results generated by models - via Bayesian ANOVA

### Generate distributions

```{r}
library(tidyposterior)
# Bayesian ANOVA

# function to get a name of the best preprocessor from your resample
best_preprocessor <- function(res_obj) {
  res_obj %>% select_best("roc_auc") %>% pull(.config)
}

# function to make a df with AUCs from the best preproc
best_aucs <- function(res_obj, model_name){
  res_obj %>%
    collect_metrics(summarize = FALSE) %>%
    filter(.metric == "roc_auc", .config == best_preprocessor(res_obj)) %>%
    select(id, id2, {{model_name}} := .estimate)
}

mod_comp <-
  map2(list(lr_ncorr_res, lr_ncorq_res, lr_pca_res),
       c("norm", "orq", "pca"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "orq" = .y.y, "pca" = .y) %>% 
  unite(id, id, id2)

mod_posterior <- perf_mod(mod_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(mod_posterior)

```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(mod_posterior, seed = 100) 

summary(preproc_diff)
```
Columns `upper` and `lower` represent credible interval for the mean difference between preprocessing's AUCs.

Positive mean value means that mean of norm > mean of orq etc.

If CI doesn't include 0, then the difference is significant.

In this case standard normalization is significantly better than the ordered quantile technique.

The ordered quantile normalization is significantly better than PCA recipe.

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r, fig.width=8}
mars_ncorr_res <- readRDS(paste0(models_path, "mars_ncorr_rs20_corr_tune.rds"))

autoplot(mars_ncorr_res)
```

This plot suggests that greater number of model terms can give greater AUC and J-index

### Gain curve

```{r}
mars_ncorr_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

### Best ROC

```{r}
make_roc(mars_ncorr_res, "") %>% 
  autoplot() + ggtitle("MARS")
```

## PCA recipe

```{r, fig.width=8}
mars_pca_res <- readRDS(paste0(models_path, "mars_pca_rs20_corr_tune.rds"))

autoplot(mars_pca_res)
```

The same picture here: more model terms should lead to better AUC

### Gain curve

```{r}
mars_pca_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
mars_pca_res %>% 
  show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(mars_pca_res, "") %>% 
  autoplot() + ggtitle("MARS (PCA)")
```

## NCORQ recipe

```{r}
mars_ncorq_res <- readRDS(paste0(models_path, "mars_ncorq_rs20_corr_tune.rds"))

autoplot(mars_ncorq_res)
```

### Best models

```{r}
mars_ncorq_res %>% show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(mars_ncorq_res, "") %>% 
  autoplot() + ggtitle("MARS (ORQ)")
```


## Comparison of the preprocessing steps

```{r}
mars_comp <-
  map2(list(mars_ncorr_res, mars_pca_res, mars_ncorq_res),
       c("norm", "pca", "orq"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "pca" = .y.y, "orq" = .y) %>% 
  unite(id, id, id2)

mars_posterior <- perf_mod(mars_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(mars_posterior)
```

```{r}
mars_preproc_diff <- contrast_models(mars_posterior, seed = 100) 

summary(mars_preproc_diff)
```

ORQ is significantly worse than the rest.

The difference between standard and PCA is insignificant

# Linear support vector machines (lSVM)

## NCORR recipe

Correlation threshold was tuned.

```{r}
lsvm_ncorr_res <- readRDS(paste0(models_path, "lsvm_ncorr_rs20_corr_tune.rds"))

autoplot(lsvm_ncorr_res)
```

### Gain curve

```{r}
lsvm_ncorr_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(lsvm_ncorr_res, "") %>% 
  autoplot() + ggtitle("linSVM")
```

## NCORQ recipe

```{r}
lsvm_ncorq_res <- readRDS(paste0(models_path, "lsvm_ncorq_rs20_corr_tune.rds"))

autoplot(lsvm_ncorq_res)
```

### Gain urve

```{r}
lsvm_ncorq_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(lsvm_ncorq_res, "") %>% 
  autoplot() + ggtitle("linSVM (ORQ)")
```


## PCA recipe

```{r}
lsvm_pca_res <- readRDS(paste0(models_path, "lsvm_pca_rs20_corr_tune.rds"))

autoplot(lsvm_pca_res)
```

### Gain curve

```{r}
lsvm_pca_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
lsvm_pca_res %>% 
  show_best(metric = "roc_auc")
```

### Best ROC

```{r}
make_roc(lsvm_pca_res, "") %>% 
  autoplot() + ggtitle("linSVM (PCA)")
```

## Comparison of the preprocessors

```{r}

lsvm_comp <-
  map2(list(lsvm_ncorr_res, lsvm_pca_res, lsvm_ncorq_res),
       c("norm", "pca", "orq"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "pca" = .y.y, "orq" = .y) %>% 
  unite(id, id, id2)

lsvm_posterior <- perf_mod(lsvm_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(lsvm_posterior)
```

PCA clearly sucks

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

PCA is significantly worse than the rest.

ORQ is no different form standard normalization.

# Polynomial support vector machines (pSVM)

## NCORR recipe

```{r, fig.width=10, fig.height=6}
psvm_ncorr_res <- readRDS(paste0(models_path, "psvm_ncorr_rs20_corr_tune.rds"))

autoplot(psvm_ncorr_res)
```

### Gain curve

```{r}
psvm_ncorr_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(psvm_ncorr_res, "") %>% 
  autoplot() + ggtitle("polySVM")
```

## NCORQ recipe

```{r, fig.width=10, fig.height=6}
psvm_ncorq_res <- readRDS(paste0(models_path, "psvm_ncorq_rs20_corr_tune.rds"))

autoplot(psvm_ncorq_res)
```
### Gain curve

```{r}
psvm_ncorq_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(psvm_ncorq_res, "") %>% 
  autoplot() + ggtitle("polySVM")
```

## PCA recipe

```{r, fig.width=10, fig.height=6}
psvm_pca_res <- readRDS(paste0(models_path, "psvm_pca_rs20_corr_tune.rds"))

autoplot(psvm_pca_res)
```

### Gain curve

```{r}
psvm_pca_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(psvm_pca_res, "") %>% 
  autoplot() + ggtitle("polySVM")
```

## Comparison of the preprocessing steps

```{r}
psvm_comp <-
  map2(list(psvm_ncorr_res, psvm_ncorq_res, psvm_pca_res),
       c("norm", "orq", "pca"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "pca" = .y.y, "orq" = .y) %>% 
  unite(id, id, id2)

psvm_posterior <- perf_mod(psvm_comp, iter = 4000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(psvm_posterior)
```

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```

It seems, the difference is not significant even between norm and orq just because it's too small

# K-nearest neighbors (KNN)

## NCORR recipe 

```{r, fig.width = 12, fig.height=6}
knn_ncorr_res <- readRDS(paste0(models_path, "knn_ncorr_rs20_corr_tune.rds"))

knn_ncorr_res %>% 
  autoplot()
```

### Best models

```{r}
knn_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### Gain curve

```{r}
knn_ncorr_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best ROC

```{r}
make_roc(knn_ncorr_res, "") %>% 
  autoplot()
```

## NCORQ recipe

```{r, fig.width = 12, fig.height=6}
knn_ncorq_res <- readRDS(paste0(models_path, "knn_ncorq_rs20_corr_tune.rds"))

knn_ncorq_res %>% 
  autoplot()
```
### Best models

```{r}
knn_ncorq_res %>% 
  show_best("roc_auc", n = 5)
```

### Gain curve

```{r}
knn_ncorq_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best ROC

```{r}
make_roc(knn_ncorq_res, "") %>% 
  autoplot()
```

## PCA recipe


```{r, fig.width = 12, fig.height=6}
knn_pca_res <- readRDS(paste0(models_path, "knn_pca_rs20_corr_tune.rds"))

knn_pca_res %>% 
  autoplot()
```
### Best models

```{r}
knn_pca_res %>% 
  show_best("roc_auc", n = 5)
```

### Gain curve

```{r}
knn_pca_res %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best ROC

```{r}
make_roc(knn_pca_res, "") %>% 
  autoplot()
```

## Comparison of the preprocessors

```{r}
knn_comp <-
  map2(list(knn_ncorr_res, knn_pca_res, knn_ncorq_res),
       c("norm", "pca", "orq"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "pca" = .y.y, "orq" = .y) %>% 
  unite(id, id, id2)

knn_posterior <- perf_mod(knn_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(knn_posterior)
```

```{r}
knn_preproc_diff <- contrast_models(knn_posterior, seed = 100) 

summary(knn_preproc_diff)
```

ORQ recipe worked clearly better

# Naive Bayes

```{r}
nb_ncorr <- readRDS(paste0(models_path, "nb_ncorr.rds"))

nb_ncorr %>% autoplot()
```

```{r}
make_roc(nb_ncorr, "") %>% autoplot()
```

```{r}
nb_ncorr %>% show_best("roc_auc", n = 5)
```

# Random Forest (RF)

## Main recipe

```{r}
rf_main_res <- readRDS(paste0(models_path, "rf_main_rs20.rds"))

rf_main_res %>% 
  autoplot()
```

### Gain curve

```{r}
rf_main_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
rf_main_res %>% 
  show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(rf_main_res, "") %>% 
  autoplot() + ggtitle("RF (all predictors)")
```

## NCORR recipe

```{r}
rf_ncorr_res <- readRDS(paste0(models_path, "rf_ncorr_rs20_corr_tune.rds"))

rf_ncorr_res %>% autoplot()
```

### Gain curve

```{r}
rf_ncorr_res %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
rf_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(rf_ncorr_res, "") %>% 
  autoplot() + ggtitle("RF (no corr)")
```

## Comparison of preprocessors

```{r}
rf_comp <-
  map2(list(rf_ncorr_res, rf_main_res),
       c("no_corr", "main"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("no_corr" = .y.x, "main" = .y.y) %>% 
  unite(id, id, id2)

rf_posterior <- perf_mod(rf_comp, iter = 10000, seed = 100, refresh = 0, chains = 5, cores = 6)

autoplot(rf_posterior)
```

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

Difference is significant but very small


# Boosted Trees (BT)

BT are sensitive to hyper-parameters that's why, firstly, I will use Bayesian grid search. 

## Main recipe

```{r, fig.width=14, fig.height=8}
xgb_main_bres <- readRDS(paste0(models_path, "btb_main_rs20.rds"))

autoplot(xgb_main_bres)
```

### Iterations

```{r}
autoplot(xgb_main_bres, type = "performance")
```

### Best models

```{r}
xgb_main_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### Gain curve

```{r}
xgb_main_bres %>% 
  collect_predictions() %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### ROC

```{r}
make_roc(xgb_main_bres, "") %>% 
  autoplot() + ggtitle("XGB (all predictors)")
```

## NCORR recipe

```{r, fig.width=14, fig.height=8}
xgb_ncorr_bres <- readRDS(paste0(models_path, "btb_ncorr_rs20_corr_tune.rds"))

xgb_ncorr_bres %>% autoplot()
```

### Iterations

```{r}
xgb_ncorr_bres %>% 
  autoplot(type = "performance")
```

### Gain curve

```{r}
xgb_ncorr_bres %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best models

```{r}
xgb_ncorr_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_ncorr_bres, "XGB (no corr)") %>% 
  autoplot()
```

**NB:** This ROC looks much better than any other ROC so far

## NCORR + win-loss grid search

```{r}
xgb_ncorr_wl <- readRDS(paste0(models_path, "bt_ncorr_rs50_corr_tune_wl.rds"))

plot_race(xgb_ncorr_wl)
```

```{r}
xgb_ncorr_wl %>% 
  show_best(metric = "roc_auc")
```

```{r}
make_roc(xgb_ncorr_wl, "XGB (no corr wl)") %>% 
  autoplot()
```


## NCORR + Many iterations

```{r, fig.width=14}
xgb_ncorr_bres_long <- readRDS(paste0(models_path, "btb_ncorr_rs100_it300_n50.rds"))

xgb_ncorr_bres_long %>% 
  autoplot()
```

### Gain curve

```{r}
xgb_ncorr_bres_long %>% 
  collect_predictions() %>% 
  #group_by(id) %>% 
  gain_curve(truth = resistance, estimate = .pred_HR) %>% 
  autoplot()
```

### Best

```{r}
xgb_ncorr_bres_long %>% 
  show_best(metric = "roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(xgb_ncorr_bres_long, "") %>% 
  autoplot()
```



## PCA recipe

```{r, fig.width=14, fig.height=8}
xgb_pca_bres <- readRDS(paste0(models_path, "btb_pca_rs50.rds"))

xgb_pca_bres %>% autoplot()
```

### Best models

```{r}
xgb_pca_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_pca_bres, "XGB (PCA)") %>% autoplot()
```

## Comparison of the preprocessors

```{r}
xgb_comp <-
  map2(list(xgb_main_bres, xgb_ncorr_bres, xgb_ncorr_bres_long, xgb_ncorr_wl, xgb_pca_bres),
       c("main", "ncorr", "ncorr_long", "ncorr_wl", "pca"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("main" = .y.x, "ncorr" = .y.y, "ncorr_long" = .y.x.x, "ncorr_wl" = .y.y.y, "pca" = .y) %>% 
  unite(id, id, id2)

xgb_posterior <- perf_mod(xgb_comp, iter = 4000, seed = 100, refresh = 0, chains = 5, cores = 4)

autoplot(xgb_posterior)
```

```{r}
xgb_preproc_diff <- contrast_models(xgb_posterior, seed = 100) 

summary(xgb_preproc_diff)
```

Only PCA recipe gives significantly worse performance

# Multilayer perceptron (MLP NNET)

Applying ORQ-normalization improves MLP (nnet) performance

## NCORR recipe

```{r, fig.width=10, fig.height=4}
nnet_ncorr <- readRDS(paste0(models_path, "mlp_ncorr_rs20_corr_tune.rds"))

nnet_ncorr %>% autoplot()
```

### Best models

```{r}
nnet_ncorr %>% show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(nnet_ncorr, "") %>% 
  autoplot() + ggtitle("MLP (no corr)")
```

## NCORQ recipe

```{r, fig.width=10, fig.height=4}
nnet_ncorq <- readRDS(paste0(models_path, "mlp_ncorq_rs20_corr_tune.rds"))

nnet_ncorq %>% autoplot()
```

### Best models

```{r}
nnet_ncorq %>% 
  show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(nnet_ncorq, "") %>% 
  autoplot() + ggtitle("MLP (ORQ)")
```

## PCA recipe

```{r, fig.width=10, fig.height=4}
nnet_pca <- readRDS(paste0(models_path, "mlp_pca_rs20_corr_tune.rds"))

nnet_pca %>% autoplot()
```

### Best models

```{r}
nnet_pca %>% show_best("roc_auc")
```

### Best ROC

```{r}
nnet_pca %>% 
  make_roc("") %>% 
  autoplot() + ggtitle("MLP (PCA)")
```

## Comparison of the preprocessors

```{r}
mlp_comp <-
  map2(list(nnet_ncorr, nnet_ncorq, nnet_pca),
       c("norm", "orq", "pca"),
       ~ best_aucs(res_obj = .x, model_name = .y)) %>% # model_names is not passed correctly
  reduce(inner_join, by = c("id", "id2")) %>% 
  rename("norm" = .y.x, "orq" = .y.y, "pca" = .y) %>% 
  unite(id, id, id2)

mlp_posterior <- perf_mod(mlp_comp, iter = 2000, seed = 100, refresh = 0, chains = 5, cores = 2)

autoplot(mlp_posterior)
```

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```

Both PCA and NCORQ recipes work better than standard normalization

## NCORQ + Bayesian grid search

```{r}
mlp_ncorq_bres <- readRDS(paste0(models_path, "mlpb_ncorq_rs20_corr_tune.rds"))

mlp_ncorq_bres %>% autoplot()
```

### Best ones

```{r}
mlp_ncorq_bres %>% 
  show_best("roc_auc")
```

### Best ROC

```{r}
make_roc(mlp_ncorq_bres, "") %>% 
  autoplot() + ggtitle("MLP (ORQ) Bayesian")
```

# Bagged MLP

```{r}
library(baguette)

bag_mlp_mod <- bag_mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>% 
      set_engine("nnet", num.threads = 8) %>% 
      set_mode("classification") %>% 
      translate()

bag_mlp_mod
```

Error:

! parsnip could not locate an implementation for `bag_mlp`
classification model specifications using the `nnet` engine. ℹ The
parsnip extension package baguette implements support for this
specification. ℹ Please install (if needed) and load to continue.

Bagged Neural Network Model Specification (classification)

Main Arguments: hidden_units = tune() penalty = tune() epochs = tune()

Engine-Specific Arguments: num.threads = 8

Computational engine: nnet

# Model comparison

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each resamples object

```{r}
library(tidyposterior)

best_preprocessor <- function(res_obj) {
  res_obj %>% select_best("roc_auc") %>% pull(.config)
}

mod_comparison <- inner_join(
  lr_ncorr_res %>% 
    collect_metrics(summarize = FALSE) %>% 
    filter(.metric == "roc_auc", .config == best_preprocessor(lr_ncorr_res)) %>% 
    select(id, id2, LR_norm = .estimate),
  lr_ncorq_res %>% 
    collect_metrics(summarize =FALSE) %>% 
    filter(.metric == "roc_auc", .config == best_preprocessor(lr_ncorq_res)) %>% 
    select(id, id2, LR_orq = .estimate),
  by = c("id", "id2")
)

mod_posterior <- perf_mod(mod_comparison)

autoplot(mod_posterior)
```


## ROC of all models

```{r, fig.width=8, fig.height=6}
# calculate rocs

roc_df <- map2_dfr(list(lr_ncorr_res, rf_ncorr_bres, xgb_ncorr_bres, nnet_ncorr_res, lsvm_ncorr_res, psvm_ncorr_res, knn_ncorr_res, nb_ncorr, nnet_ncorr_bres, xgb_ncorr_bres_long),
                            c("LR", "RF", "BT", "MLP", "linear SVM", "polynomial SVM", "KNN", "NB", "MLP b", "BT l"), 
                            function(x, y) make_roc(x, y))
roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(direction = -1, option = "H")

```

XGB is the best, LR no corr is the second

XGB here looks better than in previous version (it reaches sensitivity =
1 faster)

## ROC of the best ones

LR no corr, BT no corr, BT no corr ORQ, BT all, MLP (nnet no corr)

```{r, fig.width=8, fig.height=6}
roc_selected_df <- map2_dfr(list(lr_pca_res, lr_ncorq_res, lr_ncorr_res, xgb_ncorr_bres, nnet_ncorq),
                            c("LR (PCA)", "LR (ORQ)", "LR (Norm)", "XGB (no corr)", "MLP (ORQ)"), 
                            function(x, y) make_roc(x, y))

roc_selected_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  theme_bw()
```

# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Model and workflow

```{r}
# tuned recipe
best_corr <- xgb_ncorr_bres %>% 
  select_best("roc_auc") %>% 
  pull(corr_tune)

ncorr_recipe_tuned <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = best_corr) %>% # tuned threshold
  step_smote(resistance, over_ratio = 1, seed = 100)

# model spec
xgb_mod <- set_model("bt", 8)

best <- xgb_ncorr_bres %>% 
  select_best("roc_auc")

last_mod <-
  boost_tree(
    mtry = best$mtry,
    min_n = best$min_n,
    tree_depth = best$tree_depth,
    learn_rate = best$learn_rate,
    loss_reduction = best$loss_reduction,
    sample_size = best$sample_size,
    stop_iter = best$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
xgb_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(ncorr_recipe_tuned) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

**NB**: This is forbidden action!

```{r}
ncorr_recipe_tuned <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = 0.406) %>% # tuned threshold
  step_smote(resistance, over_ratio = 1, seed = 100)

# model spec
lr_mod <- set_model("lr", 8)

best <- lr_ncorr_res %>% 
  select_best("roc_auc")

last_mod <-
  logistic_reg(
    penalty = best$penalty
  ) %>% 
  set_engine("glmnet", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
lr_wf <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(ncorr_recipe_tuned) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lr_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

```{r}
# model spec
mlp_mod <- set_model("nnet", 2)

best <- nnet_ncorr_bres %>% 
  select_best("roc_auc")

last_mod <-
  mlp(
    hidden_units = best$hidden_units,
    penalty = best$penalty,
    epochs = best$epochs
  )%>% 
  set_engine("nnet", num.threads = 4) %>% 
  set_mode("classification") 

# this was the XGB workflow
mlp_wf <- workflow() %>% 
  add_model(mlp_mod) %>% 
  add_recipe(ncorr_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  mlp_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

```{r}
# model spec
lr_mod <- set_model("lr", 8)

best <- lr_pca_res %>% 
  select_best("roc_auc")

last_mod <-
  logistic_reg(
    penalty = best$penalty
  ) %>% 
  set_engine("glmnet", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
lr_wf <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(pca_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lr_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

## Features importance

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 40, include_type = T) 
```

Sequencing related features (coverage and read length) are here - among
the most important

## Final model's parameters

```{r, eval=F}
final_fit %>% 
  extract_fit_parsnip() 
```

## ROC

```{r}
final_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

## Confusion Matrix

Probability cut-off = 0.5

```{r, fig.width=6}
cm <- final_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

## Probability cutoff adjustment by maximum j-index

```{r, fig.width=8}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  final_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# max_j_index_threshold may be a vector, use its last element
if (length(max_j_index_threshold) > 1){
  max_j_index_threshold <- max_j_index_threshold[length(max_j_index_threshold)]
}

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: ?"
  )

sens_spec_j_plot
```

What performance will I get using this maximal j-index?

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)

```{r, fig.width=6}
pred_optimized <- final_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

autoplot(cm_optimized, type = "heatmap")
```

### Optimized performance

```{r}
summary(cm_optimized)
```

## Extract predictions

With probability threshold = 0.5

### Collect False Positives

Should be 8 of them

```{r}
test_row_numbers <- data_strain %>% 
  select(strain, resistance) %>% 
  mutate(.row = row_number())

fp_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "nonHR") %>% 
  select(strain) %>% 
  mutate(prediction = "FP")
  
fp_df
```

### Collect False Negatives

Should be 6 of them

```{r}
fn_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "HR") %>% 
  select(strain)%>% 
  mutate(prediction = "FN")

fn_df
```

```{r}
tp_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "HR") %>% 
  select(strain)%>% 
  mutate(prediction = "COR")

dim(tp_df)

tn_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "nonHR") %>% 
  select(strain)%>% 
  mutate(prediction = "COR")

dim(tn_df)

pred_df <- bind_rows(tp_df, tn_df, fp_df, fn_df)

dim(pred_df)
```

------------------------------------------------------------------------

# Models stacking

For successful stacking, you need the same set of predictors in every
candidate model.

Models with NOCORR recipe were among the best

Models must be trained using the same set of features!

Each model in the same stack must have same resampling method!

## Make a BRES stack

```{r}
library(stacks)

# no corr set
ncorr_stack_bres <-
  stacks() %>% 
  add_candidates(rf_ncorr_bres) %>%
  add_candidates(xgb_ncorr_bres_long) %>% 
  add_candidates(nnet_ncorr_bres)

as_tibble(ncorr_stack_bres)
```

Evaluating the stack

```{r}
ncorr_stack_bres %>% 
  blend_predictions(penalty = seq(0.01, 0.4, 0.01)) %>% 
  autoplot()
```

Let's choose penalty

```{r}
ncorr_stack_model <- 
  ncorr_stack_bres %>% 
  blend_predictions(penalty = 0.03)
```

```{r}
autoplot(ncorr_stack_model, type = "weights")
```

```{r}
ncorr_stack_model <- 
  ncorr_stack_model %>% 
  fit_members()

ncorr_stack_model
```

```{r}
data_pred <- df_test %>% 
  bind_cols(predict(ncorr_stack_model, ., type = "class"))

npv(data_pred, truth = resistance, .pred_class)
```

```{r}
roc_curve(data = data_pred, truth = resistance, .pred_HR_nnet_ncorr_bres_1_18) %>% 
  autoplot()
```

```{r}
yardstick::kap(data_pred, truth = resistance, .pred_HR)
```

```{r}
# updated workspace
save.image("data/all_models_local_update.RData")
```
