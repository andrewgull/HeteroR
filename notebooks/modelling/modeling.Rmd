---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(embed) # for UMAP
```

# Read and process data

The data sets are the same as in EDA

```{r}
data_strain <- readr::read_csv("data/features_strain.csv", na = c("NA", "-Inf"))

hr_testing <- readr::read_csv("data/heteroresistance_testing_gr12.csv")

data_strain <- data_strain %>% 
  left_join(hr_testing, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac > 3 and >4

```{r}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.3, n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```


```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing

## Basic Recipe

>Some models (notably neural networks, KNN, and support vector machines) require predictors that have been centered and scaled, so some model workflows will require recipes with these preprocessing steps. For other models, a traditional response surface design model expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe
```

### Bake and check

```{r}
train_data <- main_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

head(train_data)
```

```{r}
train_data %>% group_by(resistance) %>% count()
```


### NZV predictors

```{r}
setdiff(names(df), names(train_data))
```


## PCA + Yeo-Johnson transformation recipe

From [here](https://www.tmwr.org/grid-search.html):

>Because of the high degree of correlation between predictors, it makes sense to use PCA feature extraction to decorrelate the predictors. The following recipe contains steps to transform the predictors to increase symmetry, normalize them to be on the same scale, then conduct feature extraction. The number of PCA components to retain is also tuned, along with the model parameters.

>While the resulting PCA components are technically on the same scale, the lower-rank components tend to have a wider range than the higher-rank components. For this reason, we normalize again to coerce the predictors to have the same mean and variance.

>Many of the predictors have skewed distributions. Since PCA is variance based, extreme values can have a detrimental effect on these calculations. To counter this, let’s add a recipe step estimating a Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000). While originally intended as a transformation of the outcome, it can also be used to estimate transformations that encourage more symmetric distributions

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

#  step_YeoJohnson(all_numeric_predictors()) %>% 
#  step_normalize(all_numeric_predictors()) %>% 
#  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
#  step_normalize(all_numeric_predictors())

pca_recipe
```

## UMAP and Ordered-Quantile normalization

I use supervised UMAP

```{r}
# function for umap plot
library(ggforce)

plot_validation_results <- function(dat) {
  dat %>%
    select(-strain) %>% 
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y, color = resistance, fill = resistance)) +
    geom_point(alpha = 0.4, size = 1) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-resistance), layer.diag = 2) + 
    scale_color_brewer(palette = "Set1") + 
    scale_fill_brewer(palette = "Set1")
}
```

What position of `step_smote()` is preferable??

```{r, fig.width=12, fig.height=12}
umap_rec <- recipe(resistance ~., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_numeric_predictors(), outcome = "resistance", num_comp = 10) 

# to make a plot
data_umap <- prep(umap_rec, retain = TRUE)

data_umap$template %>% 
  plot_validation_results() +
  ggtitle("UMAP (supervised)")
```

Seems OK

### Update the recipe adding SMOTE step

```{r}
umap_rec <- umap_rec %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

umap_rec
```


## LR recipe

```{r}
# adjust the main recipe
lr_recipe <- main_recipe %>%
  step_corr(threshold = 0.75)
```

## Folds and metrics

>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) # is better than v=5
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

## Grid

For creating a tuning grid the maximum entropy design is used `tune()`'s default.

# Penalized LR with UMAP

```{r}
set.seed(233)

lr_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_umap_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(umap_rec)

# create a tune grid
# lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_umap_res <- tune_grid(lr_umap_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)

autoplot(lr_umap_res)
```

## Best ROC-AUC

```{r}
lr_umap_best <- lr_umap_res %>%
  select_best(metric = "roc_auc")

lr_umap_auc <- 
  lr_umap_res %>% 
  collect_predictions(parameters = lr_umap_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR (UMAP)")

autoplot(lr_umap_auc)
```


# Penalized Logistic regression

## Space-filling grid: LR recipe

```{r}
set.seed(333)
# set model type/engine
lr_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
# lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE, save_workflow = TRUE),
              metrics = cls_metrics)

```

### Metrics autoplot

```{r}
autoplot(lr_res)
```

### Best models

```{r}
lr_res %>% 
  show_best("roc_auc", n = 5) 
```

#### ROC of the best model

```{r}
lr_best <- lr_res %>%
  select_best(metric = "roc_auc")

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR")

autoplot(lr_auc)
```

## Space-filling grid: PCA recipe

```{r}
lr_pca_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
lr_pca_res <- tune_grid(lr_pca_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(lr_pca_res)
```


### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
lr_pca_best <- lr_pca_res %>%
  select_best(metric = "roc_auc")

lr_pca_auc <- 
  lr_pca_res %>% 
  collect_predictions(parameters = lr_pca_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR + PCA")

autoplot(lr_pca_auc)
```

## Bayesian grid: LR recipe

```{r}
# extract settings
lr_set <- extract_parameter_set_dials(lr_workflow)

set.seed(12)

lr_bres <-
  lr_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = lr_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE, save_workflow = TRUE)
  )

# roc_auc = 0.8889232 (+/-0.00785)
# penalty=  0.009127915	

autoplot(lr_bres)
```

### Best models

```{r}
lr_bres %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
lr_bres_best <- lr_bres %>%
  select_best(metric = "roc_auc")

lr_bres_auc <- 
  lr_bres %>% 
  collect_predictions(parameters = lr_bres_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR + Bayes")

autoplot(lr_bres_auc)
```



## Bayesian grid: PCA recipe

```{r}
# extraxt settings
lr_set <- extract_parameter_set_dials(lr_workflow)

set.seed(12)

lr_pca_bres <-
  lr_pca_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = lr_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE)
  )
```


# Multivariate adaptive regression splines (MARS)

## Space-filling grid

```{r}
# prune method default: ‘backward’ 
# other methods "backward", "none", "exhaustive", "forward", "seqrep", "cv"
mars_mod <- 
  mars(
    mode = "classification",
    engine = "earth",
    num_terms = tune(),
    prod_degree = tune(),
    prune_method = "backward") %>% 
  translate()

# define the workflow
mars_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
mars_res <- tune_grid(mars_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(mars_res)
```

### Best models

#### Based on AUC

```{r}
mars_res %>% 
  show_best(metric = "roc_auc")
```

#### Based on J-index

```{r}
mars_res %>% 
  show_best(metric = "j_index")
```


#### ROC of the best model

```{r}
mars_best <- mars_res %>%
  select_best(metric = "roc_auc")

mars_auc <- 
  mars_res %>% 
  collect_predictions(parameters = mars_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "MARS")

autoplot(mars_auc)
```

## Space-filling grid with PCA recipe

Works better than with LR recipe

```{r}
mars_pca_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
mars_pca_res <- tune_grid(mars_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Best models

```{r}
mars_pca_res %>% 
  show_best("roc_auc", n = 5)

# best auc = 0.87
```



# Linear support vector machines (lSVM)

## Space-filling grid: LR recipe

```{r}
svm_mod <- 
  svm_linear(
    cost = tune()) %>% # margin - for regression only
  set_mode("classification") %>%
  set_engine("kernlab")  # default

# define the workflow
svm_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_res <- tune_grid(svm_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE, save_workflow = TRUE),
              metrics = cls_metrics)

autoplot(svm_res)
```

### Best models

```{r}
svm_res %>% 
  show_best("roc_auc", n = 5)
```


## Space-filling grid: PCA recipe

```{r}
# define the workflow
svm_pca_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(pca_recipe) # recipe for LR here

# train and tune the model
svm_pca_res <- tune_grid(svm_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
# roc_auc = 0.8513420
# cost = 0.002377809
```


## Bayesian grid

```{r}
# extract settings
svm_set <- extract_parameter_set_dials(svm_workflow)

set.seed(12)

svm_bres <-
  svm_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE)
  )

svm_bres %>% 
  show_best(metric = "roc_auc")

# cost = 0.01040482
# mean auc = 0.8893831	
```


## Metrics autoplot

```{r}
autoplot(svm_res)
```

## Best models

```{r}
svm_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
svm_best <- svm_res %>%
  select_best(metric = "roc_auc")

svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM")

autoplot(svm_auc)
```

# Polynomial SVM

## Tune with space-filling grid

```{r}
set.seed(101)

svm_poly_mod <- svm_poly(
    cost = tune(),
    degree = tune(),
    scale_factor = tune(),
    margin = NULL ) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab", num.threads = 4L)  # default

# define the workflow
svm_poly_workflow <- 
  workflow() %>% 
  add_model(svm_poly_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_poly_res <- tune_grid(svm_poly_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```


## Tune with Bayesian-optimized grid

```{r}
svm_set <- extract_parameter_set_dials(svm_poly_workflow)
svm_set
```



```{r}
library(doParallel)
registerDoParallel(cores=4)
library(foreach)

set.seed(12)

# use foreach here

svm_poly_bres <-
  svm_poly_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = TRUE)
  )


```


## Metrics autoplot

```{r}
autoplot(svm_poly_res)
```

## Best models

```{r}
svm_poly_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
svm_poly_best <- svm_poly_res %>%
  select_best(metric = "roc_auc")

svm_poly_auc <- 
  svm_poly_res %>% 
  collect_predictions(parameters = svm_poly_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_polynomial")

autoplot(svm_poly_auc)
```


# Radial basis function SVM

## Tune

```{r}
svm_rbf_mod <- 
  svm_rbf(
    cost = tune(),
    rbf_sigma = tune(),
    margin = NULL) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab")  # default

# define the workflow
svm_rbf_workflow <- 
  workflow() %>% 
  add_model(svm_rbf_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_rbf_res <- tune_grid(svm_rbf_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(svm_rbf_res)
```

## Best models

```{r}
svm_rbf_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
svm_rbf_best <- svm_rbf_res %>%
  select_best(metric = "roc_auc")

svm_rbf_auc <- 
  svm_rbf_res %>% 
  collect_predictions(parameters = svm_rbf_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_radial")

autoplot(svm_rbf_auc)
```


# K-nearest neighbors (KNN)

## Tune

```{r}
knn_mod <- nearest_neighbor(
    neighbors = tune(),
    weight_func = tune(),
    dist_power = tune()) %>% #
  set_engine("kknn") %>%
  set_mode("classification")

# define the workflow
knn_workflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here


# train and tune the model
knn_res <- tune_grid(knn_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(knn_res)
```

## Best models

```{r}
knn_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
knn_best <- knn_res %>%
  select_best(metric = "roc_auc")

knn_auc <- 
  knn_res %>% 
  collect_predictions(parameters = knn_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "KNN")

autoplot(knn_auc)
```

# Random Forest (RF)

## Tune

```{r}
cores <- 4L

# model
rf_mod <- 
  rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

rm_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(main_recipe) # main recipe here

set.seed(5732)
# run
rf_res <- tune_grid(rm_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(rf_res)
```

## Best models

```{r}
# automatic choice of the best model
rf_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
rf_best <- 
  rf_res %>% 
    select_best(metric = "roc_auc")

rf_auc <- 
  rf_res %>% 
    collect_predictions(parameters = rf_best) %>% 
    roc_curve(resistance, .pred_HR) %>% 
    mutate(model = "RF")

autoplot(rf_auc)
```

### Variable Importance

```{r}
# the last model

rf_best_mod <-
  rand_forest(
      mtry = rf_best$mtry, 
      min_n = rf_best$min_n, 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = 2) %>% 
  set_mode("classification")

# the last workflow
rf_best_wf <- 
  svm_poly_workflow %>% 
  update_model(rf_best_mod)

# the last fit
set.seed(345)
rf_best_fit <- 
  rf_best_wf %>% 
  last_fit(data_split)

rf_best_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 67)
```


# Boosted Trees (BT)

Using Extreme Gradient Boosting

## Space-fiiling grid 

```{r}
set.seed(732)

# number of cores available on Kaggle
cores <- 4L 

# model specification
xgb_mod <- 
  boost_tree(
    trees = 50, 
    mtry = tune(), 
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = tune(), 
    loss_reduction = tune(), 
    sample_size = tune(), 
    stop_iter = tune()) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
xgb_cv_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(main_recipe)

# tune models, this takes time
xgb_res <- tune_grid(xgb_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Bayesian tuning

```{r}
# extract settings
xgb_set <- extract_parameter_set_dials(xgb_cv_wf) %>%
  finalize(x = df_train %>% select(-resistance))

set.seed(12)

xgb_bres <-
  xgb_cv_wf %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = xgb_set,
    # Generate 5x7 at semi-random to start
    initial = 35,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE)
  )

xgb_bres %>% 
  show_best(metric = "roc_auc")

# mtry = 74 or 13
# min_n = 2 or 2
# tree_depth = 10 or 7
# learn_rate = 0.18751350 or 0.10381054
# loss_reduction = 5.456852e-07 or 1.287068e-06
# sample_size = 0.6358425 or 0.6781397
# stop_iter = 6 or 6
# roc_auc = 0.8840152 or 0.8811742	
# n = 100
```

## Metrics autoplot

```{r, fig.width=14}
autoplot(xgb_res)
```

## Best models

```{r}
xgb_res %>% 
  show_best(metric = "roc_auc", n = 5)
```


```{r}
xgb_res %>% 
  show_best(metric = "j_index", n = 40) %>% 
  filter(.config == "Preprocessor1_Model11")
```

### ROC of the best

```{r}
xgb_best <- xgb_res %>% 
  select_best(metric = "roc_auc")

xgb_auc <- xgb_res %>% 
  collect_predictions(parameters = xgb_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "BT")

autoplot(xgb_auc)
```

### Variable Importance

```{r}
xgb_res %>% vip() 
```


# Ensemble of the models

```{r}

```


# Comparison of all models

```{r, fig.width=8}
roc_plot <- bind_rows(xgb_auc, rf_auc, lr_auc, mars_auc, svm_auc, svm_poly_auc, svm_rbf_auc, knn_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Dark2")

roc_plot
```

SVM is the best according to AUC, RF and BT are close

```{r}
ggsave("plots/roc.png", roc_plot)
```


# Final fit

```{r}
# the last model
# svm poly
last_mod <-
  svm_poly(
    cost = svm_poly_best$cost,
    degree = svm_poly_best$degree,
    scale_factor = svm_poly_best$scale_factor
  ) %>% 
  set_mode("classification") %>%
  set_engine("kernlab") 

# xgb bayes
# mtry = 74 or 13
# min_n = 2 or 2
# tree_depth = 10 or 7
# learn_rate = 0.18751350 or 0.10381054
# loss_reduction = 5.456852e-07 or 1.287068e-06
# sample_size = 0.6358425 or 0.6781397
# stop_iter = 6 or 6

last_mod <-
  boost_tree(
    mtry = 74,
    min_n = 2,
    tree_depth = 10,
    learn_rate = 0.18751350,
    loss_reduction = 5.456852e-07,
    sample_size = 0.6358425,
    stop_iter = 6
  ) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification") 

# the last workflow
xgb_last_wf <- 
  xgb_cv_wf %>% 
  update_model(last_mod)

# the last fit
set.seed(345)
xgb_last_fit <- 
  xgb_last_wf %>% 
  last_fit(data_split)

xgb_last_fit %>% 
  collect_metrics()
```

```{r}
xgb_last_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 77)
```

```{r}
last_fit %>% extract_fit_parsnip() 
```


## ROC

```{r}
last_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

```{r}
xgb_last_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```


## Confusion Matrix

```{r, fig.width=6}
cm <- last_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

```{r, fig.width=6}
cm_xgb <- xgb_last_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm_xgb, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

```{r}
summary(cm_xgb)
```

## Probability cutoff adjustment

```{r, fig.width=6}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  last_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: polynomial SVM"
  )

sens_spec_j_plot
```

Moving threshold to the crossing point between sensitivity and specificity curves (0.405) doesn't improve overall performance of the model.

```{r, fig.width=6}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  xgb_last_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold_BT <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold_BT, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: BT"
  )

sens_spec_j_plot
```

```{r}
ggsave("plots/prob_cutoff_BT_final.png", sens_spec_j_plot)
```


## Optimized confusion matrix

j-index

```{r, fig.width=6}
pred_optimized <- last_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

cm_heatmap <- autoplot(cm_optimized, type = "heatmap")
cm_heatmap
```

```{r}
summary(cm_optimized)
```

Metrics are better after the probability threshold adjustment.

```{r, fig.width=6}
pred_optimized <- xgb_last_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold_BT
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

cm_heatmap <- autoplot(cm_optimized, type = "heatmap")
cm_heatmap
```

```{r}
summary(cm_optimized)
```

## Extract predictions

### Collect False Positives

Should be 9 of them

```{r}
test_row_numbers <- df %>% 
  select(strain, resistance) %>% 
  mutate(.row = row_number())

xgb_last_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "nonHR") %>% 
  select(strain)
  
```

### Collect False Negatives

Should be 4 of them

```{r}
xgb_last_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "HR") %>% 
  select(strain)
```



```{r}
ggsave("plots/confusion_matrix_BT.png", cm_heatmap)
```

---

# Stacking test

Let's take a couple of simple models and make a stack object: `lr_res` and `svm_res` with `save_pred=TRUE` and `save_workflow=TRUE`

You can use `run_models.R` to get trained models and then use them for stacking

```{r}
# don't forget to make a copy of each model before you load new environment
load("/home/andrei/Data/HeteroR/notebooks/svmb.RData")
svm_bres <- model_bres

# and so on for each new environment
load("/home/andrei/Data/HeteroR/notebooks/knnb.RData")
knn_bres <- model_bres

load("/home/andrei/Data/HeteroR/notebooks/marsb.RData")
mars_bres <- model_bres

load("/home/andrei/Data/HeteroR/notebooks/lrb.RData")
lr_bres <- model_bres
```


Let's choose the best models

```{r}
models_list <- list(lr_bres, knn_bres, mars_bres, svm_bres)

lapply(models_list, function(x){show_best(x, metric = "roc_auc", n = 1)}) 
```



```{r}
library(stacks)

test_stack <- 
  stacks() %>% 
  add_candidates(lr_bres) %>% 
  add_candidates(svm_bres) 

test_stack
```

```{r}
as_tibble(test_stack)
```

```{r}
test_stack_model <- 
  test_stack %>% 
  blend_predictions(penalty = c(0.01, 0.015, 0.02, 0.03, 0.04, 0.045, 0.05))

autoplot(test_stack_model)
```

```{r}
autoplot(test_stack_model, type = "members")
```

```{r}
autoplot(test_stack_model, type = "weights")
```

```{r}
test_stack_model <- 
  test_stack_model %>% 
  fit_members()
```


```{r}
df_stack_test <- 
  df_test %>%
  bind_cols(predict(test_stack_model, .))
```


```{r}
df_stack_test
```

```{r}
test_stack_model
```



```{r}
# updated workspace
save.image("data/all_models_local_update.RData")
```

