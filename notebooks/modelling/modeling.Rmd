---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(embed) # for UMAP

# path for models
models_path <- "~/Data/HeteroR/notebooks/models/"
```

# Read and process data

The data sets are the same as in EDA

```{r}
data_strain <- readr::read_csv("data/features_strain.csv", na = c("NA", "-Inf"))

hr_testing <- readr::read_csv("data/heteroresistance_testing_gr12.csv")

data_strain <- data_strain %>% 
  left_join(hr_testing, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \> 3 and \>4

```{r}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.3, n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing

## Basic Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe
```

### Bake and check

```{r}
train_data <- main_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

head(train_data)
```

```{r}
train_data %>% group_by(resistance) %>% count()
```

## PCA + ORQ-normalization

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

For PCA recipe I will use ORQ-normalization.

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

pca_recipe
```

## UMAP and Ordered-Quantile normalization

I use supervised UMAP

What position of `step_smote()` is preferable??

```{r, fig.width=12, fig.height=12}
umap_recipe <- recipe(resistance ~., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_numeric_predictors(), outcome = "resistance", num_comp = 14) 

# to make a plot
# data_umap <- prep(umap_rec, retain = TRUE)

```

### Add SMOTE step

```{r}
umap_recipe <- umap_recipe %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

umap_recipe
```

## No correlation recipe

with manually set correlation threshold

```{r}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = 0.75) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) # is better than v=5
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

## Grid

For creating a tuning grid the maximum entropy design is used `tune()`'s
default.

## Functions to simplify things

To make model creation less repetitive

```{r}

## MAKE MODEL SPECIFICATION ##

set_model <- function(mod, cores) {
  if (mod == "lr") {
    my_mod <- logistic_reg(
        penalty = tune(), 
        mixture = 1) %>% 
      set_engine("glmnet")
  } else if (mod == "mars") {
    my_mod <- mars(
      mode = "classification",
      engine = "earth",
      num_terms = tune(),
      prod_degree = tune(),
      prune_method = "backward") %>% 
      translate()
  } else if(mod == "svm") {
    my_mod <- svm_linear(
        cost = tune()) %>% # margin - for regression only
      set_mode("classification") %>%
      set_engine("kernlab")  # default
  } else if(mod == "rf") {
    my_mod <- rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
      set_engine("ranger", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "bt") {
    my_mod <- boost_tree(
      trees = 50, 
      mtry = tune(), 
      min_n = tune(), 
      tree_depth = tune(), 
      learn_rate = tune(), 
      loss_reduction = tune(), 
      sample_size = tune(), 
      stop_iter = tune()) %>% 
      set_engine("xgboost", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "knn") {
    my_mod <- nearest_neighbor(
      neighbors = tune(),
      weight_func = tune(),
      dist_power = tune()) %>%
      set_engine("kknn") %>%
      set_mode("classification")
  } else if (mod == "nnet"){
    my_mod <-
      mlp(hidden_units = tune(), 
          penalty = tune(), 
          epochs = tune()) %>%
      set_mode("classification") %>%
      set_engine("nnet", num.threads = cores)
  }
  return(my_mod)
}

## SET WORKFLOW ##

set_wf <- function(mod, rec, cores){
  # mod: model type, one of: lr, knn, mars, svm, rf, bt
  # rec: recipe object (one of: main, ncorr, pca, umap)
  # rec must be in GlobalEnv
  
  if (rec == "main"){
    rc <- main_recipe
  } else if (rec == "ncorr"){
    rc <- ncorr_recipe
  } else if (rec == "pca") {
    rc <- pca_recipe
  } else if (rec == "umap") {
    rc <- umap_recipe
  } else {
    print("ERROR! Undefined recipe!")
  }

  wf <- workflow() %>% 
    add_model(set_model(mod = mod, cores = cores)) %>% 
    add_recipe(rc)
  
  return(wf)
}

## MK ROC OBJECT ##
# for autoplot()

make_roc <- function(mod_res, title){
  
  mod_best <- mod_res %>%
    select_best(metric = "roc_auc")
  
  mod_auc <- mod_res %>% 
    collect_predictions(parameters = mod_best) %>% 
    roc_curve(resistance, .pred_HR) %>% 
    mutate(model = title)
  
  return(mod_auc)
}


```

# Penalized LR

## UMAP

If you want to use UMAP instead of real features, then you should not
use supervised UMAP.

Also, (supervised) UMAP recipe dumps core for whatever reason

## NCORR recipe

For LR I used space filling grid, cause Bayesian would cause errors
during resampling and the advantage it gives is marginal.

You can load an `.rds` file if it exists.

```{r}
lr_ncorr_res <- readRDS(paste0(models_path, "lr_ncorr.rds"))
```

Or fit and resample it anew

```{r, eval=F}
set.seed(333)
# set model type/engine
lr_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(ncorr_recipe)

# create a tune grid
# lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_ncorr_res <- tune_grid(lr_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE, save_workflow = TRUE),
              metrics = cls_metrics)

```

### Metrics autoplot

```{r}
autoplot(lr_ncorr_res)
```

### Best models

```{r}
lr_ncorr_res %>% 
  show_best("roc_auc", n = 5) 
```

#### ROC of the best model

```{r}
lr_ncorr_auc <- make_roc(lr_ncorr_res, "LR (no corr)")

autoplot(lr_ncorr_auc)
```

## PCA recipe

Space-filling grid as well

load

```{r}
lr_pca_res <- readRDS(paste0(models_path,"lr_pca.rds"))
```

or compute

```{r, eval=FALSE}
lr_pca_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
lr_pca_res <- tune_grid(lr_pca_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

**NB**: these problems appear:

```{r}
lr_pca_res$.notes[[99]]$note
```

> "Error in glmnet::glmnet(x = maybe_matrix(x), y = y, alpha = \~1,
> family ="binomial"): x should be a matrix with 2 or more columns"

### Metrics autoplot

```{r}
autoplot(lr_pca_res)
```

### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```

Doesn't look good

### ROC

```{r}
lr_pca_auc <- make_roc(lr_pca_res, "LR (PCA)")

autoplot(lr_pca_auc)
```

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

Space-filling grid as well

Load

```{r}
mars_ncorr_res <- readRDS(paste0(models_path, "mars_ncorr.rds"))
```

or compute

```{r, eval=F}
# prune method default: ‘backward’ 
# other methods "backward", "none", "exhaustive", "forward", "seqrep", "cv"
mars_mod <- 
  mars(
    mode = "classification",
    engine = "earth",
    num_terms = tune(),
    prod_degree = tune(),
    prune_method = "backward") %>% 
  translate()

# define the workflow
mars_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(ncorr_recipe) # recipe for LR here

# train and tune the model
mars_ncorr_res <- tune_grid(mars_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(mars_ncorr_res)
```

### Best models

#### Based on AUC

```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

#### ROC of the best model

```{r}
mars_auc <- make_roc(mars_ncorr_res, "MARS (no corr.)")

autoplot(mars_auc)
```

## PCA recipe

Space-filling grid

Works better than with NCORR recipe

Load:

```{r}
mars_pca_res <- readRDS(paste0(models_path, "mars_pca.rds"))
```

Or compute:

```{r, eval=F}
mars_pca_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
mars_pca_res <- tune_grid(mars_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(mars_pca_res)
```

### Best models

```{r}
mars_pca_res %>% 
  show_best("roc_auc", n = 5)
```

# Linear support vector machines (lSVM)

## NCORR

Load

```{r}
svm_ncorr_res <- readRDS(paste0(models_path, "lsvm_ncorr.rds"))
```

or compute

```{r, eval=F}
svm_mod <- 
  svm_linear(
    cost = tune()) %>% # margin - for regression only
  set_mode("classification") %>%
  set_engine("kernlab")  # default

# define the workflow
svm_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(ncorr_recipe) # recipe for LR here

# train and tune the model
svm_ncorr_res <- tune_grid(svm_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE, save_workflow = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(svm_ncorr_res)
```

### Best models

```{r}
svm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## PCA recipe

```{r}
# define the workflow
svm_pca_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(pca_recipe) # recipe for LR here

# train and tune the model
svm_pca_res <- tune_grid(svm_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
# roc_auc = 0.8513420
# cost = 0.002377809
```

## Bayesian grid

```{r}
# extract settings
svm_set <- extract_parameter_set_dials(svm_workflow)

set.seed(12)

svm_bres <-
  svm_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE)
  )

svm_bres %>% 
  show_best(metric = "roc_auc")

# cost = 0.01040482
# mean auc = 0.8893831	
```

## Metrics autoplot

```{r}
autoplot(svm_res)
```

## Best models

```{r}
svm_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
svm_best <- svm_res %>%
  select_best(metric = "roc_auc")

svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM")

autoplot(svm_auc)
```

# Polynomial SVM

## NCORR recipe + space-filling grid

load

```{r}
svm_poly_res <- readRDS(paste0(models_path, "psvm_ncorr.rds"))
```

or compute anew

```{r, eval=F}
set.seed(101)

svm_poly_mod <- svm_poly(
    cost = tune(),
    degree = tune(),
    scale_factor = tune(),
    margin = NULL ) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab", num.threads = 4L)  # default

# define the workflow
svm_poly_workflow <- 
  workflow() %>% 
  add_model(svm_poly_mod) %>% 
  add_recipe(ncorr_recipe) # recipe for LR here

# train and tune the model
svm_poly_res <- tune_grid(svm_poly_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(svm_poly_res)
```

### Best AUC

```{r}
svm_poly_res %>% show_best("roc_auc", n = 5)
```

## NCORR resipe + Bayesian-optimized grid

load

```{r}
smv_poly_bres <- readRDS(paste0(models_path, "psvmb_ncorr.rds"))
```

or compute

```{r, eval=F}
svm_set <- extract_parameter_set_dials(svm_poly_workflow)

library(doParallel)
registerDoParallel(cores=4)
library(foreach)

set.seed(12)

# use foreach here

svm_poly_bres <-
  svm_poly_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = TRUE)
  )


```

## Metrics autoplot

```{r}
autoplot(svm_poly_res)
```

## Best models

```{r}
svm_poly_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
svm_poly_best <- svm_poly_res %>%
  select_best(metric = "roc_auc")

svm_poly_auc <- 
  svm_poly_res %>% 
  collect_predictions(parameters = svm_poly_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_polynomial")

autoplot(svm_poly_auc)
```

# Radial basis function SVM

## Tune

```{r}
svm_rbf_mod <- 
  svm_rbf(
    cost = tune(),
    rbf_sigma = tune(),
    margin = NULL) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab")  # default

# define the workflow
svm_rbf_workflow <- 
  workflow() %>% 
  add_model(svm_rbf_mod) %>% 
  add_recipe(ncorr_recipe) # recipe for LR here

# train and tune the model
svm_rbf_res <- tune_grid(svm_rbf_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(svm_rbf_res)
```

## Best models

```{r}
svm_rbf_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
svm_rbf_best <- svm_rbf_res %>%
  select_best(metric = "roc_auc")

svm_rbf_auc <- 
  svm_rbf_res %>% 
  collect_predictions(parameters = svm_rbf_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_radial")

autoplot(svm_rbf_auc)
```

# K-nearest neighbors (KNN)

## NCORR recipe + Bayes

**NB**: I get these errors while using Bayesian grid search

> The Gaussian process model is being fit using 12 features but only has
> 9 data points to do so. This may cause errors or a poor model fit

or

> Gaussian process model: did not converge in 10 iterations

Work with the following model with caution

Load:

```{r}
knn_ncor_bres <- readRDS(paste0(models_path, "knnb_ncorr.rds"))
```

### Metric autoplot

```{r}
knn_ncor_bres %>% autoplot()
```

### Best AUC

```{r}
knn_ncor_bres %>% show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(knn_ncor_bres, title="KNN (bayes, no corr.") %>% 
  autoplot()
```

## NCORR + Space-filling

```{r, eval=F}
knn_mod <- nearest_neighbor(
    neighbors = tune(),
    weight_func = tune(),
    dist_power = tune()) %>% #
  set_engine("kknn") %>%
  set_mode("classification")

# define the workflow
knn_workflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(ncorr_recipe) # recipe for LR here


# train and tune the model
knn_ncorr_res <- tune_grid(knn_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(knn_ncorr_res)
```

## Best models

```{r}
knn_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
make_roc(knn_ncor_res, "KNN (space, no corr.") %>% 
  autoplot()
```

# Random Forest (RF)

## Main recipe + Bayes

### Metrics autoplot

```{r}
rf_main_bres <- readRDS(paste0(models_path, "rfb.rds"))

rf_main_bres %>% autoplot()
```

### Best models

```{r}
rf_main_bres %>% show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(rf_main_bres, "RF (bayes") %>% autoplot()
```

## Main recipe + Space-filling

Load

or compute

```{r, eval=F}
cores <- 4L

# model
rf_mod <- 
  rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

rm_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(main_recipe) # main recipe here

set.seed(5732)
# run
rf_res <- tune_grid(rm_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(rf_res)
```

### Best models

```{r}
# automatic choice of the best model
rf_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
make_roc(rf_res, "RF") %>% autoplot()
```

## NCORR recipe + Bayesian optimization

```{r}
rf_ncorr_bres <- readRDS(paste0(models_path, "rfb_ncorr.rds"))

rf_ncorr_bres %>% autoplot()
```

### Best models

```{r}
rf_ncorr_bres %>% show_best("roc_auc", n = 5)
```

### ROC 

```{r}
rf_ncorr_bres %>% 
  make_roc("RF (no corr)") %>% 
  autoplot()
```


# Boosted Trees (BT)

Using Extreme Gradient Boosting

## Space-filling grid

Space filling grid proved to be less efficient in finding hyper
parameters than Bayesian optimization

## BT with the main recipe

Bayesian tuning

```{r}
xgb_main_bres <- readRDS(paste0(models_path, "btb.rds"))
```

or compute

```{r}
# extract settings
xgb_set <- extract_parameter_set_dials(xgb_cv_wf) %>%
  finalize(x = df_train %>% select(-resistance))

set.seed(12)

xgb_main_bres <-
  xgb_cv_wf %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = xgb_set,
    # Generate 5x7 at semi-random to start
    initial = 35,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE)
  )

xgb_main_bres %>% 
  show_best(metric = "roc_auc")

# mtry = 74 or 13
# min_n = 2 or 2
# tree_depth = 10 or 7
# learn_rate = 0.18751350 or 0.10381054
# loss_reduction = 5.456852e-07 or 1.287068e-06
# sample_size = 0.6358425 or 0.6781397
# stop_iter = 6 or 6
# roc_auc = 0.8840152 or 0.8811742	
# n = 100
```

### Metrics autoplot

```{r, fig.width=14, fig.height=8}
autoplot(xgb_main_bres)
```

### Best models

```{r}
xgb_main_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC of the best

```{r}
make_roc(xgb_main_bres, "XGB (bayes)") %>% autoplot()
```

### Variable Importance

```{r}
xgb_main_bres %>% vip() 
```

## BT with NCORR recipe

```{r, fig.width=14, fig.height=8}
xgb_ncorr_bres <- readRDS(paste0(models_path, "btb_ncorr.rds"))

xgb_ncorr_bres %>% autoplot()
```

### Best models

```{r}
xgb_ncorr_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```


### ROC

```{r}
make_roc(xgb_ncorr_bres, "XGB (no corr)") %>% autoplot()
```

## BT with PCA recipe

```{r, fig.width=14, fig.height=8}
xgb_pca_bres <- readRDS(paste0(models_path, "btb_pca.rds"))

xgb_pca_bres %>% autoplot()
```

### Best models

```{r}
xgb_pca_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_pca_bres, "XGB (PCA)") %>% autoplot()
```

**NB**: the following warnings were raised during resampling process:

> ! There are 8 tuning parameters and 8 grid points were requested. •
> There are as many tuning parameters as there are initial points. This
> is likely to cause numerical issues in the first few search
> iterations. ! The Gaussian process model is being fit using 8 features
> but only has 8 data points to do so. This may cause errors or a poor
> model fit. ! The Gaussian process model is being fit using 8 features
> but only has 9

# Neural network

```{r}
nnet_mod <- set_model("nnet", 10)

nnet_wf <- set_wf("nnet", "ncorr", 10)
```


## NCORR recipe + space filling

```{r}
nnet_ncorr_res <- readRDS(paste0(models_path, "nn_ncorr.rds"))

nnet_ncorr_res %>% autoplot()
```

## Best models

```{r}
nnet_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## ROC

```{r}
make_roc(nnet_ncorr_res, "MLP") %>% autoplot()
```



## NCORR + keras + space filling

```{r}
nnet_ncorr_kres <- readRDS(paste0(models_path, "nn_keras_ncorr.rds"))

nnet_ncorr_kres %>% autoplot()
```

```{r}
nnet_ncorr_kres %>% show_best("roc_auc", n = 5)
```



# ROC of all models

```{r, fig.width=8, fig.height=6}
# calculate rocs
models_list <- list(knn_ncor_bres, lr_ncorr_res, lr_pca_res, mars_ncorr_res, mars_pca_res, rf_main_bres, rf_ncorr_bres, xgb_main_bres, xgb_ncorr_bres, xgb_pca_bres, svm_ncorr_res, svm_poly_res, nnet_ncorr_res)

models_names <- c("KNN (no corr)", "LR (no corr)", "LR (PCA)", "MARS (no corr)", "MARS (PCA)", "RF (all)", "RF (no corr)", "XGB (all)", "XGB (no corr)", "XGB (PCA)", "SVM lin (no corr)", "SVM poly (no corr)", "MLP (no corr)")


roc_df <- map2_dfr(models_list, models_names, function(x, y) make_roc(x, y))

roc_plot <- roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(direction = -1, option = "H")+
  theme_bw()

roc_plot
```

XGB is the best, LR no corr is the second

XGB here looks better than in previous version (it reaches sensitivity =
1 faster)

```{r}
ggsave("plots/roc.png", roc_plot)
```

# Final fit

## Model and workflow

```{r}
# model spec
xgb_mod <- set_model("bt", 10)

best <- xgb_ncorr_bres %>% 
  select_best("roc_auc")

last_mod <-
  boost_tree(
    mtry = best$mtry,
    min_n = best$min_n,
    tree_depth = best$tree_depth,
    learn_rate = best$learn_rate,
    loss_reduction = best$loss_reduction,
    sample_size = best$sample_size,
    stop_iter = best$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads = 10) %>% 
  set_mode("classification") 

# this was the XGB workflow
xgb_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(ncorr_recipe)

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

## Features importance

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 77, include_type = T) 
```

Sequencing related features (coverage and read length) are here - among
the most important

## Final model's parameters

```{r}
final_fit %>% 
  extract_fit_parsnip() 
```

## ROC

```{r}
final_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

## Confusion Matrix

Probability cut-off = 0.5

```{r, fig.width=6}
cm <- final_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

Previous both CM and performance (on full set of predictors) looked a
worse.

T.ex.: 
accuracy 0.8279570
kap  0.4723404
sens  0.6470588
spec  0.8684211
ppv  0.5238095
npv  0.9166667
mcc  0.4765046
j_index  0.5154799
bal_accuracy  0.7577399
detection_prevalence  0.2258065

The final fit from December run without full set of strains was like
this:

T.ex: PPV = 0.57, NPV = 0.92, acc = 0.816

## Probability cutoff adjustment by maximum j-index

```{r, fig.width=6}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  final_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# max_j_index_threshold may be a vector, use its last element
if (length(max_j_index_threshold) > 1){
  max_j_index_threshold <- max_j_index_threshold[length(max_j_index_threshold)]
}

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: Boosted Trees"
  )

sens_spec_j_plot
```

```{r}
ggsave("plots/prob_cutoff_BT_ncorr_final.png", sens_spec_j_plot)
```

What performance will I get using this maximal j-index?

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)

```{r, fig.width=6}
pred_optimized <- final_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

autoplot(cm_optimized, type = "heatmap")
```

### Optimized performance

```{r}
summary(cm_optimized)
```


## Extract predictions

### Collect False Positives

Should be 9 of them

```{r}
test_row_numbers <- df %>% 
  select(strain, resistance) %>% 
  mutate(.row = row_number())

xgb_last_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "nonHR") %>% 
  select(strain)
  
```

### Collect False Negatives

Should be 4 of them

```{r}
xgb_last_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "HR") %>% 
  select(strain)
```


------------------------------------------------------------------------

# Models stacking

For successful stacking, you need the same set of predictors in every candidate model.

My choice: LR (no corr), LSVM (no corr), XGB (all), RF (all)

```{r}
# models_names

map(models_list[c(2, 6, 7, 8, 9)], function(x) show_best(x, metric = "roc_auc", n = 1)) 
```

Models must be trained using the same set of features!

## Two types of stacks

```{r}
library(stacks)

# full features set
test_stack_1 <- 
  stacks() %>% 
  add_candidates(models_list[[6]]) %>%
  add_candidates(models_list[[7]])

# no corr set
ncorr_stack <-
  stacks() %>% 
  add_candidates(models_list[[1]]) %>%
  add_candidates(models_list[[2]]) %>% 
  add_candidates(models_list[[7]]) %>% 
  add_candidates(models_list[[9]]) %>% 
  add_candidates(models_list[[11]]) %>% 
  add_candidates(models_list[[12]])
  
```


## NCORR stack

```{r}
ncorr_stack %>% 
  blend_predictions(penalty = seq(0.01, 0.5, 0.02)) %>% 
  autoplot()
```

Let's choose penalty = 0.07

```{r}
ncorr_stack_model <- 
  ncorr_stack %>% 
  blend_predictions(penalty = 0.07)

autoplot(ncorr_stack_model)
```

```{r}
autoplot(ncorr_stack_model, type = "weights")
```

```{r}
ncorr_stack_model <- 
  ncorr_stack_model %>% 
  fit_members()

ncorr_stack_model
```

```{r}
data_pred <- df_test %>% 
  bind_cols(predict(ncorr_stack_model, ., type = "prob"))

yardstick::roc_auc(data_pred, truth = resistance, contains(".pred_HR"))
```

Remainder: the final fit's AUC = 0.8939628


```{r}
# updated workspace
save.image("data/all_models_local_update.RData")
```
