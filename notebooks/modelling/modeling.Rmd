---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(embed) # for UMAP

# path for models
models_path <- "~/Data/HeteroR/notebooks/models/scheme123/"
models_path_cv5 <- "~/Data/HeteroR/notebooks/models/scheme123/folds5/"
```

# Read and process data

The data sets are the same as in EDA

```{r}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf"))

# TWO SCHEMES
hr_testing12 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr12)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr12 )

hr_testing13 <- read_csv("data/heteroresistance_testing.csv", col_select = c(strain, Gr13)) %>% 
  filter(!is.na(strain)) %>% 
  rename("resistance" = Gr13 )

# created during EDA
hr_testing <- read_csv("data/heteroresistance_testing_gr123.csv") 

# USE HR 1+2+3
data_strain <- data_strain %>% 
  left_join(hr_testing, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \> 3 and \>4

```{r}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.3 = factor(ifelse(n.beta.lac > 3, "yes", "no"))) %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.3, n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain$NA. <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```


# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Basic Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe
```

### Bake and check

```{r}
train_data <- main_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

head(train_data)
```

```{r}
train_data %>% group_by(resistance) %>% count()
```

## PCA + ORQ-normalization

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

**NB:** As comparison in EDA notebook showed, OQ-normalization works
better than Yeo-Johnson transformation.

For PCA recipe I will use ORQ-normalization.

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  update_role(resistance, new_role = "outcome") %>% 
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 20) %>% 
  step_normalize(all_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

pca_recipe
```

## UMAP and Ordered-Quantile normalization

I use supervised UMAP

What position of `step_smote()` is preferable??

```{r, fig.width=12, fig.height=12}
umap_recipe <- recipe(resistance ~., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_umap(all_numeric_predictors(), outcome = "resistance", num_comp = 14) 

# to make a plot
# data_umap <- prep(umap_rec, retain = TRUE)

```

### Add SMOTE step

```{r}
umap_recipe <- umap_recipe %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

umap_recipe
```

## No correlation recipe

with manually set correlation threshold

*NB* Dummies are created after normalization and transformation! (This recipe performed better thn others)

```{r}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = 0.75) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```

## No correlation + ORQ recipe

Dummies are created after normalization and transformation - same as in NCORR (before it was different: dummies before order norm!)

```{r}
ncorq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_corr(threshold = 0.75) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to une hyper-parameters:

- the maximum entropy design (space-filling grid) which used in `tune()`

- Bayesian optimization of hyper-parameters which is used by `tune_bayes()`. 
Performs better than space-filling grid for models that are sensitive to hyper-parameters (like boosted trees)

# Functions to simplify things

To make model creation less repetitive

```{r}

## MAKE MODEL SPECIFICATION ##

set_model <- function(mod, cores) {
  if (mod == "lr") {
    my_mod <- logistic_reg(
        penalty = tune(), 
        mixture = 1) %>% 
      set_engine("glmnet")
  } else if (mod == "mars") {
    my_mod <- mars(
      mode = "classification",
      engine = "earth",
      num_terms = tune(),
      prod_degree = tune(),
      prune_method = "backward") %>% 
      translate()
  } else if(mod == "svm") {
    my_mod <- svm_linear(
        cost = tune()) %>% # margin - for regression only
      set_mode("classification") %>%
      set_engine("kernlab")  # default
  } else if(mod == "rf") {
    my_mod <- rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
      set_engine("ranger", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "bt") {
    my_mod <- boost_tree(
      trees = 50, 
      mtry = tune(), 
      min_n = tune(), 
      tree_depth = tune(), 
      learn_rate = tune(), 
      loss_reduction = tune(), 
      sample_size = tune(), 
      stop_iter = tune()) %>% 
      set_engine("xgboost", num.threads = cores) %>% 
      set_mode("classification")
  } else if(mod == "knn") {
    my_mod <- nearest_neighbor(
      neighbors = tune(),
      weight_func = tune(),
      dist_power = tune()) %>%
      set_engine("kknn") %>%
      set_mode("classification")
  } else if (mod == "nnet"){
    my_mod <-
      mlp(hidden_units = tune(), 
          penalty = tune(), 
          epochs = tune()) %>%
      set_mode("classification") %>%
      set_engine("nnet", num.threads = cores)
  }
  return(my_mod)
}

## SET WORKFLOW ##

set_wf <- function(mod, rec, cores){
  # mod: model type, one of: lr, knn, mars, svm, rf, bt
  # rec: recipe object (one of: main, ncorr, pca, umap)
  # rec must be in GlobalEnv
  
  if (rec == "main"){
    rc <- main_recipe
  } else if (rec == "ncorr"){
    rc <- ncorr_recipe
  } else if (rec == "pca") {
    rc <- pca_recipe
  } else if (rec == "umap") {
    rc <- umap_recipe
  } else {
    print("ERROR! Undefined recipe!")
  }

  wf <- workflow() %>% 
    add_model(set_model(mod = mod, cores = cores)) %>% 
    add_recipe(rc)
  
  return(wf)
}

## MK ROC OBJECT ##
# for autoplot()

make_roc <- function(mod_res, title){
  
  mod_best <- mod_res %>%
    select_best(metric = "roc_auc")
  
  mod_auc <- mod_res %>% 
    collect_predictions(parameters = mod_best) %>% 
    roc_curve(resistance, .pred_HR) %>% 
    mutate(model = title)
  
  return(mod_auc)
}

# Also, there is a function 'run_models.R' to run resampling from terminal
```

# Penalized LR

## UMAP

If you want to use UMAP instead of real features, then you should not
use supervised UMAP.

Also, (supervised) UMAP recipe dumps core for whatever reason

## NCORR recipe

For LR I used space filling grid, cause Bayesian would cause errors
during resampling and the advantage it gives is marginal.

You can load an `.rds` file if it exists.

```{r, fig.width=10}
lr_ncorr_res <- readRDS(paste0(models_path, "lr_ncorr_rs20_prop75_rep20.rds"))

autoplot(lr_ncorr_res)
```

### Best models

```{r}
lr_ncorr_res %>% 
  show_best("j_index", n = 5) 
```


```{r}
lr_ncorr_res %>% 
  show_best("roc_auc", n = 5) 
```

#### ROC of the best model

```{r}
make_roc(lr_ncorr_res, "LR (no corr)") %>% 
  autoplot()
```

## NCORQ recipe

New ncorq with dummies after ORQ doesn't make any changes

```{r}
#lr_ncorq_res <- readRDS(paste0(models_path, "lr_ncorr_orqED.rds"))
lr_ncorq_res <- readRDS(paste0(models_path, "lr_ncorr_orq.rds"))

lr_ncorq_res %>% 
  show_best("roc_auc", n = 5)
```

```{r}
make_roc(lr_ncorq_res, "LR (orq)") %>% 
  autoplot()
```

Very close

## PCA recipe

Space-filling grid as well

load

```{r, fig.width = 10}
lr_pca_res <- readRDS(paste0(models_path, "lr_ncorr_rs20_prop75_pca20.rds"))


autoplot(lr_pca_res)
```


**NB**: these problems appear:

```{r, eval=F}
lr_pca_res$.notes[[99]]$note
```

> "Error in glmnet::glmnet(x = maybe_matrix(x), y = y, alpha = \~1,
> family ="binomial"): x should be a matrix with 2 or more columns"


### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```

Doesn't look good

### ROC

```{r}
make_roc(lr_pca_res, "LR (PCA)") %>% 
  autoplot()
```

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r}
mars_ncorr_res <- readRDS(paste0(models_path, "mars_ncorr.rds"))

autoplot(mars_ncorr_res)
```

### Best models


```{r}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
mars_auc <- make_roc(mars_ncorr_res, "MARS (no corr.)")

autoplot(mars_auc)
```

## PCA recipe

Space-filling grid

Works better than with NCORR recipe

Load:

```{r}
mars_pca_res <- readRDS(paste0(models_path, "mars_pca.rds"))

autoplot(mars_pca_res)
```

### Best models

```{r}
mars_pca_res %>% 
  show_best("roc_auc", n = 5)
```

# Linear support vector machines (lSVM)

## NCORR recipe

```{r}
lsvm_ncorr_res <- readRDS(paste0(models_path, "lsvm_ncorr.rds"))

autoplot(lsvm_ncorr_res)
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(lsvm_ncorr_res, "") %>% 
  autoplot()
```


## PCA recipe

```{r}
lsvm_pca_res <- readRDS(paste0(models_path, "lsvm_pca.rds"))

autoplot(lsvm_pca_res)
```

### Best models

```{r}
lsvm_pca_res %>% 
  show_best(metric = "roc_auc")
```

# Polynomial SVM

## NCORR recipe + space-filling grid

```{r, fig.width=10}
psvm_ncorr_res <- readRDS(paste0(models_path, "psvm_ncorr.rds"))

autoplot(psvm_ncorr_res)
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(psvm_ncorr_res, "") %>% 
  autoplot()
```


## NCORR resipe + Bayesian


```{r, fig.width = 12, fig.height=5}
psvm_ncorr_bres <- readRDS(paste0(models_path, "psvmb_ncorr.rds"))

autoplot(psvm_ncorr_bres)
```

### Best models

```{r}
psvm_ncorr_bres %>% 
  show_best(metric = "roc_auc")
```

### ROC

```{r}
make_roc(psvm_ncorr_bres, "") %>% 
  autoplot()
```

# K-nearest neighbors (KNN)

## NCORR recipe + Bayes

**NB**: I get these warnings while using Bayesian grid search

> Gaussian process model: did not converge in 10 iterations

Work with the following model with caution

```{r, fig.width = 12, fig.height=6}
knn_ncorr_bres <- readRDS(paste0(models_path, "knnb_ncorr.rds"))

knn_ncorr_bres %>% 
  autoplot()
```

### Best models

```{r}
knn_ncorr_bres %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
make_roc(knn_ncorr_bres, "") %>% 
  autoplot()
```

## NCORR + Space-filling

```{r}
knn_ncorr_res <- readRDS(paste0(models_path, "knn_ncorr.rds"))

autoplot(knn_ncorr_res)
```

### Best models

```{r}
knn_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

### ROC

```{r}
make_roc(knn_ncorr_res, "") %>% 
  autoplot()
```

# Naive Bayes

```{r}
nb_ncorr <- readRDS(paste0(models_path, "nb_ncorr.rds"))

nb_ncorr %>% autoplot()
```

```{r}
make_roc(nb_ncorr, "") %>% autoplot()
```


```{r}
nb_ncorr %>% show_best("roc_auc", n = 5)
```


# Random Forest (RF)

## Main recipe + Bayes

```{r}
rf_main_bres <- readRDS(paste0(models_path, "rfb_main.rds"))

rf_main_bres %>% 
  autoplot()
```

### Best models

```{r}
rf_main_bres %>% 
  show_best("roc_auc", n = 5)
```

### Best ROC

```{r}
make_roc(rf_main_bres, "") %>% 
  autoplot()
```

### Iterations

```{r}
rf_main_bres %>% 
  autoplot(type = "performance")
```


## Main recipe + Space-filling

```{r}
rf_main_res <- readRDS(paste0(models_path, "rf_main.rds"))

autoplot(rf_res)
```

### Best models

```{r}
# automatic choice of the best model
rf_main_res %>% 
  show_best(metric = "roc_auc")
```

### ROC

```{r}
make_roc(rf_main_res, "") %>% 
  autoplot()
```

## NCORR recipe + Bayes

```{r}
rf_ncorr_bres <- readRDS(paste0(models_path_cv5, "rfb_ncorr_i50_rs20.rds"))

rf_ncorr_bres %>% autoplot()
```

```{r}
rf_ncorr_bres %>% 
  autoplot(type = "performance")
```


### Best models

```{r}
rf_ncorr_bres %>% 
  show_best("roc_auc", n = 5)
```

### ROC 

```{r}
rf_ncorr_bres %>% 
  make_roc("") %>% 
  autoplot()
```


# Boosted Trees (BT)

Using Extreme Gradient Boosting

## Space-filling grid

Space filling grid proved to be less efficient in finding hyper-parameters than Bayesian optimization

The following models all use Bayesian tuning 

## Main recipe

Bayesian tuning

```{r, fig.width=14, fig.height=8}
xgb_main_bres <- readRDS(paste0(models_path, "btb.rds"))

autoplot(xgb_main_bres)
```

### Iterations

```{r}
autoplot(xgb_main_bres, type = "performance")
```

### Best models

```{r}
xgb_main_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_main_bres, "") %>% 
  autoplot()
```


## NCORR recipe


```{r, fig.width=14, fig.height=8}
xgb_ncorr_bres <- readRDS(paste0(models_path_cv5, "btb_ncorr_i50_rs20.rds"))

xgb_ncorr_bres %>% autoplot()
```

### Iterations

```{r}
xgb_ncorr_bres %>% 
  autoplot(type = "performance")
```


### Best models

```{r}
xgb_ncorr_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```


### ROC

```{r}
make_roc(xgb_ncorr_bres, "XGB (no corr)") %>% 
  autoplot()
```

**NB:** This ROC looks much better than any other ROC so far


## NCORR + Many iterations


```{r, fig.width=14}
xgb_ncorr_bres_long <- readRDS(paste0(models_path, "btb_ncorr_i100_rs30.rds"))

xgb_ncorr_bres_long %>% 
  autoplot()
```

### Best

```{r}
xgb_ncorr_bres_long %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_ncorr_bres_long, "") %>% 
  autoplot()
```

```{r}
xgb_ncorr_bres_long %>% 
  autoplot(type = "performance")
```


## NCORR + ORQ recipe


```{r, fig.width=14, fig.height=8}

xgb_ncorq_bres <- readRDS(paste0(models_path, "btb_ncorr_orqED_rs8_it50.rds"))  # new ncorq recipe with dummies after order norm

xgb_ncorq_bres %>% autoplot()

```

Performance per iteration

```{r}
xgb_ncorq_bres %>% autoplot(type = "performance")
```

I also tried 100 iterations with 8 starting points, it was not better than 50 iterations and 8 starting points, but the performance plot looks peculiar:

```{r, fig.width=10}
xgb_ncorq_bres_it100 <- readRDS(paste0(models_path, "btb_ncorr_orqED_rs30_it100.rds"))

xgb_ncorq_bres_it100 %>% 
  autoplot(type = "performance")
```

Not closer to 0.9 then the default approach

```{r}
xgb_ncorq_bres_it100 %>% 
  show_best("roc_auc", n = 2)
```

Testing more starting points and new ncorq recipe version

```{r, fig.width=14, fig.height=8}
xgb_ncorq_bres_rs30_it50 <- readRDS(paste0(models_path, "btb_ncorr_orqED_rs30_it50.rds"))

xgb_ncorq_bres_rs30_it50 %>% autoplot()
```

```{r}
xgb_ncorq_bres_rs30_it50 %>% autoplot(type = "performance")
```


```{r}
xgb_ncorq_bres_rs30_it50 %>% show_best("roc_auc", n = 5)
```

Same as with default number of starting points

### Best models

```{r}
xgb_ncorq_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

Hm, looks slightly better then no corr without ORQ

```{r}
make_roc(xgb_ncorq_bres, "BT (no corr + ORQ") %>% autoplot()
```


## BT with PCA recipe

```{r, fig.width=14, fig.height=8}
xgb_pca_bres <- readRDS(paste0(models_path, "btb_pca.rds"))

xgb_pca_bres %>% autoplot()
```

### Best models

```{r}
xgb_pca_bres %>% 
  show_best(metric = "roc_auc", n = 5)
```

### ROC

```{r}
make_roc(xgb_pca_bres, "XGB (PCA)") %>% autoplot()
```

# Multilayer perceptron, engine 'nnet'

```{r}
nnet_mod <- set_model("nnet", 10)

nnet_wf <- set_wf("nnet", "ncorr", 10)
```


Applying ORQ-normalization improves MLP (nnet) performance

## NCORR + space-filling 8 starting points (default) 

```{r, fig.width=12}
nnet_ncorr_res <- readRDS(paste0(models_path, "mlp_nnet_ncorr.rds"))

nnet_ncorr_res %>% autoplot()
```


```{r}
make_roc(nnet_ncorr_res, "") %>% autoplot()
```

```{r}
nnet_ncorr_res %>% show_best("roc_auc")
```

## NCORR + Bayes 20 starting points

```{r, fig.width=12}
nnet_ncorr_bres <- readRDS(paste0(models_path_cv5, "mlpb_nnet_ncorr_i50_rs20.rds"))

nnet_ncorr_bres %>% autoplot()
```

```{r}
nnet_ncorr_bres %>% 
  autoplot(type = "performance")
```

```{r}
nnet_ncorr_bres %>% show_best("roc_auc", n = 5)
```

```{r}
make_roc(nnet_ncorr_bres, "") %>% 
  autoplot()
```

```{r}
cm <- nnet_ncorr_bres %>%
  select_best("roc_auc") %>% 
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm)
```


## NCORQ recipe + space filling (nnet)

```{r, fig.width=10}
nnet_ncorr_res <- readRDS(paste0(models_path, "mlp_nnet_ncorr_orq.rds"))

nnet_ncorr_res %>% autoplot()
```

## Best models

```{r}
nnet_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## ROC

```{r}
make_roc(nnet_ncorr_res, "MLP") %>% autoplot()
```


## NCORQ + Bayes opt (nnet)


```{r}
mlp_ncorq_bres <- readRDS(paste0(models_path, "mlpb_nnet_ncorr_orq.rds"))

mlp_ncorq_bres %>% autoplot()
```

### Best ones

```{r}
mlp_ncorq_bres %>% 
  show_best("roc_auc", n = 5)
```


### ROC

```{r}
make_roc(mlp_ncorq_bres, "MLP") %>% autoplot()
```

Not much differs from space filling

## NCORR + space filling (keras)

```{r}
nnet_ncorr_kres <- readRDS(paste0(models_path, "mlp_keras_ncorr.rds"))

nnet_ncorr_kres %>% autoplot()
```

```{r}
nnet_ncorr_kres %>% show_best("roc_auc", n = 5)
```

## NCORQ + Bayes (keras)

```{r}
mlp_ncorq_kbres <- readRDS(paste0(models_path, "mlpb_keras_ncorr_orq.rds"))

mlp_ncorq_kbres %>% autoplot()
```

```{r}
mlp_ncorq_kbres %>% show_best("roc_auc", n = 5)
```


```{r}
make_roc(mlp_ncorq_kbres, "MLP (no corr + orq)") %>% autoplot()
```

## Bagged MLP


```{r}
library(baguette)

bag_mlp_mod <- bag_mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>% 
      set_engine("nnet", num.threads = 8) %>% 
      set_mode("classification") %>% 
      translate()

bag_mlp_mod
```

Error:

! parsnip could not locate an implementation for `bag_mlp` classification model specifications using the `nnet`
  engine.
ℹ The parsnip extension package baguette implements support for this specification.
ℹ Please install (if needed) and load to continue.

Bagged Neural Network Model Specification (classification)

Main Arguments:
  hidden_units = tune()
  penalty = tune()
  epochs = tune()

Engine-Specific Arguments:
  num.threads = 8

Computational engine: nnet 

# ROC of all models

```{r, fig.width=8, fig.height=6}
# calculate rocs

roc_df <- map2_dfr(list(lr_ncorr_res, rf_ncorr_bres, xgb_ncorr_bres, nnet_ncorr_res, lsvm_ncorr_res, psvm_ncorr_res, knn_ncorr_res, nb_ncorr, nnet_ncorr_bres, xgb_ncorr_bres_long),
                            c("LR", "RF", "BT", "MLP", "linear SVM", "polynomial SVM", "KNN", "NB", "MLP b", "BT l"), 
                            function(x, y) make_roc(x, y))
roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(direction = -1, option = "H")

```

XGB is the best, LR no corr is the second

XGB here looks better than in previous version (it reaches sensitivity =
1 faster)

# ROC of the best ones

LR no corr, BT no corr, BT no corr ORQ, BT all, MLP (nnet no corr)

```{r, fig.width=8, fig.height=6}
roc_selected_df <- map2_dfr(list(lr_ncorr_res, nnet_ncorr_bres, xgb_ncorr_bres, rf_ncorr_bres),
                            c("LR", "MLP b", "XGB", "RF"), 
                            function(x, y) make_roc(x, y))

roc_selected_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  theme_bw()
```


# Final fit

> emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set.


## Model and workflow

```{r}
# model spec
xgb_mod <- set_model("bt", 8)

best <- xgb_ncorr_bres %>% 
  select_best("roc_auc")

last_mod <-
  boost_tree(
    mtry = best$mtry,
    min_n = best$min_n,
    tree_depth = best$tree_depth,
    learn_rate = best$learn_rate,
    loss_reduction = best$loss_reduction,
    sample_size = best$sample_size,
    stop_iter = best$stop_iter
  ) %>% 
  set_engine("xgboost", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
xgb_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(ncorr_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  xgb_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```
**NB**: This is forbidden action!

```{r}
# model spec
lr_mod <- set_model("lr", 8)

best <- lr_ncorr_res %>% 
  select_best("roc_auc")

last_mod <-
  logistic_reg(
    penalty = best$penalty
  ) %>% 
  set_engine("glmnet", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
lr_wf <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(ncorr_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lr_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

```{r}
# model spec
mlp_mod <- set_model("nnet", 2)

best <- nnet_ncorr_bres %>% 
  select_best("roc_auc")

last_mod <-
  mlp(
    hidden_units = best$hidden_units,
    penalty = best$penalty,
    epochs = best$epochs
  )%>% 
  set_engine("nnet", num.threads = 4) %>% 
  set_mode("classification") 

# this was the XGB workflow
mlp_wf <- workflow() %>% 
  add_model(mlp_mod) %>% 
  add_recipe(ncorr_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  mlp_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```



```{r}
# model spec
lr_mod <- set_model("lr", 8)

best <- lr_pca_res %>% 
  select_best("roc_auc")

last_mod <-
  logistic_reg(
    penalty = best$penalty
  ) %>% 
  set_engine("glmnet", num.threads = 8) %>% 
  set_mode("classification") 

# this was the XGB workflow
lr_wf <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(pca_recipe) # the recipe here should be the one that was used for this model

# the last workflow
last_wf <- 
  lr_wf %>%  
  update_model(last_mod)

# the last fit
set.seed(345)

final_fit <- 
  last_wf %>% 
  last_fit(data_split)

# emulates the process where, after determining the best model, 
# the final fit on the entire training set is needed 
# and is then evaluated on the test set.

final_fit %>% 
  collect_metrics()
```

## Features importance

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 25, include_type = T) 
```

Sequencing related features (coverage and read length) are here - among
the most important

## Final model's parameters

```{r, eval=F}
final_fit %>% 
  extract_fit_parsnip() 
```

## ROC

```{r}
final_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

## Confusion Matrix

Probability cut-off = 0.5

```{r, fig.width=6}
cm <- final_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```


## Probability cutoff adjustment by maximum j-index

```{r, fig.width=8}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  final_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# max_j_index_threshold may be a vector, use its last element
if (length(max_j_index_threshold) > 1){
  max_j_index_threshold <- max_j_index_threshold[length(max_j_index_threshold)]
}

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: ?"
  )

sens_spec_j_plot
```

What performance will I get using this maximal j-index?

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)

```{r, fig.width=6}
pred_optimized <- final_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

autoplot(cm_optimized, type = "heatmap")
```

### Optimized performance

```{r}
summary(cm_optimized)
```


## Extract predictions

With probability threshold = 0.5

### Collect False Positives

Should be 8 of them

```{r}
test_row_numbers <- data_strain %>% 
  select(strain, resistance) %>% 
  mutate(.row = row_number())

fp_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "nonHR") %>% 
  select(strain) %>% 
  mutate(prediction = "FP")
  
fp_df
```

### Collect False Negatives

Should be 6 of them

```{r}
fn_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "HR") %>% 
  select(strain)%>% 
  mutate(prediction = "FN")

fn_df
```

```{r}
tp_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "HR", resistance.x == "HR") %>% 
  select(strain)%>% 
  mutate(prediction = "COR")

dim(tp_df)

tn_df <- final_fit %>%
  collect_predictions() %>% 
  select(.row, .pred_class, resistance) %>% 
  left_join(test_row_numbers, by = ".row") %>% 
  filter(.pred_class == "nonHR", resistance.x == "nonHR") %>% 
  select(strain)%>% 
  mutate(prediction = "COR")

dim(tn_df)

pred_df <- bind_rows(tp_df, tn_df, fp_df, fn_df)

dim(pred_df)
```


------------------------------------------------------------------------

# Models stacking

For successful stacking, you need the same set of predictors in every candidate model.

Models with NOCORR recipe were among the best

Models must be trained using the same set of features!

Each model in the same stack must have same resampling method!

## Make a BRES stack

```{r}
library(stacks)

# no corr set
ncorr_stack_bres <-
  stacks() %>% 
  add_candidates(rf_ncorr_bres) %>%
  add_candidates(xgb_ncorr_bres_long) %>% 
  add_candidates(nnet_ncorr_bres)

as_tibble(ncorr_stack_bres)
```

Evaluating the stack

```{r}
ncorr_stack_bres %>% 
  blend_predictions(penalty = seq(0.01, 0.4, 0.01)) %>% 
  autoplot()
```

Let's choose penalty

```{r}
ncorr_stack_model <- 
  ncorr_stack_bres %>% 
  blend_predictions(penalty = 0.03)
```

```{r}
autoplot(ncorr_stack_model, type = "weights")
```

```{r}
ncorr_stack_model <- 
  ncorr_stack_model %>% 
  fit_members()

ncorr_stack_model
```

```{r}
data_pred <- df_test %>% 
  bind_cols(predict(ncorr_stack_model, ., type = "class"))

npv(data_pred, truth = resistance, .pred_class)
```

```{r}
roc_curve(data = data_pred, truth = resistance, .pred_HR_nnet_ncorr_bres_1_18) %>% 
  autoplot()
```


```{r}
yardstick::kap(data_pred, truth = resistance, .pred_HR)
```



```{r}
# updated workspace
save.image("data/all_models_local_update.RData")
```
