---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(ggpubr)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(tidyposterior) # for Bayesian ANOVA
#library(embed) # for UMAP
library(finetune) # for win-loss tuning
source("functions.R")

# path for models
models_path <- "~/Data/HeteroR/results/models/scheme12/"
```

# Read and process data

The data sets are the same as in EDA

```{r read data}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf")) %>% 
    filter(!(strain %in% c("DA63246", "DA63068"))) %>%  
  select(-contains("oriC")) %>% 
  select(-contains("plus"))

hr_testing <- read_csv("data/heteroresistance_testing.csv") %>% 
  filter(!is.na(strain))

data_strain <- data_strain %>% 
  left_join(hr_testing, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \>4

```{r init processing}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  # mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  # relocate(n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))
# TEMPORARILY: remove n.beta.lac.chrom & nc.beta.lac.plasmid -> 1st one has negative numbers, 2nd one has many NAs
# reasons for both are currently unclear
data_strain$nc.beta.lac.plasmid <- NULL
data_strain$n.beta.lac.chrom <- NULL
data_strain$N50 <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r data split}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Base Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r main recipe}
base_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
base_recipe %>% prep() %>% juice() %>% dim()
```

Target class:

```{r}
base_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```


## Base + YJ transformation

```{r}
base_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## Base + ORQ transformation

```{r}
base_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## PCA

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

The following code chunk is borrowed from 'Tidy modeling with R': Chapter 13.2 'Evaluating the grid'

```{r pca+yj recipe}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>%
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_numeric_predictors())
```

## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r norm recipe}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + Yeo-Johnson transform

development of the approach above, but with decorrelation and no PCA

```{r yj recipe}
ncorr_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + ORQ-transform recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r orq recipe}
ncorr_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## Base + Boruta feature selection

from `colino` package

to use with RF and BT algorithms

fixed on local copy - the original package in the tibble creation has
`name = c("top_p", "threshold"),` instead of
`name = c("top_p", "threshold", "cutoff"),`

the git repository was cloned locally, corrected and the package
installed following <https://kbroman.org/pkg_primer/pages/build.html>

(this was done to employ the RFE steps)

```{r rfe recipe}
if (!require(colino, quietly = TRUE))
  devtools::install_github("andrewgull/colino")

base_boruta_recipe <- base_recipe %>%
  step_select_boruta(all_predictors(), outcome = "resistance")

base_boruta_recipe %>% prep() %>% juice() %>% ncol()
```

## Base + YJ transform + Boruta feature selection

```{r boruta+yj recipe}
base_yj_boruta_recipe <- base_yj_recipe %>% 
  step_select_boruta(all_predictors(), outcome = "resistance")

base_yj_boruta_recipe %>% prep() %>% juice() %>% ncol()
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r cv and metrics}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to tune hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Rule-based prediction

## Performance on Train data

```{r base rule}
rule_pred <- 
  df_train %>% 
  mutate(n.beta.lac.4 = if_else(n.beta.lac > 4, "yes", "no"),
    pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred)
```

```{r}
conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```

## performance on Test

```{r}
rule_pred <- 
  df_test %>% 
  mutate(n.beta.lac.4 = if_else(n.beta.lac > 4, "yes", "no"),
    pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```

## performance on full data set

```{r}
rule_pred <- 
  data_strain %>% 
  mutate(n.beta.lac.4 = if_else(n.beta.lac > 4, "yes", "no"),
    pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```

# Penalized LR

By default all models here are LASSO regression models, unless specific
type of penalization is stated.


## Base recipe 

```{r lr norm, fig.height=6, fig.width=6}
lr_base_res <- read_wfset("models_llr.rds", "base_LLR")

autoplot(lr_base_res)
```

### Best models

```{r}
lr_base_res %>% 
  show_best("roc_auc", n = 5) 
```

## Base + YJ recipe

```{r lr yj, fig.height=6, fig.width=6}
lr_base_yj_res <- read_wfset("models_llr.rds", "base_yj_LLR")

autoplot(lr_base_yj_res)
```

### Best models

```{r}
show_best(lr_base_yj_res)
```

```{r}
show_best(lr_base_yj_res, metric = "j_index")
```

The same model is the best

## Base + ORQ recipe

```{r lr orq}
lr_base_orq_res <- read_wfset("models_llr.rds", "base_orq_LLR")

lr_base_orq_res %>% autoplot()
```

### Best models

```{r}
lr_base_orq_res %>%
  show_best("roc_auc", n = 5)
```

## PCA recipe

```{r lr pca}
lr_pca_res <- read_wfset("models_llr.rds", "pca_LLR")

autoplot(lr_pca_res)
```

### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```



## Compare preprocessors

### ROCs overlapped

```{r}
lr_best_res <- list(lr_base_res,
                    lr_base_yj_res,
                    lr_base_orq_res)

lr_best_names <-
  c("base",
    "base_yj",
    "base_orq")
```


```{r lr compare roc}
lr_rocs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_roc(x, y))

lr_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, ROC curves")
```

### PR-curves overlapped

```{r lr compare pr}
lr_prs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_pr(x, y))

lr_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, PR curves")
```

### Generate distributions

*post hoc* analysis of resampling results generated by models - via
Bayesian ANOVA

```{r}
lr_comp <-
  res_comp_table(
    res_list = lr_best_res,
    mod_names = lr_best_names
  )

lr_posterior <-
  perf_mod(
    lr_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(lr_posterior)
```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(lr_posterior, seed = 100) 

summary(preproc_diff)
```

Columns `upper` and `lower` represent credible interval for the mean
difference between preprocessing's AUCs.

Positive mean value means that mean of norm \> mean of orq etc.

If CI doesn't include 0, then the difference is significant.

PCA performs significantly worse than the other models.

Base, YJ and ORQ pre-processing recipes don't differ significantly in ROC AUC, but there is a difference in PR curves: base has better PR-curve.



# Linear support vector machines (linSVM)

```{r}
# models in this file
readRDS(paste0(models_path, "models_svm.rds")) %>% 
  filter(grepl("linSVM", wflow_id)) %>% 
  pull(wflow_id)
```


## NCORR recipe

```{r lsvm norm}
lsvm_ncorr_res <- read_wfset("models_svm.rds", "ncorr_linSVM")

autoplot(lsvm_ncorr_res)
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ recipe

```{r}
lsvm_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_linSVM")

autoplot(lsvm_ncoryj_res)
```

### Best models

```{r}
lsvm_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + ORQ recipe

```{r}
lsvm_ncorq_res <-  read_wfset("models_svm.rds", "ncorr_orq_linSVM")

autoplot(lsvm_ncorq_res)
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```

## PCA

```{r}
lsvm_pca_res <-  read_wfset("models_svm.rds", "pca_linSVM")

autoplot(lsvm_pca_res)
```

### Best models

```{r}
show_best(lsvm_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROCs overlapped

```{r}
lsvm_best_res <- list(
    lsvm_ncorr_res,
    lsvm_ncoryj_res,
    lsvm_ncorq_res,
    lsvm_pca_res
  )
  lsvm_best_names <- c(
    "ncorr",
    "ncorr_yj",
    "ncorr_orq",
    "pca"
  )
```


```{r}
lsvm_rocs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_roc(x, y))

lsvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, ROC curves")
```

### PR-curves overlapped

```{r lsvm compare pr}
lsvm_prs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_pr(x, y))

lsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, PR curves")
```

### Distributions

```{r}
lsvm_comp <-
  res_comp_table(
    res_list = lsvm_best_res,
    mod_names = lsvm_best_names
  )

lsvm_posterior <-
  perf_mod(
    lsvm_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(lsvm_posterior)
```

### Difference estimates

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

# Polynomial support vector machines (polySVM)

## NCORR recipe

```{r psvm norm}
psvm_ncorr_res <- read_wfset("models_svm.rds", "ncorr_polySVM")

autoplot(psvm_ncorr_res)
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ

```{r}
psvm_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_polySVM")

autoplot(psvm_ncoryj_res)
```

### Best models

```{r}
show_best(psvm_ncoryj_res, metric = "roc_auc")
```


## NCORR + ORQ recipe

```{r}
psvm_ncorq_res <- read_wfset("models_svm.rds", "ncorr_orq_polySVM")

autoplot(psvm_ncorq_res)
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

## PCA 

```{r}
psvm_pca_res <- read_wfset("models_svm.rds", "pca_polySVM")

autoplot(psvm_pca_res)
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

## Comparison of the preprocessing steps

### ROCs overlapped

```{r}
psvm_best_res <- list(
    psvm_ncorr_res,
    psvm_ncorq_res,
    psvm_ncoryj_res,
    psvm_pca_res
  )

psvm_best_names <- c(
    "ncorr",
    "ncorr_orq",
    "ncorr_yj",
    "pca"
  )
```


```{r}
psvm_rocs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_roc(x, y))

psvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, ROC curves")
```

### PR curves overlapped

```{r}
psvm_prs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_pr(x, y))

psvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, PR curves")
```


### Distributions

```{r}
psvm_comp <-
  res_comp_table(
    res_list = list(
      psvm_ncorr_res,
      psvm_ncorq_res,
      psvm_ncoryj_res,
      psvm_pca_res
    ),
    mod_names = c(
      "ncorr",
      "ncorr_orq",
      "ncorr_yj",
      "pca_yj"
    )
  )

psvm_posterior <-
  perf_mod(
    psvm_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(psvm_posterior)
```

### Estimate the difference

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```

# Radial Basis Function SVM

## NCORR recipe

```{r, fig.width=8}
rbf_ncorr_res <- read_wfset("models_svm.rds", "ncorr_rbfSVM")

autoplot(rbf_ncorr_res)
```

### Best models

```{r}
show_best(rbf_ncorr_res, metric = "roc_auc")
```

## NCORR + YJ recipe

```{r, fig.width=8}
rbf_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_rbfSVM")

autoplot(rbf_ncoryj_res)
```

### Best models

```{r}
show_best(rbf_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ recipe

```{r, fig.width=8}
rbf_ncorq_res <- read_wfset("models_svm.rds", "ncorr_orq_rbfSVM")

autoplot(rbf_ncorq_res)
```

### Best models

```{r}
show_best(rbf_ncorq_res, metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
rbf_pca_res <- read_wfset("models_svm.rds", "pca_rbfSVM")

autoplot(rbf_pca_res)
```


```{r}
show_best(rbf_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROC curves overlapped

```{r}
rsvm_best_res <- list(
    rbf_ncorr_res,
    rbf_ncorq_res,
    rbf_ncoryj_res,
    rbf_pca_res
    
  )

rsvm_best_names <- c(
    "ncorr",
    "ncorr_orq",
    "ncorr_yj",
    "pca"
  )
```


```{r}
rbf_rocs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_roc(x, y))

rbf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, ROC curves")
```

### PR curves overlapped

```{r}
rsvm_prs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_pr(x, y))

rsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, PR curves")
```

### Distributions

```{r}
rbf_comp <-
  res_comp_table(
    res_list = rsvm_best_res,
    mod_names = rsvm_best_names
  )

rbf_posterior <-
  perf_mod(
    rbf_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rbf_posterior)
```

### Difference estimates

```{r}
rbf_preproc_diff <- contrast_models(rbf_posterior, seed = 100) 

summary(rbf_preproc_diff)
```

# Random Forest (RF)

## Base recipe

```{r rf base}
rf_base_res <- read_wfset("models_rf.rds", "base_RF")

autoplot(rf_base_res)
```

### Best models

```{r}
rf_base_res %>% 
  show_best("roc_auc", n = 5)
```

## Base + Boruta feature selection

```{r}
rf_base_bor_res <- read_wfset("models_rf.rds", "base_boruta_RF")

autoplot(rf_base_bor_res)
```

```{r}
show_best(rf_base_bor_res)
```

## Comparison of preprocessors

### ROCs overlapped

```{r}
rf_best_res <- list(rf_base_res, rf_base_bor_res)
  
rf_best_names <- c("base", "boruta")
```


```{r}
rf_rocs <-
  map2_dfr(rf_best_res, 
           rf_best_names,
           function(x, y)
             make_roc(x, y))

rf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, ROC curves")
```

### PR curves overlapped

```{r}
rf_prs <-
  map2_dfr(rf_best_res,
           rf_best_names,
           function(x, y)
             make_pr(x, y))

rf_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, PR curves")
```

### Distributions

```{r}
rf_comp <-
  res_comp_table(
    res_list = rf_best_res,
    mod_names = rf_best_names
  )

rf_posterior <-
  perf_mod(
    rf_comp,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rf_posterior)
```

### Difference estimates

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

# Gradient Boosted Trees (GBT)

BT are sensitive to hyper-parameters that's why, Bayesian grid search
should be preferable.

## Base recipe

space search followed by the Bayesian optimization

```{r, fig.width=10, fig.height=6}
bt_base_res <- read_wfset("models_bt.rds", "base_BT")

autoplot(bt_base_res)
```

### Best models

```{r}
bt_base_res %>% 
  show_best(metric = "roc_auc")
```


## Base recipe + Bayesian optimization

Bayesian search

```{r, fig.width=10, fig.height=6}
bt_base_bres <- readRDS(paste0(models_path, "models_bt_bres.rds"))

bt_base_bres %>% 
  autoplot()
```

### Best models

```{r}
bt_base_bres %>% 
  show_best(metric = "roc_auc") 
```

>Shrinkage (learning rate) is a gradient boosting regularization procedure that helps modify the update rule, which is aided by a parameter known as the learning rate. The use of learning rates below 0.1 produces improvements that are significant in the generalization of a model.

## Base recipe + Boruta feature selection

```{r, fig.width=10, fig.height=6}
bt_base_bor_res <- read_wfset("models_bt.rds", "base_boruta_BT")

autoplot(bt_base_bor_res)
```

### Best models

```{r}
show_best(bt_base_bor_res)
```

## Base recipe + Boruta + Bayesian search

```{r, fig.width=10, eval=FALSE}
bt_base_bor_bres <- readRDS(paste0(models_path, "base_bt_boruta_bres_light.rds"))

autoplot(bt_base_bor_bres)
```

```{r, eval=FALSE}
show_best(bt_base_bor_bres)
```

## Comparison of the preprocessors

### ROCs

```{r}
bt_best_res <- list(
    bt_base_res,
    bt_base_bres,
    bt_base_bor_res
    #bt_base_bor_bres
  )

bt_best_names <- c(
    "base",
    "base_bayes",
    "base_bor"
    #"base_bor_bayes"
  )

bt_rocs <-
  map2_dfr(bt_best_res,
  bt_best_names,
  function(x, y)
    make_roc(x, y))

bt_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, ROC curves")
```

### PR curves overlapped

```{r}
bt_prs <-
  map2_dfr(bt_best_res,
           bt_best_names,
           function(x, y)
             make_pr(x, y))

bt_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, PR curves")
```

### Posterior distributions

```{r}
bt_comp <-
  res_comp_table(
    res_list = bt_best_res,
    mod_names = bt_best_names
  )

bt_posterior <-
  perf_mod(
    bt_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(bt_posterior)
```

### Estimating the differences

```{r}
bt_preproc_diff <- contrast_models(bt_posterior, seed = 100) 

summary(bt_preproc_diff) 
```

# Multilayer perceptron (MLP)

aka feed-forward neural network

## NCORR recipe


```{r mlp ncorr, fig.width=8}
mlp_ncor_res <- read_wfset("models_mlp.rds", "ncorr_MLP")

autoplot(mlp_ncor_res)
```

### Best models

```{r}
mlp_ncor_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ recipe

```{r, fig.width=8}
mlp_ncoryj_res <- read_wfset("models_mlp.rds", "ncorr_yj_MLP")

mlp_ncoryj_res %>% autoplot()
```

### Best models

```{r}
mlp_ncoryj_res %>% 
  show_best("roc_auc")
```

## NCORR + ORQ

```{r, fig.width=8}
mlp_ncorq_res <- read_wfset("models_mlp.rds", "ncorr_orq_MLP")

mlp_ncorq_res %>% autoplot()
```

### Best models

```{r}
mlp_ncorq_res %>% show_best(metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
mlp_pca_res <- read_wfset("models_mlp.rds", "pca_MLP")

mlp_pca_res %>% autoplot()
```

### Best models

```{r}
mlp_pca_res %>% show_best("roc_auc")
```


# Bag MLP

## NCORR

```{r, fig.width=8}
mlp_bag_ncorr_res <- read_wfset("models_mlp.rds", "ncorr_MLP_BAG")

autoplot(mlp_bag_ncorr_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorr_res)
```

## NCORR + YJ

```{r, fig.width=8}
mlp_bag_ncoryj_res <- read_wfset("models_mlp.rds", "ncorr_yj_MLP_BAG")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ

```{r, fig.width=8}
mlp_bag_ncorq_res <- read_wfset("models_mlp.rds", "ncorr_orq_MLP_BAG")

autoplot(mlp_bag_ncorq_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorq_res, metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
mlp_bag_pca_res <- read_wfset("models_mlp.rds", "pca_MLP_BAG")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_pca_res, metric = "roc_auc")
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
mlp_best_res <- list(
    mlp_ncor_res,
    mlp_ncoryj_res,
    mlp_ncorq_res,
    mlp_pca_res,
    mlp_bag_ncorr_res,
    mlp_bag_ncoryj_res,
    mlp_bag_ncorq_res,
    mlp_pca_res
  )

mlp_best_names <- c(
    "ncorr",
    "ncorr_yj",
    "ncorr_orq",
    "pca",
    "bag_ncorr",
    "bag_ncorr_yj",
    "bag_ncorr_orq",
    "bag_pca"
  )

mlp_rocs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_roc(x, y))

mlp_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, ROC curves")
```

### PR curves overlapped

```{r}
mlp_prs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_pr(x, y))

mlp_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, PR curves")
```

### Posterior distributions

```{r}
mlp_comp <-
  res_comp_table(
    res_list = mlp_best_res,
    mod_names = mlp_best_names
  )

mlp_posterior <-
  perf_mod(
    mlp_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(mlp_posterior)
```

### Difference estimates

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```


# Comparison of models

One best from each group

LR YJ
rbfSVM ORQ
linSVM ORQ
polySVM ORQ
MLP BAG
GBT Bayes

## ROCs

```{r comparison1}
# you will need these two objects later
best_resamples_list <- list(
    lr_base_yj_res,
    lsvm_ncorq_res,
    psvm_ncorq_res,
    rbf_ncorq_res,
    mlp_bag_ncoryj_res,
    bt_base_bres
  )
best_resamples_names <- c(
    "LLR",
    "linear SVM",
    "poly SVM",
    "rbf SVM",
    "MLP BAG",
    "GBT"
  )

# calculate rocs
roc_df <-
  map2_dfr(best_resamples_list,
  best_resamples_names,
  function(x, y)
    make_roc(x, y))

Fig_all_rocs <- roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Dark2") +
  theme_minimal()

ggsave(file="manuscript/images/Fig_all_ROCs.png", 
       plot=Fig_all_rocs, width = 8, height=4, units = "in")

Fig_all_rocs
```

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each
resamples object

```{r}
set.seed(125)

mod_comparison <- res_comp_table(best_resamples_list,
                                 best_resamples_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

Fig_all_posterior <- autoplot(mod_posterior) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal() +
  theme(plot.margin = margin(
    t = 23,
    r = 15,
    b = 23,
    l = 15,
    unit = "pt"
  ))

ggsave(file="manuscript/images/Fig_all_posterior.png", 
       plot=Fig_all_posterior, width = 8, height=4, units = "in")

Fig_all_posterior
```

```{r, fig.width=8}
Fig_all_rocs_all_posterior <- ggarrange(Fig_all_rocs, Fig_all_posterior,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 1, common.legend = TRUE,
                    legend = "bottom")

ggsave(file="manuscript/images/Fig_all_rocs_all_posterior.png", 
       plot=Fig_all_rocs_all_posterior, width = 8, height=4, units = "in")

Fig_all_rocs_all_posterior
```

## Difference estimates

```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

Save this table

```{r}
write_csv(summary(mod_diff), "manuscript/tables/contrast_all_models.csv" )
```


### Insignificant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper > 0)
```

### Significant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper < 0)
```

```{r}
summary(mod_diff) %>% filter(lower > 0, upper > 0)
```


## Top models

LR is the simplest, MLP and GBT are significantly better in mean ROC AUC.

### ROC curves

```{r comparison2}
# calculate rocs
top_models <- list(
    lr_base_yj_res,
    bt_base_bres,
    mlp_bag_ncoryj_res)

top_names <- c("LLR", "GBT", "MLP")

roc3_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_roc(x, y))

Fig_top_rocs <- roc3_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  theme_minimal()

ggsave(file="manuscript/images/Fig_top_rocs.png", 
       plot=Fig_top_rocs, width = 8, height=4, units = "in")

Fig_top_rocs
```

### PR curves

```{r}
pr_top_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_pr(x, y))
pr_top_df %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0,1)) + 
  scale_color_brewer(palette = "Set1") +
  theme_minimal() +
  ggtitle("PR curves, top models")
```

### Difference estimates

```{r}
mod_comparison <- res_comp_table(top_models,
                                 top_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

Fig_top_posterior <- autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set1") +
  theme_minimal()

ggsave(file="manuscript/images/Fig_top_posterior.png", 
       plot=Fig_top_posterior, width = 8, height=4, units = "in")

Fig_top_posterior
```

```{r}
Fig_top_rocs_top_posterior <- ggarrange(Fig_top_rocs, Fig_top_posterior,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 1, common.legend = TRUE,
                    legend = "bottom")

ggsave(file="manuscript/images/Fig_top_rocs_top_posterior.png", 
       plot=Fig_top_rocs_top_posterior, width = 8, height=4, units = "in")

Fig_top_rocs_top_posterior
```



```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

write_csv(summary(mod_diff), "manuscript/tables/contrast_top_models.csv" )

summary(mod_diff)
```

# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Functions

```{r ff func}
# function to run last fit process
run_last_fit <-
  function(model_name,
           blank_spec,
           tuned_spec,
           recipe,
           train_test_split = data_split,
           return_fit = FALSE) {
    # last workflow
    last_wf <- workflow() %>%
      add_model(blank_spec) %>%
      add_recipe(recipe) %>%
      update_model(tuned_spec)
    
    set.seed(345)
    # final fit
    final_fit <- last_wf %>%
      last_fit(train_test_split)
    # final metrics
    final_metrics <-
      final_fit %>%
      collect_metrics() %>%
      select(.metric, .estimate) %>%
      mutate(model = model_name)
    if (return_fit) {
      return(list(final_metrics, final_fit))
    } else {
      return(list(final_metrics, NULL))
    }
  }

# required by models with ncorr recipes
update_corr_treshold <- function(base_recipe, step_no, corr_threshold) {
  # update step_corr threshold
  base_recipe$steps[[step_no]] <-
   recipes::update(base_recipe$steps[[step_no]], threshold = corr_threshold)
  
  return(base_recipe)
}

# best models
best_models <- map(top_models, ~select_best(., "roc_auc"))
```

## LR final

```{r lr ff}
set.seed(800)

# LR specs
lr_spec_blank <- logistic_reg(
  penalty = tune(),
  mixture = 1,
  engine = "glmnet",
  mode = "classification"
)

lr_spec_tuned <-
  logistic_reg(
    penalty = best_models[[1]]$penalty,
    mixture = 1,
    engine = "glmnet",
    mode = "classification"
  )

# LR last fit and metrics
lr_final <-
  run_last_fit("LR",
               lr_spec_blank,
               lr_spec_tuned,
               recipe = base_yj_recipe,
               return_fit = TRUE)

lr_final[[1]]
```



## GBT final

```{r gbt ff}
set.seed(300)

library(bonsai)

# GBT base specs /number 2/
gbt_spec_blank <- boost_tree(
      trees = tune(),
      #mtry = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune()) %>%
      set_engine("lightgbm", num.threads = 8) %>%
      set_mode("classification") %>% translate()

gbt_spec_tuned <- boost_tree(
  trees = best_models[[2]]$trees,
  #mtry = best_models[[2]]$mtry,
  min_n = best_models[[2]]$min_n,
  tree_depth = best_models[[2]]$tree_depth,
  learn_rate = best_models[[2]]$learn_rate,
  loss_reduction = best_models[[2]]$loss_reduction
) %>%
  set_engine("lightgbm", num.threads = 8) %>%
  set_mode("classification")

# GBT base last fit and metrics
gbt_base_final <-
  run_last_fit(
    "GBT",
    gbt_spec_blank,
    gbt_spec_tuned,
    recipe = base_recipe,
    return_fit = TRUE
  )

gbt_base_final[[1]]
```

## MLP final

```{r}
set.seed(100)

mlp_bag_spec <- bag_mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
      set_engine("nnet", num.threads = 8) %>%
      set_mode("classification") %>%
      translate()


mlp_bag_tuned <- bag_mlp(
  hidden_units = best_models[[3]]$hidden_units,
  penalty = best_models[[3]]$penalty,
  epochs = best_models[[3]]$epochs
) %>% 
  set_mode("classification") %>%
  set_engine("nnet", num.threads = 4)

corr_threshold_tuned <- best_models[[3]]$corr_tune

ncorr_yj_recipe_tuned <-
  update_corr_treshold(base_recipe = ncorr_yj_recipe,
                       step_no = 8,
                       corr_threshold = corr_threshold_tuned)

mlp_bag_ncoryj_final <-
  run_last_fit(
    "MLP",
    mlp_bag_spec,
    mlp_bag_tuned,
    recipe = ncorr_yj_recipe_tuned,
    return_fit = TRUE
  )

mlp_bag_ncoryj_final[[1]]
```


## ROCs

```{r ff roc, fig.width=9}
mod_labels <- c("LLR", "GBT")
mod_finals <- list(lr_final[[2]], gbt_base_final[[2]])

make_final_roc <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

rocs_final <- map2(
  mod_finals,
  mod_labels,
  ~ make_final_roc(.x, .y)
)

ggarrange(plotlist = rocs_final, nrow=1, ncol = 2)
```

## PRs

```{r pr, fig.width=9}
make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

prs_final <-  map2(
  mod_finals,
  mod_labels,
  ~ make_final_pr(.x, .y)
)

ggarrange(plotlist = prs_final,
          ncol = 2,
          nrow = 1)
```

## Confusion Matrix

Probability cut-off = 0.5

```{r ff cm, fig.width=10}
make_cm <- function(fit_obj, title="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + ggtitle(title)
}

cm_final <-
  map2(
    mod_finals,
    mod_labels,
    ~ make_cm(.x, .y)
  )

ggarrange(plotlist = cm_final, ncol = 2, nrow = 1)
```



## Performance

```{r ff metrics}
get_metrics <- function(fit_obj, label="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  summary() %>% 
  select(-.estimator) %>% 
  mutate(model = label)
}

map2_dfr(
  mod_finals,
  mod_labels,
  ~ get_metrics(.x, .y)) %>%
  pivot_wider(names_from = model, values_from = .estimate)
```

## Probability cutoff adjustment by maximum j-index

```{r, fig.height=8, fig.width=10}
senspec_plots <- map2(
  mod_finals,
  mod_labels,
  ~ senspec_plot(.x, .y)
)

ggarrange(plotlist = senspec_plots,
                  nrow = 2,
                  ncol = 1,
                  common.legend = T)
```


### LR prob cutoff

```{r}
Fig_LR_prob_cutoff <- senspec_plot(lr_final[[2]], "LR")

ggsave(file="manuscript/images/Fig_LR_prob_cutoff.png", 
       plot=Fig_LR_prob_cutoff, width = 8, height=4, units = "in")

Fig_LR_prob_cutoff
```


#### j-index and the probability threshold

```{r}
lr_final[[2]] %>%
  get_threshold() %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  slice_tail()
```

### GBT prob cutoff

```{r}
Fig_GBT_prob_cutoff <- senspec_plot(gbt_base_final[[2]], "GBT")

ggsave(file="manuscript/images/Fig_GBT_prob_cutoff.png", 
       plot=Fig_GBT_prob_cutoff, width = 8, height=4, units = "in")

Fig_GBT_prob_cutoff
```

#### j-index and the probability threshold

```{r}
gbt_base_final[[2]] %>%
  get_threshold() %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  slice_tail()
```

### LR and GBT prob cutoff together

```{r, fig.width=12}
Fig_LR_GBT_prob_cutoff <- ggarrange(plotlist = senspec_plots[1:2],
                  nrow = 1,
                  ncol = 2,
                  common.legend = T)

ggsave(file="manuscript/images/Fig_LR_GBT_prob_cutoff.png", 
       plot=Fig_LR_GBT_prob_cutoff, width = 8, height=4, units = "in")

Fig_LR_GBT_prob_cutoff
```


### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)


```{r opt cm, fig.width=9}
optimize_prediction <- function(fit.obj){
  prob_cutoff <- maxj(get_threshold(fit.obj))
  
  optimized <- fit.obj %>%
    collect_predictions() %>%
    mutate(.pred = make_two_class_pred(
      estimate = .pred_HR,
      levels = levels(resistance),
      threshold = prob_cutoff
    )) %>%
    select(resistance, contains(".pred"))
  
  return(optimized)
}

cm_optimized_plots <- map2(
  mod_finals,
  mod_labels,
  ~ optimize_prediction(.x) %>% conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle(.y)
)

ggarrange(plotlist = cm_optimized_plots,
          nrow = 1,
          ncol = 2,
          common.legend = F)
```

### Optimized performance

```{r opt perf}
get_metrics2 <- function(fit.obj, label){
  optimize_prediction(fit.obj) %>% 
    conf_mat(truth = resistance, estimate = .pred) %>% 
    summary() %>% 
    mutate(model = label)
}

map2_dfr(
  mod_finals,
  mod_labels,
  ~ get_metrics2(.x, .y)) %>%
  pivot_wider(names_from = model, values_from = .estimate) %>% 
  select(-.estimator)
```

# Post-modelling analysis

## Features used by LLR

### How many predictors were removed

Overall, we have `r ncol(df_train) - 2` predictors.

#### Predictors left after the first `step_nzv()`

```{r n nzv}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  juice() %>% 
  ncol()
```


#### Predictors left after the first and the second `step_nzv()`

```{r n corr}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>% prep() %>% juice() %>% ncol()
```

## Features importance: LLR

Non-zero LLR coefficients:

```{r lr coeff}
lr_coeff_df <- lr_final[[2]] %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate)) %>% 
  mutate(odds.ratio = 1/exp(estimate),
         term = gsub(term, pattern= "_", replacement = ".", fixed=T)) %>% 
  select(-penalty) %>%
  mutate(
    HR_probability = if_else(estimate < 0, "increased", "decreased"),
    category = case_when(
      grepl("IS", term) |
        grepl("rep", term) |
        grepl("AR", term) | grepl("is", term) ~ "repeats",
      grepl("beta", term) |
        grepl("transfer", term) |
        grepl("n.c.plasmids", term) |
        grepl("APH6", term) |
        grepl("AAC3", term) | grepl("ANT3", term) ~ "genes",
      grepl("linear", term) |
        grepl("coverage", term) ~ "technical"
    ),
    # make readable term names
    term = recode(term,
                  "n.beta.lac" = "number of β-lactamases",
                   "TEM.beta.lactamase" = "number of tem β-lactamases",
                   "ampC.type.beta.lactamase" = "number of ampC β-lactamases",
                  "med.AR.len.TEM" = "median AR length around tem β-lactamases",
                  "CTX.M.beta.lactamase" = "number of ctx-m β-lactamases",
                  "macrolide.phosphotransferase.MPH" = "number of mph genes",
                  "n.rep.100.plasmid" = "number of repeats longer than 100 nt on plasmids",
                  "n.is.fam" = "number of IS families",
                  "max.rep.len" = "maximal repeat length",
                  "IS30" = "number of IS30 elements",
                  "med.AR.len.cen.plasmid" = "median length of AR spanning RG on plasmdis",
                  "min.distance.is.gene" = "distance between IS element and closest resistance gene",
                  "IS5" = "number of IS5 elements",
                  "ANT3" = "number of ANT3 genes",
                  "med.tot.rep.len.chrom" = "median repeat length on chromosome",
                  "ISNCY" = "number of ISNCY elements",
                  "med.AR.len.cen" = "median AR length",
                  "chloramphenicol.acetyltransferase.CAT" = "number of cat genes",
                  "n.is.tot" = "total number of IS families",
                  "AAC3" = "number of aac3 genes",
                  "IS21" = "number of IS21 elements",
                  "IS481" = "number of IS481 elements",
                  "ISL3" = "number of ISL3 elements",
                  "med.rep.len.ampC" = "median repeat length around ampC β-lactamases",
                  "n.beta.lac.plasmid" = "number of β-lactamases on plasmids",
                  "ISAS1" = "number of ISAS1 elements",
                  "IS3" = "number of IS3 elements",
                  "IS1" = "number of IS1 elements",
                  "med.AR.len.plasmid" = "median AR length on plasmids",
                  "med.tot.rep.len" = "median repeat length",
                  "max.rep.len.cen" = "maximal length of repeats spanning RG",
                  "IS200/IS605" = "number of IS200/IS605 elements",
                  "med.AR.len.plasmid.TEM" = "median AR length around tem β-lactamases on plasmids",
                  "med.rep.len.plasmid.TEM" = "median repeat length around tem β-lactamases on plasmids",
                  "IS630" = "number of IS630 elements",
                  "max.is.len" = "maximal IS element length",
                  "min.AR.len.cen" = "minimal AR length",
                  "med.rep.len.chrom" = "median repeat length on chromosomes",
                  "n.rep.chrom.ampC" = "number of repeats around ampC β-lactamases on chromosomes",
                  "min.AR.len.cen.chrom" = "minimal AR length on chromosomes",
                  "IS110" = "number of IS110 elements",
                  "IS4" = "number of IS4 elements",
                  "n.rep.500.cen.tot" = "number of repeats longer than 500 nt",
                  "chrom.status.linear" = "incomplete genome assembly",
                  "n.rep.100.total" = "number of repeats longer than 100 nt",
                  "sulfonamide.resistant.sul" = "number of sul genes",
                  "trimethoprim.resistant.dihydrofolate.reductase.dfr" = "number of dfr genes",
                  "IS6" = "number of IS6 elements",
                  "IS66" = "number of IS66 elements",
                  "APH6" = "number of aph6 genes",
                  "n.c.plasmids" = "copy number of plasmids"
                  )) %>%
  filter(term != "(Intercept)")

lr_coeff_df
```

On the `odds.ratio` column: 

- units in which predictors are measured have been transformed

- apparently, nonHR is outcome 1 here and HR is outcome 0, because increase in `n.beta.lac` decreases chances of outcome 1.


### Predictors decreasing the risk of HR:

```{r feat pos}
lr_coeff_df %>% 
  filter(odds.ratio < 1.0) %>% 
  select(term)
```

### Predictors increasing the risk of HR

```{r feat neg}
lr_coeff_df %>% 
  filter(odds.ratio >= 1.0) %>% 
  select(term)
```

### Plots

```{r, fig.width=8, fig.height=6}
Fig_LR_predictors <- lr_coeff_df %>%
  mutate(effect = if_else(estimate < 0, "increase HR prob.", "decrease HR prob.")) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(reorder(term, odds.ratio, decreasing = F), odds.ratio)) +
  geom_col(aes(fill = effect)) +
  scale_fill_brewer(palette = "Paired", guide = guide_legend(title = NULL)) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.9,
      hjust = 0.9,
      size = 8
    ),
    legend.position = "bottom"
  ) +
  xlab("") +
  ylab("odds ratio") +
  coord_flip()

ggsave(file="manuscript/images/Fig_LR_predictors.png", plot=Fig_LR_predictors, 
       width = 10, height=6, units = "in")

Fig_LR_predictors
```

Alternative 1: categories of predictors + a break

```{r, fig.width=8, fig.height=6}
library(ggbreak) # for y/x axis break
library(scales) # for labels wrapping

Fig_LR_predictors_alt <-
  ggplot(lr_coeff_df,
         aes(reorder(term, -odds.ratio, decreasing = T), odds.ratio)) +
  geom_col(aes(fill = category)) +
  coord_flip() +
  scale_fill_brewer(
    palette = "Dark2",
    name =,
    direction = -1
  ) +
  scale_y_break(c(4, 29), ticklabels = c(30)) +
  theme(
    legend.position = "top",
    axis.text.x = element_text(
      vjust = 0.9,
      hjust = 0.9,
      size = 7
    )
  ) +
  labs(y = "odds ratio", x = NULL) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  theme_minimal()

ggsave(file="manuscript/images/Fig_LR_predictors_alt.png", 
       plot=Fig_LR_predictors_alt, width = 8, height=6, units = "in")

Fig_LR_predictors_alt
```

## Features used by GBT

[Source](https://ema.drwhy.ai/featureImportance.html#featureImportanceIntuition)

### Fit the model with selected (h)-params

```{r, eval=FALSE}
# IT'S THE WAY TO GET MLP FIT OBJECT W/O MY BULKY FUNCTIONS for final fit
# 
bt <- boost_tree(
      trees = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune(),
      sample_size = tune(),
      stop_iter = tune()) %>%
      set_engine("lightgbm", num.threads = 8) %>%
      set_mode("classification") %>% translate()

bt_best_param <- select_best(bt_base_bres, "roc_auc")
bt_model_final <- finalize_model(bt, bt_best_param)

# I need processed data
df_train_proc <- base_recipe %>% 
  prep() %>% 
  juice() %>% 
  select(-strain)

# fit the tuned model to the data
bt_fit <- fit(bt_model_final, resistance ~ ., df_train_proc)

bt_fit 

# the same as three functions above
# fit_best(mlp_bag_ncoryj_res)
```

## Features importance: GBT

Aka "global explanations"

```{r, fig.height=12, fig.width=9}
library(DALEXtra)
set.seed(1003)

df_test_proc <- base_recipe %>% 
  prep() %>% 
  bake(df_test) %>% 
  select(-c(resistance, strain))

explainer_final <- 
  explain_tidymodels(
    bt_fit, 
    data = df_test_proc, 
    y = if_else(df_test$resistance == "HR", 1, 0),
    label = "",
    verbose = FALSE
  )

vip_gbt <- model_parts(explainer_final, 
                       loss_function = loss_one_minus_auc,
                       B = 50, 
                       type = "difference",
                       N = NULL)

# Mean variable-importance over 50 permutations
Fig_GBT_predictors <- 
  plot(vip_gbt) +
  ggtitle("", "")


ggsave(file="manuscript/images/Fig_GBT_predictors.png", plot=Fig_GBT_predictors, width = 9, height=12, units = "in")

Fig_GBT_predictors
```


Alternative:

```{r, fig.height=12, fig.width=9}
library(forcats)

ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, 
                      "after permutations\n(higher indicates more important)")
  
  full_vip <- bind_rows(obj) %>%
    filter(variable != "_baseline_")
  
  perm_vals <- full_vip %>% 
    filter(variable == "_full_model_") %>% 
    group_by(label) %>% 
    summarise(dropout_loss = mean(dropout_loss))
  
  p <- full_vip %>%
    filter(variable != "_full_model_") %>% 
    mutate(variable = fct_reorder(variable, dropout_loss)) %>%
    ggplot(aes(dropout_loss, variable)) 
  if(length(obj) > 1) {
    p <- p + 
      facet_wrap(vars(label)) +
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)
  } else {
    p <- p + 
      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),
                 linewidth = 1.4, lty = 2, alpha = 0.7) +
      geom_boxplot(fill = "#1F78B4", alpha = 0.4)
    
  }
  p +
    theme(legend.position = "none") +
    labs(x = metric_lab, 
         y = NULL,  fill = NULL,  color = NULL)
}



# Mean variable-importance over 50 permutations
Fig_GBT_predictors_alt <- 
  ggplot_imp(vip_gbt)

ggsave(file="manuscript/images/Fig_GBT_predictors_alt.png", plot=Fig_GBT_predictors_alt, width = 9, height=12, units = "in")

Fig_GBT_predictors_alt
```



### Local explanations

You can choose for instance false positives and look what influenced
their classification as HR

#### Select FP/FN

```{r}
bt_model_final[[2]] %>% collect_predictions() %>% 
  mutate(FP = if_else(.pred_class == "HR" & resistance == "nonHR", "FP", NA),
         FN = if_else(.pred_class == "nonHR" & resistance == "HR", "FN", NA),
         TP = if_else(.pred_class == "HR" & resistance == "HR", "TP", NA),
         TN = if_else(.pred_class == "nonHR" & resistance == "nonHR", "TN", NA),
         strain = df_test$strain) %>%  
  select(-c(id, .row, .config))
```

#### Explain those of interest

>Model break-down explanations... depend on the order of the features

```{r, eval=FALSE}
library(DALEXtra)

df_test_proc <- base_recipe %>% prep() %>% bake(df_test) %>% 
  mutate(strain = df_test$strain) %>% 
  select(-resistance)

explainer_final <- 
  explain_tidymodels(
    bt_fit, 
    data = df_test_proc, 
    y = df_test$resistance,
    label = "GBT",
    verbose = FALSE
  )

# a function to make predictor's contribution table
show_contribution <- function(strain_name, explainer = explainer_final, data = df_test_proc){
  observation <- filter(data, strain == strain_name)
  contribution <- tibble(predict_parts(explainer, observation))
  return(contribution)
}

# local explanations
fp_contrib <- show_contribution("DA63072")
tp_contrib <- show_contribution("DA63150")

fp_contrib %>% arrange(-contribution)
```

```{r}
fp_contrib %>% 
  filter(variable_name != "") %>% 
  slice_max(order_by = abs(contribution), n = 20) %>% 
  ggplot(aes(reorder(variable_name, -contribution), contribution)) +
  geom_col(aes(fill=sign)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6, size=9, hjust = 0.5)) +
  xlab("") +
  ggtitle("DA63072")
```

```{r}
tp_contrib %>% 
  filter(variable_name != "") %>% 
  slice_max(order_by = abs(contribution), n = 20) %>% 
  ggplot(aes(reorder(variable_name, -contribution), contribution)) +
  geom_col(aes(fill=sign)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.7, size=9, hjust = 0.5)) +
  xlab("") +
  ggtitle("DA63150")
```




### SHAP

TP and FP

```{r, fig.width=10, fig.height=5}
library(forcats)
library(patchwork)

set.seed(1801)
shap_tp <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63150"), 
    type = "shap",
    B = 20
  )

# Shapley explanations for TP example
plot_shap_tp20 <- shap_tp %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63150, TP")

# Shapley explanations for FP example
shap_fp <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63004"), 
    type = "shap",
    B = 20
  )

plot_shap_fp20 <- shap_fp %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63004, FP")

# Compare
(plot_shap_tp20 | plot_shap_fp20)
```

TN and FN

```{r, fig.width=10, fig.height=5}
shap_tn <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA62896"), 
    type = "shap",
    B = 20
  )

shap_fn <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63072"), 
    type = "shap",
    B = 20
  )

plot_shap_tn20 <- shap_tn %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA62896, TN")

plot_shap_fn20 <- shap_fn %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63072, FN")

(plot_shap_tn20 | plot_shap_fn20)
```
