---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, message = F, warning = F, cache = F)
library(readr)
library(tidymodels)
library(ggpubr)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
library(bestNormalize) # for ord QQ norm
library(tidyposterior) # for Bayesian ANOVA
#library(embed) # for UMAP
library(finetune) # for win-loss tuning
source("functions.R")

# path for models
models_path <- "~/Data/HeteroR/results/models/scheme12/"
```

# Read and process data

The data sets are the same as in EDA

```{r read data}
data_strain <- read_csv("data/features_strain.csv", na = c("NA", "-Inf")) %>% 
    filter(!(strain %in% c("DA63310", "DA63246", "DA63068"))) %>%  
  select(-contains("oriC")) %>% 
  select(-contains("plus"))

hr_testing <- read_csv("data/heteroresistance_testing.csv") %>% 
  filter(!is.na(strain))

data_strain <- data_strain %>% 
  left_join(hr_testing, by = "strain")

data_strain %>% 
  group_by(resistance) %>% 
  count()
```

Add n.beta.lac \>4

```{r init processing}
strains <- data_strain$strain

data_strain <- data_strain %>% 
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>% 
  relocate(n.beta.lac.4, .before = "n.plasmids") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR")),
         chrom.status = factor(chrom.status))

data_strain$N50 <- NULL
data_strain[is.na(data_strain)] <- 0
```

```{r}
skim(data_strain) %>% yank("factor")
```

We have unbalanced target class

Number of not tested strains in the list I got from Karin

```{r}
hr_testing %>% filter(is.na(resistance))
```

# Data split

Stratified split

```{r data split}
set.seed(124)

data_split <- initial_split(data_strain, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing recipes

## Base Recipe

> Some models (notably neural networks, KNN, and support vector
> machines) require predictors that have been centered and scaled, so
> some model workflows will require recipes with these preprocessing
> steps. For other models, a traditional response surface design model
> expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r main recipe}
base_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
base_recipe %>% prep() %>% juice() %>% dim()
```

Target class:

```{r}
base_recipe %>% prep() %>% juice() %>% group_by(resistance) %>% count()
```


## Base + YJ transformation

```{r}
base_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## Base + ORQ transformation

```{r}
base_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
```


## PCA

From [here](https://www.tmwr.org/grid-search.html):

> Because of the high degree of correlation between predictors, it makes
> sense to use PCA feature extraction to decorrelate the predictors. The
> following recipe contains steps to transform the predictors to
> increase symmetry, normalize them to be on the same scale, then
> conduct feature extraction. The number of PCA components to retain is
> also tuned, along with the model parameters.

> While the resulting PCA components are technically on the same scale,
> the lower-rank components tend to have a wider range than the
> higher-rank components. For this reason, we normalize again to coerce
> the predictors to have the same mean and variance.

> Many of the predictors have skewed distributions. Since PCA is
> variance based, extreme values can have a detrimental effect on these
> calculations. To counter this, let's add a recipe step estimating a
> Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000).
> While originally intended as a transformation of the outcome, it can
> also be used to estimate transformations that encourage more symmetric
> distributions

The following code chunk is borrowed from 'Tidy modeling with R': Chapter 13.2 'Evaluating the grid'

```{r pca+yj recipe}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>%
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_numeric_predictors())
```

## No correlation recipe

with tuned correlation threshold

*NB* Dummies are created after normalization and transformation! (This
recipe performed better than others)

```{r norm recipe}
ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + Yeo-Johnson transform

development of the approach above, but with decorrelation and no PCA

```{r yj recipe}
ncorr_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## No correlation + ORQ-transform recipe

Dummies are created after normalization and transformation - same as in
NCORR (before it was different: dummies before order norm!)

```{r orq recipe}
ncorr_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))
```

## Base + Boruta feature selection

from `colino` package

to use with RF and BT algorithms

fixed on local copy - the original package in the tibble creation has
`name = c("top_p", "threshold"),` instead of
`name = c("top_p", "threshold", "cutoff"),`

the git repository was cloned locally, corrected and the package
installed following <https://kbroman.org/pkg_primer/pages/build.html>

(this was done to employ the RFE steps)

```{r rfe recipe}
if (!require(colino, quietly = TRUE))
  devtools::install_github("andrewgull/colino")

base_boruta_recipe <- base_recipe %>%
  step_select_boruta(all_predictors(), outcome = "resistance")

base_boruta_recipe %>% prep() %>% juice() %>% ncol()
```

## Base + YJ transform + Boruta feature selection

```{r boruta+yj recipe}
base_yj_boruta_recipe <- base_yj_recipe %>% 
  step_select_boruta(all_predictors(), outcome = "resistance")

base_yj_boruta_recipe %>% prep() %>% juice() %>% ncol()
```


# Folds and metrics

> If a model is poorly calibrated, the ROC curve value might not show
> diminished performance. However, the J index would be lower for models
> with pathological distributions for the class probabilities.

> Youden's J statistic is defined as: sens + spec - 1

> The value of the J-index ranges from [0, 1] and is 1 when there are no
> false positives and no false negatives.

```{r cv and metrics}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) 
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Grid

There are two ways to tune hyper-parameters:

-   the maximum entropy design (space-filling grid) which used in
    `tune()`

-   Bayesian optimization of hyper-parameters which is used by
    `tune_bayes()`. Performs better than space-filling grid for models
    that are sensitive to hyper-parameters (like boosted trees)

# Rule-based prediction

Let's take a rule 'if there is more than 4 beta-lactamases, then the strain is HR' (from EDA)

## Performance on Train data

```{r base rule}
rule_pred <- 
  df_train %>% 
  mutate(pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred)
```

```{r}
conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```

## performance on Test

```{r}
rule_pred <- 
  df_test %>% 
  mutate(pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```

## performance on full data set

```{r}
rule_pred <- 
  data_strain %>% 
  mutate(pred = as.factor(if_else(n.beta.lac.4 == "yes", "HR", "nonHR"))) %>% 
  select(pred, resistance)

conf_mat(rule_pred, truth = resistance, estimate = pred) %>% 
  summary()
```


# Penalized LR

By default all models here are LASSO regression models, unless specific
type of penalization is stated.


## Base recipe 

```{r lr norm, fig.height=6, fig.width=6}
lr_base_res <- read_wfset("models_llr.rds", "base_LLR")

autoplot(lr_base_res)
```

### Best models

```{r}
lr_base_res %>% 
  show_best("roc_auc", n = 5) 
```

## Base + YJ recipe

```{r lr yj, fig.height=6, fig.width=6}
lr_base_yj_res <- read_wfset("models_llr.rds", "base_yj_LLR")

autoplot(lr_base_yj_res)
```

### Best models

```{r}
show_best(lr_base_yj_res)
```

```{r}
show_best(lr_base_yj_res, metric = "j_index")
```

The same model is the best

## Base + ORQ recipe

```{r lr orq}
lr_base_orq_res <- read_wfset("models_llr.rds", "base_orq_LLR")

lr_base_orq_res %>% autoplot()
```

### Best models

```{r}
lr_base_orq_res %>%
  show_best("roc_auc", n = 5)
```

## PCA recipe

```{r lr pca}
lr_pca_res <- read_wfset("models_llr.rds", "pca_LLR")

autoplot(lr_pca_res)
```

### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```



## Compare preprocessors

### ROCs overlapped

```{r}
lr_best_res <- list(lr_base_res,
                    lr_base_yj_res,
                    lr_base_orq_res,
                    lr_pca_res)

lr_best_names <-
  c("base",
    "base_yj",
    "base_orq",
    "pca")
```


```{r lr compare roc}
lr_rocs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_roc(x, y))

lr_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, ROC curves")
```

### PR-curves overlapped

```{r lr compare pr}
lr_prs <-
  map2_dfr(lr_best_res,
  lr_best_names,
  function(x, y)
    make_pr(x, y))

lr_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("LR, PR curves")
```

### Generate distributions

*post hoc* analysis of resampling results generated by models - via
Bayesian ANOVA

```{r}
lr_comp <-
  res_comp_table(
    res_list = lr_best_res,
    mod_names = lr_best_names
  )

lr_posterior <-
  perf_mod(
    lr_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(lr_posterior)
```

### Estimate the Difference

```{r}
preproc_diff <- contrast_models(lr_posterior, seed = 100) 

summary(preproc_diff)
```

Columns `upper` and `lower` represent credible interval for the mean
difference between preprocessing's AUCs.

Positive mean value means that mean of norm \> mean of orq etc.

If CI doesn't include 0, then the difference is significant.

PCA performs significantly worse than the other models.

Base, YJ and ORQ pre-processing recipes don't differ significantly in ROC AUC, but there is a difference in PR curves: base has better PR-curve.

# Multivariate adaptive regression splines (MARS)

## NCORR recipe

```{r mars norm, eval=FALSE}
mars_ncorr_res <- read_wfset("models_mars.rds", "ncorr_MARS")

autoplot(mars_ncorr_res)
```

### Best models

```{r, eval=FALSE}
mars_ncorr_res %>% 
  show_best(metric = "roc_auc")
```

## NCORR + YJ recipe

```{r, eval=FALSE}
mars_ncoryj_res <- read_wfset("models_mars.rds", "ncorr_yj_MARS")

autoplot(mars_ncoryj_res)
```

### Best models

```{r, eval=FALSE}
mars_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + ORQ recipe

```{r, eval=FALSE}
mars_ncorq_res <- read_wfset("models_mars.rds", "ncorr_orq_MARS")

autoplot(mars_ncorq_res)
```

### Best models

```{r, eval=FALSE}
mars_ncorq_res %>% show_best("roc_auc", n = 5)
```

## PCA

```{r, eval=FALSE}
mars_pca_res <- read_wfset("models_mars.rds", "pca_MARS")

autoplot(mars_pca_res)
```

```{r, eval=FALSE}
show_best(mars_pca_res, metric = "roc_auc")
```


# Bagged MARS 

## NCORR + YJ recipe

```{r, eval=FALSE}
mars_bag_ncoryj_res <- read_wfset("models_mars.rds", "ncorr_yj_MARS_BAG")

autoplot(mars_bag_ncoryj_res)
```

### Best models

```{r, eval=FALSE}
show_best(mars_bag_ncoryj_res)
```

## NCORR + ORQ recipe

```{r, eval=FALSE}
mars_bag_ncorq_res <- read_wfset("models_mars.rds", "ncorr_orq_MARS_BAG")

autoplot(mars_bag_ncorq_res)
```

```{r, eval=FALSE}
show_best(mars_bag_ncorq_res, "roc_auc")
```

## PCA

```{r, eval=FALSE}
mars_bag_pca_res <- read_wfset("models_mars.rds", "pca_MARS_BAG")

autoplot(mars_bag_pca_res)
```

```{r, eval=FALSE}
show_best(mars_bag_pca_res)
```


## Compare preprocessors

### ROCs overlapped

```{r, eval=FALSE}
mars_best_res <- list(
    mars_ncorr_res,
    mars_ncoryj_res,
    mars_ncorq_res,
    mars_pca_res,
    mars_bag_ncoryj_res,
    mars_bag_ncorq_res,
    mars_bag_pca_res
  )
  
mars_best_names <- c(
    "ncorr",
    "ncorr_yj",
    "ncorr_orq",
    "pca",
    "ncorr_yj_bag",
    "ncorr_orq_bag",
    "pca_bag"
  )

mars_rocs <-
  map2_dfr(mars_best_res, mars_best_names,
  function(x, y)
    make_roc(x, y))

mars_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MARS, ROC curves")
```

### PR curves overlapped

```{r, eval=FALSE}
mars_prs <-
  map2_dfr(mars_best_res,
  mars_best_names,
  function(x, y)
    make_pr(x, y))

mars_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MARS, PR curves")
```


## Comparison of the preprocessing steps

```{r, eval=FALSE}
mars_comp <-
  res_comp_table(
    res_list = mars_best_res,
    mod_names = mars_best_names
  )

mars_posterior <-
  perf_mod(
    mars_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(mars_posterior)
```

```{r, eval=FALSE}
mars_preproc_diff <- contrast_models(mars_posterior, seed = 100) 

summary(mars_preproc_diff)
```

# Linear support vector machines (linSVM)

```{r}
# models in this file
readRDS(paste0(models_path, "models_svm.rds")) %>% 
  filter(grepl("linSVM", wflow_id)) %>% 
  pull(wflow_id)
```


## NCORR recipe

```{r lsvm norm}
lsvm_ncorr_res <- read_wfset("models_svm.rds", "ncorr_linSVM")

autoplot(lsvm_ncorr_res)
```

### Best models

```{r}
lsvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ recipe

```{r}
lsvm_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_linSVM")

autoplot(lsvm_ncoryj_res)
```

### Best models

```{r}
lsvm_ncoryj_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + ORQ recipe

```{r}
lsvm_ncorq_res <-  read_wfset("models_svm.rds", "ncorr_orq_linSVM")

autoplot(lsvm_ncorq_res)
```

### Best models

```{r}
lsvm_ncorq_res %>% show_best("roc_auc")
```

## PCA

```{r}
lsvm_pca_res <-  read_wfset("models_svm.rds", "pca_linSVM")

autoplot(lsvm_pca_res)
```

### Best models

```{r}
show_best(lsvm_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROCs overlapped

```{r}
lsvm_best_res <- list(
    lsvm_ncorr_res,
    lsvm_ncoryj_res,
    lsvm_ncorq_res,
    lsvm_pca_res
  )
  lsvm_best_names <- c(
    "ncorr",
    "ncorr_yj",
    "ncorr_orq",
    "pca"
  )
```


```{r}
lsvm_rocs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_roc(x, y))

lsvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, ROC curves")
```

### PR-curves overlapped

```{r lsvm compare pr}
lsvm_prs <-
  map2_dfr(lsvm_best_res,
  lsvm_best_names,
  function(x, y)
    make_pr(x, y))

lsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("linSVM, PR curves")
```

### Distributions

```{r}
lsvm_comp <-
  res_comp_table(
    res_list = lsvm_best_res,
    mod_names = lsvm_best_names
  )

lsvm_posterior <-
  perf_mod(
    lsvm_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(lsvm_posterior)
```

### Difference estimates

```{r}
lsvm_preproc_diff <- contrast_models(lsvm_posterior, seed = 100) 

summary(lsvm_preproc_diff)
```

# Polynomial support vector machines (polySVM)

## NCORR recipe

```{r psvm norm}
psvm_ncorr_res <- read_wfset("models_svm.rds", "ncorr_polySVM")

autoplot(psvm_ncorr_res)
```

### Best models

```{r}
psvm_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```


## NCORR + YJ

```{r}
psvm_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_polySVM")

autoplot(psvm_ncoryj_res)
```

### Best models

```{r}
show_best(psvm_ncoryj_res, metric = "roc_auc")
```


## NCORR + ORQ recipe

```{r}
psvm_ncorq_res <- read_wfset("models_svm.rds", "ncorr_orq_polySVM")

autoplot(psvm_ncorq_res)
```

### Best models

```{r}
psvm_ncorq_res %>% 
  show_best("roc_auc")
```

## PCA 

```{r}
psvm_pca_res <- read_wfset("models_svm.rds", "pca_polySVM")

autoplot(psvm_pca_res)
```

### Best models

```{r}
psvm_pca_res %>% 
  show_best("roc_auc")
```

## Comparison of the preprocessing steps

### ROCs overlapped

```{r}
psvm_best_res <- list(
    psvm_ncorr_res,
    psvm_ncorq_res,
    psvm_ncoryj_res,
    psvm_pca_res
  )

psvm_best_names <- c(
    "ncorr",
    "ncorr_orq",
    "ncorr_yj",
    "pca"
  )
```


```{r}
psvm_rocs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_roc(x, y))

psvm_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, ROC curves")
```

### PR curves overlapped

```{r}
psvm_prs <-
  map2_dfr(psvm_best_res,
           psvm_best_names,
           function(x, y)
             make_pr(x, y))

psvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("polySVM, PR curves")
```


### Distributions

```{r}
psvm_comp <-
  res_comp_table(
    res_list = list(
      psvm_ncorr_res,
      psvm_ncorq_res,
      psvm_ncoryj_res,
      psvm_pca_res
    ),
    mod_names = c(
      "ncorr",
      "ncorr_orq",
      "ncorr_yj",
      "pca_yj"
    )
  )

psvm_posterior <-
  perf_mod(
    psvm_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(psvm_posterior)
```

### Estimate the difference

```{r}
psvm_preproc_diff <- contrast_models(psvm_posterior, seed = 100) 

summary(psvm_preproc_diff)
```

# Radial Basis Function SVM

## NCORR recipe

```{r, fig.width=8}
rbf_ncorr_res <- read_wfset("models_svm.rds", "ncorr_rbfSVM")

autoplot(rbf_ncorr_res)
```

### Best models

```{r}
show_best(rbf_ncorr_res, metric = "roc_auc")
```

## NCORR + YJ recipe

```{r, fig.width=8}
rbf_ncoryj_res <- read_wfset("models_svm.rds", "ncorr_yj_rbfSVM")

autoplot(rbf_ncoryj_res)
```

### Best models

```{r}
show_best(rbf_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ recipe

```{r, fig.width=8}
rbf_ncorq_res <- read_wfset("models_svm.rds", "ncorr_orq_rbfSVM")

autoplot(rbf_ncorq_res)
```

### Best models

```{r}
show_best(rbf_ncorq_res, metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
rbf_pca_res <- read_wfset("models_svm.rds", "pca_rbfSVM")

autoplot(rbf_pca_res)
```


```{r}
show_best(rbf_pca_res, metric = "roc_auc")
```


## Comparison of the preprocessors

### ROC curves overlapped

```{r}
rsvm_best_res <- list(
    rbf_ncorr_res,
    rbf_ncorq_res,
    rbf_ncoryj_res,
    rbf_pca_res
    
  )

rsvm_best_names <- c(
    "ncorr",
    "ncorr_orq",
    "ncorr_yj",
    "pca"
  )
```


```{r}
rbf_rocs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_roc(x, y))

rbf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, ROC curves")
```

### PR curves overlapped

```{r}
rsvm_prs <-
  map2_dfr(rsvm_best_res,
           rsvm_best_names,
           function(x, y)
             make_pr(x, y))

rsvm_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("rbfSVM, PR curves")
```

### Distributions

```{r}
rbf_comp <-
  res_comp_table(
    res_list = rsvm_best_res,
    mod_names = rsvm_best_names
  )

rbf_posterior <-
  perf_mod(
    rbf_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rbf_posterior)
```

### Difference estimates

```{r}
rbf_preproc_diff <- contrast_models(rbf_posterior, seed = 100) 

summary(rbf_preproc_diff)
```

# k-Nearest Neighbors (KNN)

## NCORR recipe

```{r knn norm, fig.width=8, eval=FALSE}
knn_ncorr_res <- read_wfset("models_knn.rds", "ncorr_kNN")

autoplot(knn_ncorr_res)
```

### Best models

```{r, eval=FALSE}
knn_ncorr_res %>% 
  show_best("roc_auc", n = 5)
```

## NCORR + YJ

```{r, fig.width=8, eval=FALSE}
knn_ncoryj_res <- read_wfset("models_knn.rds", "ncorr_yj_kNN")

autoplot(knn_ncoryj_res)
```

### Best models

```{r, eval=FALSE}
show_best(knn_ncoryj_res)
```

## NCORR + ORQ recipe

```{r, fig.width=8, eval=FALSE}
knn_ncorq_res <- read_wfset("models_knn.rds", "ncorr_orq_kNN")

autoplot(knn_ncorq_res)
```

### Best models

```{r, eval=FALSE}
knn_ncorq_res %>% 
  show_best("roc_auc")
```


## PCA

```{r, fig.width=8, eval=FALSE}
knn_pca_res <- read_wfset("models_knn.rds", "pca_kNN")

autoplot(knn_pca_res)
```

### Best models

```{r, eval=FALSE}
knn_pca_res %>% 
  show_best("roc_auc")
```

## Comparison of the preprocessors

### ROCs overlapped

```{r, eval=FALSE}
knn_best_res <- list(knn_ncorr_res,
                     knn_ncorq_res,
                     knn_ncoryj_res,
                     knn_pca_res)
knn_best_names <- c("ncorr",
                    "ncorr_orq",
                    "ncorr_yj",
                    "pca")
```


```{r, eval=FALSE}
knn_rocs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_roc(x, y))

knn_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, ROC curves")
```

### PR curves overlapped

```{r, eval=FALSE}
knn_prs <-
  map2_dfr(knn_best_res,
           knn_best_names,
           function(x, y)
             make_pr(x, y))

knn_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("kNN, PR curves")
```

### Posterior distributions

```{r, eval=FALSE}
knn_comp <-
  res_comp_table(
    res_list = knn_best_res,
    knn_best_names
  )

knn_posterior <-
  perf_mod(
    knn_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(knn_posterior)
```

### Difference estimates

```{r, eval=FALSE}
knn_preproc_diff <- contrast_models(knn_posterior, seed = 100) 

summary(knn_preproc_diff)
```

# Random Forest (RF)

## Base recipe

```{r rf base}
rf_base_res <- read_wfset("models_rf.rds", "base_RF")

autoplot(rf_base_res)
```

### Best models

```{r}
rf_base_res %>% 
  show_best("roc_auc", n = 5)
```

## Base + Boruta feature selection

```{r}
rf_base_bor_res <- read_wfset("models_rf.rds", "base_boruta_RF")

autoplot(rf_base_bor_res)
```

```{r}
show_best(rf_base_bor_res)
```

## Comparison of preprocessors

### ROCs overlapped

```{r}
rf_best_res <- list(rf_base_res, rf_base_bor_res)
  
rf_best_names <- c("base", "boruta")
```


```{r}
rf_rocs <-
  map2_dfr(rf_best_res, 
           rf_best_names,
           function(x, y)
             make_roc(x, y))

rf_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, ROC curves")
```

### PR curves overlapped

```{r}
rf_prs <-
  map2_dfr(rf_best_res,
           rf_best_names,
           function(x, y)
             make_pr(x, y))

rf_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("Random Forest, PR curves")
```

### Distributions

```{r}
rf_comp <-
  res_comp_table(
    res_list = rf_best_res,
    mod_names = rf_best_names
  )

rf_posterior <-
  perf_mod(
    rf_comp,
    iter = 10000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(rf_posterior)
```

### Difference estimates

```{r}
rf_preproc_diff <- contrast_models(rf_posterior, seed = 100) 

summary(rf_preproc_diff)
```

# Gradient Boosted Trees (GBT)

BT are sensitive to hyper-parameters that's why, Bayesian grid search
should be preferable.

## Base recipe

space search followed by the Bayesian optimization

```{r, fig.width=10, fig.height=6}
bt_base_res <- read_wfset("models_bt.rds", "base_BT")

autoplot(bt_base_res)
```

### Best models

```{r}
bt_base_res %>% 
  show_best(metric = "roc_auc")
```


## Base recipe + Bayesian optimization

Bayesian search

```{r, fig.width=10, fig.height=6}
bt_base_bres <- readRDS(paste0(models_path, "models_bt_bres.rds"))

bt_base_bres %>% 
  autoplot()
```

### Best models

```{r}
bt_base_bres %>% 
  show_best(metric = "roc_auc") 
```

>Shrinkage (learning rate) is a gradient boosting regularization procedure that helps modify the update rule, which is aided by a parameter known as the learning rate. The use of learning rates below 0.1 produces improvements that are significant in the generalization of a model.

## Base recipe + Boruta feature selection

```{r, fig.width=10, fig.height=6}
bt_base_bor_res <- read_wfset("models_bt.rds", "base_boruta_BT")

autoplot(bt_base_bor_res)
```

### Best models

```{r}
show_best(bt_base_bor_res)
```

## Base recipe + Boruta + Bayesian search

```{r, fig.width=10, eval=FALSE}
bt_base_bor_bres <- readRDS(paste0(models_path, "base_bt_boruta_bres_light.rds"))

autoplot(bt_base_bor_bres)
```

```{r, eval=FALSE}
show_best(bt_base_bor_bres)
```

## Comparison of the preprocessors

### ROCs

```{r}
bt_best_res <- list(
    bt_base_res,
    bt_base_bres,
    bt_base_bor_res
    #bt_base_bor_bres
  )

bt_best_names <- c(
    "base",
    "base_bayes",
    "base_bor"
    #"base_bor_bayes"
  )

bt_rocs <-
  map2_dfr(bt_best_res,
  bt_best_names,
  function(x, y)
    make_roc(x, y))

bt_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, ROC curves")
```

### PR curves overlapped

```{r}
bt_prs <-
  map2_dfr(bt_best_res,
           bt_best_names,
           function(x, y)
             make_pr(x, y))

bt_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("GBT, PR curves")
```

### Posterior distributions

```{r}
bt_comp <-
  res_comp_table(
    res_list = bt_best_res,
    mod_names = bt_best_names
  )

bt_posterior <-
  perf_mod(
    bt_comp,
    iter = 4000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 4
  )

autoplot(bt_posterior)
```

### Estimating the differences

```{r}
bt_preproc_diff <- contrast_models(bt_posterior, seed = 100) 

summary(bt_preproc_diff) 
```

# Multilayer perceptron (MLP)

aka feed-forward neural network

## NCORR recipe


```{r mlp ncorr, fig.width=8}
mlp_ncor_res <- read_wfset("models_mlp.rds", "ncorr_MLP")

autoplot(mlp_ncor_res)
```

### Best models

```{r}
mlp_ncor_res %>% 
  show_best("roc_auc")
```

## NCORR + YJ recipe

```{r, fig.width=8}
mlp_ncoryj_res <- read_wfset("models_mlp.rds", "ncorr_yj_MLP")

mlp_ncoryj_res %>% autoplot()
```

### Best models

```{r}
mlp_ncoryj_res %>% 
  show_best("roc_auc")
```

## NCORR + ORQ

```{r, fig.width=8}
mlp_ncorq_res <- read_wfset("models_mlp.rds", "ncorr_orq_MLP")

mlp_ncorq_res %>% autoplot()
```

### Best models

```{r}
mlp_ncorq_res %>% show_best(metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
mlp_pca_res <- read_wfset("models_mlp.rds", "pca_MLP")

mlp_pca_res %>% autoplot()
```

### Best models

```{r}
mlp_pca_res %>% show_best("roc_auc")
```


# Bag MLP

## NCORR

```{r, fig.width=8}
mlp_bag_ncorr_res <- read_wfset("models_mlp.rds", "ncorr_MLP_BAG")

autoplot(mlp_bag_ncorr_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorr_res)
```

## NCORR + YJ

```{r, fig.width=8}
mlp_bag_ncoryj_res <- read_wfset("models_mlp.rds", "ncorr_yj_MLP_BAG")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_ncoryj_res, metric = "roc_auc")
```

## NCORR + ORQ

```{r, fig.width=8}
mlp_bag_ncorq_res <- read_wfset("models_mlp.rds", "ncorr_orq_MLP_BAG")

autoplot(mlp_bag_ncorq_res)
```

### Best models

```{r}
show_best(mlp_bag_ncorq_res, metric = "roc_auc")
```

## PCA

```{r, fig.width=8}
mlp_bag_pca_res <- read_wfset("models_mlp.rds", "pca_MLP_BAG")

autoplot(mlp_bag_ncoryj_res)
```

### Best models

```{r}
show_best(mlp_bag_pca_res, metric = "roc_auc")
```

## Comparison of the preprocessors

### ROCs overlapped

```{r}
mlp_best_res <- list(
    mlp_ncor_res,
    mlp_ncoryj_res,
    mlp_ncorq_res,
    mlp_pca_res,
    mlp_bag_ncorr_res,
    mlp_bag_ncoryj_res,
    mlp_bag_ncorq_res,
    mlp_pca_res
  )

mlp_best_names <- c(
    "ncorr",
    "ncorr_yj",
    "ncorr_orq",
    "pca",
    "bag_ncorr",
    "bag_ncorr_yj",
    "bag_ncorr_orq",
    "bag_pca"
  )

mlp_rocs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_roc(x, y))

mlp_rocs %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, ROC curves")
```

### PR curves overlapped

```{r}
mlp_prs <-
  map2_dfr(mlp_best_res,
           mlp_best_names,
           function(x, y)
             make_pr(x, y))

mlp_prs %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0, 1)) + 
  scale_color_brewer(palette="Set1") +
  ggtitle("MLP, PR curves")
```

### Posterior distributions

```{r}
mlp_comp <-
  res_comp_table(
    res_list = mlp_best_res,
    mod_names = mlp_best_names
  )

mlp_posterior <-
  perf_mod(
    mlp_comp,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 6
  )

autoplot(mlp_posterior)
```

### Difference estimates

```{r}
mlp_preproc_diff <- contrast_models(mlp_posterior, seed = 100) 

summary(mlp_preproc_diff)
```


# Comparison of models

One best from each group

LR YJ
MLP BAG
GBT Bayes

## ROCs

```{r comparison1}
# you will need these two objects later
best_resamples_list <- list(
    lr_base_yj_res,
    mlp_bag_ncoryj_res,
    bt_base_bres
  )
best_resamples_names <- c(
    "LLR",
    "MLP BAG",
    "GBT"
  )

# calculate rocs
roc_df <-
  map2_dfr(best_resamples_list,
  best_resamples_names,
  function(x, y)
    make_roc(x, y))

Fig_9A <- roc_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette="Dark2") 

ggsave(file="images/manuscript/Fig_9A_ROCs.png", plot=Fig_9A, width = 8, height=4, units = "in")

Fig_9A
```

## Posterior probability distribution

Collect AUCs of the best model (with the best mean AUC) from each
resamples object

```{r}
set.seed(125)

mod_comparison <- res_comp_table(best_resamples_list,
                                 best_resamples_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

Fig_9B <- autoplot(mod_posterior) +
  scale_color_brewer(palette = "Dark2") +
  theme(plot.margin = margin(
    t = 23,
    r = 15,
    b = 23,
    l = 15,
    unit = "pt"
  ))

ggsave(file="images/manuscript/Fig_9B_posterior.png", plot=Fig_9B, width = 8, height=4, units = "in")

Fig_9B
```

```{r, fig.width=8}
Fig_9AB <- ggarrange(Fig_9A, Fig_9B,
                    labels = c("A", "B"),
                    ncol = 2, nrow = 1, common.legend = TRUE,
                    legend = "bottom")

ggsave(file="images/manuscript/Fig_9AB.png", plot=Fig_9AB, width = 8, height=4, units = "in")

Fig_9AB
```

## Difference estimates

```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

Save this table

```{r}
write_csv(summary(mod_diff), "manuscript/tables/contrast_models.csv" )
```


### Insignificant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper > 0)
```

### Significant differences in mean AUCs:

```{r}
summary(mod_diff) %>% filter(lower < 0, upper < 0)
```

```{r}
summary(mod_diff) %>% filter(lower > 0, upper > 0)
```


## Top models

LR is the simplest, MLP and GBT are significantly better in mean ROC AUC.

### ROC curves

```{r comparison2}
# calculate rocs
top_models <- list(
    lr_base_yj_res,
    bt_base_bres,
    mlp_bag_ncoryj_res)

top_names <- c("LR", "GBT", "MLP")

roc3_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_roc(x, y))
roc3_df %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("ROC curves, top models")

```

### PR curves

```{r}
pr_top_df <-
  map2_dfr(top_models,
           top_names,
           function(x, y)
             make_pr(x, y))
pr_top_df %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal(ylim=c(0,1)) + 
  scale_color_brewer(palette = "Set1") +
  ggtitle("PR curves, top models")
```

### Difference estimates

```{r}
mod_comparison <- res_comp_table(top_models,
                                 top_names)

mod_posterior <-
  perf_mod(
    mod_comparison,
    iter = 2000,
    seed = 100,
    refresh = 0,
    chains = 5,
    cores = 8
  )

autoplot(mod_posterior) + 
  scale_color_brewer(palette = "Set1")
```


```{r}
mod_diff <- contrast_models(mod_posterior, seed = 100) 

summary(mod_diff)
```

# Final fit

> emulates the process where, after determining the best model, the
> final fit on the entire training set is needed and is then evaluated
> on the test set.

## Functions

```{r ff func}
# function to run last fit process
run_last_fit <-
  function(model_name,
           blank_spec,
           tuned_spec,
           recipe,
           train_test_split = data_split,
           return_fit = FALSE) {
    # last workflow
    last_wf <- workflow() %>%
      add_model(blank_spec) %>%
      add_recipe(recipe) %>%
      update_model(tuned_spec)
    
    set.seed(345)
    # final fit
    final_fit <- final_metrics <-
      last_wf %>%
      last_fit(train_test_split)
    # final metrics
    final_metrics <-
      final_fit %>%
      collect_metrics() %>%
      select(.metric, .estimate) %>%
      mutate(model = model_name)
    if (return_fit) {
      return(list(final_metrics, final_fit))
    } else {
      return(list(final_metrics, NULL))
    }
  }

# required by models with ncorr recipes
update_corr_treshold <- function(base_recipe, step_no, corr_threshold) {
  # update step_corr threshold
  base_recipe$steps[[step_no]] <-
   recipes::update(base_recipe$steps[[step_no]], threshold = corr_threshold)
  
  return(base_recipe)
}

# best models
best_models <- map(top_models, ~select_best(., "roc_auc"))
```

## LR final

```{r lr ff}
set.seed(800)

# LR specs
lr_spec_blank <- logistic_reg(
  penalty = tune(),
  mixture = 1,
  engine = "glmnet",
  mode = "classification"
)

lr_spec_tuned <-
  logistic_reg(
    penalty = best_models[[1]]$penalty,
    mixture = 1,
    engine = "glmnet",
    mode = "classification"
  )

# LR last fit and metrics
lr_final <-
  run_last_fit("LR",
               lr_spec_blank,
               lr_spec_tuned,
               recipe = base_yj_recipe,
               return_fit = TRUE)

lr_final[[1]]
```



## GBT final

```{r gbt ff}
set.seed(300)

library(bonsai)

# GBT base specs /number 2/
gbt_spec_blank <- boost_tree(
      trees = tune(),
      #mtry = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune()) %>%
      set_engine("lightgbm", num.threads = 8) %>%
      set_mode("classification") %>% translate()

gbt_spec_tuned <- boost_tree(
  trees = best_models[[2]]$trees,
  #mtry = best_models[[2]]$mtry,
  min_n = best_models[[2]]$min_n,
  tree_depth = best_models[[2]]$tree_depth,
  learn_rate = best_models[[2]]$learn_rate,
  loss_reduction = best_models[[2]]$loss_reduction
) %>%
  set_engine("lightgbm", num.threads = 8) %>%
  set_mode("classification")

# GBT base last fit and metrics
gbt_base_final <-
  run_last_fit(
    "GBT",
    gbt_spec_blank,
    gbt_spec_tuned,
    recipe = base_recipe,
    return_fit = TRUE
  )

gbt_base_final[[1]]
```

## MLP final

```{r}
set.seed(100)

mlp_bag_spec <- bag_mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
      set_engine("nnet", num.threads = 8) %>%
      set_mode("classification") %>%
      translate()


mlp_bag_tuned <- bag_mlp(
  hidden_units = best_models[[3]]$hidden_units,
  penalty = best_models[[3]]$penalty,
  epochs = best_models[[3]]$epochs
) %>% 
  set_mode("classification") %>%
  set_engine("nnet", num.threads = 4)

corr_threshold_tuned <- best_models[[3]]$corr_tune

ncorr_yj_recipe_tuned <-
  update_corr_treshold(base_recipe = ncorr_yj_recipe,
                       step_no = 8,
                       corr_threshold = corr_threshold_tuned)

mlp_bag_ncoryj_final <-
  run_last_fit(
    "MLP",
    mlp_bag_spec,
    mlp_bag_tuned,
    recipe = ncorr_yj_recipe_tuned,
    return_fit = TRUE
  )

mlp_bag_ncoryj_final[[1]]
```


## ROCs

```{r ff roc, fig.width=9}
mod_labels <- c("LR", "GBT", "MLP")
mod_finals <- list(lr_final[[2]], gbt_base_final[[2]], mlp_bag_ncoryj_final[[2]])

make_final_roc <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

rocs_final <- map2(
  mod_finals,
  mod_labels,
  ~ make_final_roc(.x, .y)
)

ggarrange(plotlist = rocs_final, nrow=1, ncol = 3)
```

## PRs

```{r pr, fig.width=9}
make_final_pr <- function(fit_obj, title=""){
  fit_obj %>%   
  collect_predictions() %>% 
  pr_curve(resistance, .pred_HR) %>% 
  autoplot() + ggtitle(title)
}

prs_final <-  map2(
  mod_finals,
  mod_labels,
  ~ make_final_pr(.x, .y)
)

ggarrange(plotlist = prs_final,
          ncol = 3,
          nrow = 1)
```

## Confusion Matrix

Probability cut-off = 0.5

```{r ff cm, fig.width=9}
make_cm <- function(fit_obj, title="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + ggtitle(title)
}

cm_final <-
  map2(
    mod_finals,
    mod_labels,
    ~ make_cm(.x, .y)
  )

ggarrange(plotlist = cm_final, ncol = 3, nrow = 1)
```



## Performance

```{r ff metrics}
get_metrics <- function(fit_obj, label="") {
  fit_obj %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class) %>% 
  summary() %>% 
  select(-.estimator) %>% 
  mutate(model = label)
}

map2_dfr(
  mod_finals,
  mod_labels,
  ~ get_metrics(.x, .y)) %>%
  pivot_wider(names_from = model, values_from = .estimate)
```

## Probability cutoff adjustment by maximum j-index

```{r ff prob adj func}
maxj <- function(threshold_df) {
  # find max j-index
  max_j_index_threshold <- threshold_df %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
  
  # max_j_index_threshold may be a vector, use its last element
  if (length(max_j_index_threshold) > 1) {
    max_j_index_threshold <-
      max_j_index_threshold[length(max_j_index_threshold)]
  }
  
  return(max_j_index_threshold)
}

get_threshold <- function(fit_obj){
    # collect sens, spec, j-index at various cut-offs
  threshold_data <-
    fit_obj %>%
    collect_predictions() %>%
    threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.01)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" |
                               .metric == "spec" ~ "1",
                             TRUE ~ "2"))
  return(threshold_data)
}

senspec_plot <- function(fit_obj, title="") {
  # collect sens, spec, j-index at various cut-offs
  threshold_data <- get_threshold(fit_obj)
  
  # find max j-index
  max_j_index_threshold <- maxj(threshold_data)
  
  # plot metrics v cut-offs
  sens_spec_j_plot <-
    ggplot(threshold_data,
           aes(
             x = .threshold,
             y = .estimate,
             color = .metric,
             alpha = group
           )) +
    geom_line(size = 1) +
    #theme_minimal() +
    #scale_color_viridis_d(end = 0.9) +
    scale_color_brewer(palette = "Set1", guide = guide_legend(title = NULL) ) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(
      xintercept = max_j_index_threshold,
      alpha = .8,
      color = "grey30",
      linetype = "longdash"
    ) +
    labs(x = "Probability",
         y = "Metric Estimate",
         title = title)
  return(sens_spec_j_plot)
}
```


```{r, fig.height=9}
senspec_plots <- map2(
  mod_finals,
  mod_labels,
  ~ senspec_plot(.x, .y)
)

ggarrange(plotlist = senspec_plots,
                  nrow = 3,
                  ncol = 1,
                  common.legend = T)
```


### Fig_9C

```{r}
Fig_9C <- senspec_plot(lr_final[[2]], "")

ggsave(file="images/manuscript/Fig_9C_prob_cutoff.png", plot=Fig_9C, width = 8, height=4, units = "in")

Fig_9C
```


### j-index and the probability threshold

```{r}
lr_final[[2]] %>%
  get_threshold() %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  slice_tail()
```

### Optimized confusion matrix

Using suggested j-index value, I can optimize probability cut-off in
order to have 100% sensitivity of the model

(it will catch all HR cases, but there will be more false positives)


```{r opt cm, fig.width=9}
optimize_prediction <- function(fit.obj){
  prob_cutoff <- maxj(get_threshold(fit.obj))
  
  optimized <- fit.obj %>%
    collect_predictions() %>%
    mutate(.pred = make_two_class_pred(
      estimate = .pred_HR,
      levels = levels(resistance),
      threshold = prob_cutoff
    )) %>%
    select(resistance, contains(".pred"))
  
  return(optimized)
}

cm_optimized_plots <- map2(
  mod_finals,
  mod_labels,
  ~ optimize_prediction(.x) %>% conf_mat(truth = resistance, estimate = .pred) %>% 
  autoplot(type = "heatmap") + ggtitle(.y)
)

ggarrange(plotlist = cm_optimized_plots,
          nrow = 1,
          ncol = 3,
          common.legend = F)
```

### Optimized performance

```{r opt perf}
get_metrics2 <- function(fit.obj, label){
  optimize_prediction(fit.obj) %>% 
    conf_mat(truth = resistance, estimate = .pred) %>% 
    summary() %>% 
    mutate(model = label)
}

map2_dfr(
  mod_finals,
  mod_labels,
  ~ get_metrics2(.x, .y)) %>%
  pivot_wider(names_from = model, values_from = .estimate) %>% 
  select(-.estimator)
```

# Post-modelling analysis

## Features used by the best model (LR)

### How many predictors were removed

Overall, we have `r ncol(df_train) - 2` predictors.

#### Predictors left after the first `step_nzv()`

```{r n nzv}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  juice() %>% 
  ncol()
```


#### Predictors left after the first and the second `step_nzv()`

```{r n corr}
# NB: strain and resistance are still there
recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>% prep() %>% juice() %>% ncol()
```

### Features importance

Non-zero LLR coefficients:

```{r lr coeff}
lr_coeff_df <- lr_final[[2]] %>%
  extract_fit_parsnip() %>% tidy() %>% 
  filter(estimate != 0) %>% 
  arrange(-abs(estimate)) %>% 
  mutate(odds.ratio = 1/exp(estimate)) %>% 
  select(-penalty)

lr_coeff_df
```

Just 44 features plus Intercept are used by the model.

On the `odds.ratio` column: 

- units in which predictors are measured have been transformed

- apparently, nonHR is outcome 1 here and HR is outcome 0, because increase in `n.beta.lac` decreases chances of outcome 1.


### Predictors decreasing the risk of HR:

```{r feat pos}
lr_coeff_df %>% 
  filter(odds.ratio < 1.0) %>% 
  select(term)
```

### Predictors increasing the risk of HR

```{r feat neg}
lr_coeff_df %>% 
  filter(odds.ratio >= 1.0) %>% 
  select(term)
```

### Plot

```{r, fig.width=8, fig.height=6}
Fig_9D <- lr_coeff_df %>%
  mutate(effect = if_else(estimate < 0, "increase HR prob.", "decrease HR prob.")) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(reorder(term, odds.ratio, decreasing = F), odds.ratio)) +
  geom_col(aes(fill = effect)) +
  scale_fill_brewer(palette = "Paired", guide = guide_legend(title = NULL)) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.9,
      hjust = 0.9,
      size = 8
    ),
    legend.position = "bottom"
  ) +
  xlab("") +
  coord_flip()

ggsave(file="images/manuscript/Fig_9D_predictors.png", plot=Fig_9D, width = 8, height=10, units = "in")

Fig_9D 
```

Alternative: plotting estimate

```{r}
 lr_coeff_df %>%
  mutate(effect = if_else(estimate < 0, "increase HR prob.", "decrease HR prob.")) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(reorder(term, odds.ratio, decreasing = F), estimate)) +
  geom_col(aes(fill = effect)) +
  scale_fill_brewer(palette = "Paired", guide = guide_legend(title = NULL)) +
  theme(
    axis.text.x = element_text(
      angle = 90,
      vjust = 0.9,
      hjust = 0.9,
      size = 8
    ),
    legend.position = "bottom"
  ) +
  xlab("") +
  coord_flip()
```


```{r,fig.width=7, fig.height=10}
Fig_9CD <- ggarrange(Fig_9C, Fig_9D,
                    labels = c("C", "D"),
                    ncol = 1, nrow = 2)

ggsave(file="images/manuscript/Fig_9CD.png", plot=Fig_9CD, width = 8, height=11, units = "in")

Fig_9CD
```

## Analysis of misclassified strains

Extract predictions using the final model fit and adjusted probability cut-off

```{r}
optimize_prediction(lr_final[[2]])
```

My probability threshold is 0.25, these strains are predicted as HR

```{r}
prob_cutoff <- maxj(get_threshold(lr_final[[2]]))
optimize_prediction(lr_final[[2]]) %>% filter(.pred_HR >= prob_cutoff)
```

### False positives v True positives

```{r}
false_positives <- optimize_prediction(lr_final[[2]]) %>%
  mutate(strain = df_test$strain) %>%
  filter(.pred == "HR", resistance == "nonHR") %>%
  select(strain, resistance, .pred, .pred_HR) %>%
  mutate(type = "FP")

true_positives <- optimize_prediction(lr_final[[2]]) %>%
  mutate(strain = df_test$strain) %>%
  filter(.pred == "HR", resistance == "HR") %>%
  select(strain, resistance, .pred, .pred_HR) %>%
  mutate(type = "TP")

df_test_proc <-
  base_yj_recipe %>% 
  prep() %>% 
  bake(new_data = df_test) %>% 
  mutate(strain = df_test$strain)

fp_tp <- bind_rows(false_positives, true_positives) %>% 
  left_join(df_test_proc, by="strain")
```

#### Positive predictors

```{r, fig.width=8, fig.height=10}
# features increasing HR prob
predictors_positive <- lr_coeff_df %>% 
  filter(odds.ratio >= 1.0) %>% 
  select(term) %>% 
  unlist()

get_plot <- function(var_name){
  ggplot(fp_tp, aes(type, .data[[var_name]])) +
    geom_boxplot() +
    geom_violin(alpha=0.1) +
    xlab("")
}

plot_list <- map(predictors_positive, ~get_plot(.))

ggarrange(plotlist = plot_list, ncol=3, nrow=7)
```

With raw probabilities

```{r, fig.width=8, fig.height=10}
get_scatterplot <- function(var_name){
  ggplot(fp_tp, aes(.pred_HR, .data[[var_name]])) +
   geom_point(aes(color=type))
}

plot_list <- map(predictors_positive, ~get_scatterplot(.))

ggarrange(plotlist = plot_list, ncol=3, nrow=7, common.legend = T)
```

#### Negative predictors

```{r, fig.width=8, fig.height=10}
predictors_negative <- lr_coeff_df %>%
  filter(odds.ratio < 1.0) %>%
  select(term) %>% 
  filter(term != "(Intercept)") %>% 
  unlist()

plot_list <- map(predictors_negative, ~get_plot(.))

ggarrange(plotlist = plot_list, ncol=4, nrow=5)
```


With raw probabilities

```{r, fig.width=8, fig.height=10}
plot_list <- map(predictors_negative, ~get_scatterplot(.))

ggarrange(plotlist = plot_list, ncol=4, nrow=5, common.legend = T)
```


## Features used by the best model (MLP)

### Fit the model with selected (h)-params

```{r, eval=FALSE}
# IT'S THE WAY TO GET MLP FIT OBJECT W/O MY BULKY FUNCTIONS for final fit
mlp_best_param <- select_best(mlp_bag_ncoryj_res, "roc_auc")
mlp_model_final <- finalize_model(mlp_bag_spec, mlp_best_param)

# I need processed data
df_train_proc <- ncorr_yj_recipe_tuned %>% 
  prep() %>% 
  juice() %>% 
  select(-strain)

# fit the tuned model to the data
mlp_fit <- fit(mlp_model_final, resistance ~ ., df_train_proc)

mlp_fit 

# the same as three functions above
# fit_best(mlp_bag_ncoryj_res)
```

### Plot predictors importance

Top 20

```{r, fig.width=8}
mlp_fit$fit$imp %>% 
  slice_head(n = 20) %>% 
  ggplot(aes(value, reorder(term, value))) +
  geom_point() +
  geom_errorbarh(aes(xmax = value + std.error, xmin = value - std.error), size = 0.2) +
  ylab("")
```


### Build an explainer

You can choose for instance false positives and look what influenced
their classification as HR

#### Select FP/FN

```{r}
mlp_bag_ncoryj_final[[2]] %>% collect_predictions() %>% 
  mutate(FP = if_else(.pred_class == "HR" & resistance == "nonHR", "FP", NA),
         FN = if_else(.pred_class == "nonHR" & resistance == "HR", "FN", NA),
         TP = if_else(.pred_class == "HR" & resistance == "HR", "TP", NA),
         TN = if_else(.pred_class == "nonHR" & resistance == "nonHR", "TN", NA),
         strain = df_test$strain) %>%  
  select(-c(id, .row, .config))
```

#### Explain those of interest

>Model break-down explanations... depend on the order of the features

```{r, eval=FALSE}
library(DALEXtra)

df_test_proc <- ncorr_yj_recipe_tuned %>% prep() %>% bake(df_test) %>% 
  mutate(strain = df_test$strain) %>% 
  select(-resistance)

explainer_final <- 
  explain_tidymodels(
    mlp_fit, 
    data = df_test_proc, 
    y = df_test$resistance,
    label = "MLP",
    verbose = FALSE
  )

# a function to make predictor's contribution table
show_contribution <- function(strain_name, explainer = explainer_final, data = df_test_proc){
  observation <- filter(data, strain == strain_name)
  contribution <- tibble(predict_parts(explainer, observation))
  return(contribution)
}

fp_contrib <- show_contribution("DA63072")
tp_contrib <- show_contribution("DA63150")

fp_contrib %>% arrange(-contribution)
```

```{r}
fp_contrib %>% 
  filter(variable_name != "") %>% 
  slice_max(order_by = abs(contribution), n = 20) %>% 
  ggplot(aes(reorder(variable_name, -contribution), contribution)) +
  geom_col(aes(fill=sign)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6, size=9, hjust = 0.5)) +
  xlab("") +
  ggtitle("DA63072")
```

```{r}
tp_contrib %>% 
  filter(variable_name != "") %>% 
  slice_max(order_by = abs(contribution), n = 20) %>% 
  ggplot(aes(reorder(variable_name, -contribution), contribution)) +
  geom_col(aes(fill=sign)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.7, size=9, hjust = 0.5)) +
  xlab("") +
  ggtitle("DA63150")
```

### SHAP

TP and FP

```{r, fig.width=10, fig.height=5}
library(forcats)
library(patchwork)

set.seed(1801)
shap_tp <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63150"), 
    type = "shap",
    B = 20
  )

# Shapley explanations for TP example
plot_shap_tp20 <- shap_tp %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63150, TP")

# Shapley explanations for FP example
shap_fp <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63004"), 
    type = "shap",
    B = 20
  )

plot_shap_fp20 <- shap_fp %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63004, FP")

# Compare
(plot_shap_tp20 | plot_shap_fp20)
```

TN and FN

```{r, fig.width=10, fig.height=5}
shap_tn <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA62896"), 
    type = "shap",
    B = 20
  )

shap_fn <- 
  predict_parts(
    explainer = explainer_final, 
    new_observation = filter(df_test_proc, strain == "DA63072"), 
    type = "shap",
    B = 20
  )

plot_shap_tn20 <- shap_tn %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA62896, TN")

plot_shap_fn20 <- shap_fn %>%
  group_by(variable_name) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable_name = fct_reorder(variable_name, abs(mean_val))) %>%
  slice_head(n = 20) %>% 
  ggplot(aes(contribution, variable_name, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable_name, mean_val), 
           aes(mean_val, variable_name), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL) +
  ggtitle("DA63072, FN")

(plot_shap_tn20 | plot_shap_fn20)
```


# A model with minimal set of predictors chosen by Boruta

Four predictors were chosen by Boruta algorithms as being significantly more important than the rest

Here we use the same pre-processing as before: yj_recipe with tuned corr. threshold from the best `xgb_yjbor_bres`

```{r min feat set}
set.seed(42)

df_train_bor <-
  base_recipe %>% prep() %>% juice() %>% select(-strain)

bor_pred_selection <- Boruta::Boruta(resistance ~., data = df_train_bor, maxRuns=300)

bor_pred_selection
```

```{r, fig.width=10}
plot(bor_pred_selection)
```

```{r min feat boxplot}
minimal_pred <- bor_pred_selection$ImpHistory %>%
  as_tibble() %>%
  pivot_longer(cols = 1:92,
               names_to = "pred",
               values_to = "value") %>%
  group_by(pred) %>%
  mutate(median = median(value)) 

minimal_pred %>% 
  filter(median > 9) %>%
  ggplot(aes(reorder(pred, median), value)) +
  geom_boxplot(notch = T) +
  geom_violin(alpha = 0.1) +
  theme(axis.text.x = element_text(
    angle = 45,
    vjust = 0.55,
    hjust = 0.5
  )) +
  xlab("") +
  ylab("Z-score")
```

Top 4 predictors: 

```{r}
minimal_pred_best <- minimal_pred %>%
  filter(
    pred == "n.beta.lac" |
    pred == "n.rep.plasmid.TEM" |
      pred == "TEM.beta.lactamase" |
      pred == "n.TEM.1" 
  ) %>%
  select(-median)

kruskal.test(minimal_pred_best$value, minimal_pred_best$pred)
```

## BT model with the top 4 predictors

```{r}
minimal_pred_best %>% distinct(pred, .keep_all = F) 
```

```{r min recipe}
top_predictors <- 
  minimal_pred_best %>% 
  #filter(median > 10) %>% 
  distinct(pred, .keep_all = F) %>% 
  pull(pred)

data_strain_min <- data_strain %>% 
  select(resistance, all_of(top_predictors))

set.seed(124)

data_split_min <- initial_split(data_strain_min, prop = 0.8, strata = resistance)

df_train_min <- training(data_split_min)
df_test_min <- testing(data_split_min)

cv_folds_min <- vfold_cv(df_train_min, strata = "resistance", v = 10, repeats = 10)

base_min_pred_recipe <- recipe(resistance ~ ., data = df_train_min) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

lgb_spec <- boost_tree(
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = tune(),
  mode = "classification"
) %>% 
  set_engine(engine = "lightgbm", num.threads = 8) %>% 
  translate()

min_pred_wf <- workflow() %>% 
  add_model(lgb_spec) %>% 
  add_recipe(base_min_pred_recipe)

param_set <- extract_parameter_set_dials(min_pred_wf) %>%
  finalize(x = df_train_min %>% select(-resistance))
```


If you want to train this model again run this chunk:

```{r rf min3 train, eval=FALSE}
lgb_min_res <- min_pred_wf %>%
  tune_grid(
    param_info = param_set,
    grid = 20,
    resamples = cv_folds_min,
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE),
    metrics = cls_metrics
  )

saveRDS(lgb_min_res, paste0(models_path, "lgb_min_res.rds"))
```

By default this code is used:

```{r rf min3 res, fig.width=8}
lgb_min_res <- readRDS(paste0(models_path, "lgb_min_res.rds"))

autoplot(lgb_min_res)
```


```{r}
show_best(lgb_min_res, metric= "roc_auc")
```

## ROCs

```{r gbt min roc}
autoplot(make_roc(lgb_min_res, "")) +
  ggtitle("GBT, min set of predictors")
```

## Test set

### Training

```{r bt min test}
lgb_best_top <- select_best(lgb_min_res, metric = "roc_auc")

lgb_min_tuned <- boost_tree(
  mtry = lgb_best_top$mtry,
  trees = lgb_best_top$trees,
  min_n = lgb_best_top$min_n,
  tree_depth = lgb_best_top$tree_depth,
  learn_rate = lgb_best_top$learn_rate,
  loss_reduction = lgb_best_top$loss_reduction,
  sample_size = lgb_best_top$sample_size,
  stop_iter = lgb_best_top$stop_iter,
  mode = "classification"
) %>% 
  set_engine(engine = "lightgbm", num.threads = 8) %>% 
  translate()

lgb_min_final <- run_last_fit(
  "LGB top predictors",
  lgb_spec,
  lgb_min_tuned,
  recipe = base_min_pred_recipe,
  train_test_split = data_split_min,
  return_fit = TRUE
)

lgb_min_final[[1]]
```

#### Final ROC

```{r}
make_final_roc(lgb_min_final[[2]], "LGB top pred")
```

#### Final confusion matrix

```{r}
make_cm(lgb_min_final[[2]])
```

#### Final metrix

```{r}
get_metrics(lgb_min_final[[2]], "LGB top pred") %>% pivot_wider(names_from = model, values_from = .estimate)
```

#### Final sensitivity/specificity plot

```{r}
senspec_plot(lgb_min_final[[2]], "LGB top pred")
```

## AMR types and false predictions

```{r, eval=FALSE}
# this is for Dan
amr_types_strain <- read_csv("data/amr_types_strain.csv")

prediction_all_data %>%  
  left_join(amr_types_strain, by="strain") %>% 
  write.csv(file="data/amr_types_prediction_prob.csv", row.names = F)
```

```{r, eval=FALSE}
# amr_type_pred <- read_csv("data/amr_types_prediction_prob.csv")

amr_type_pred %>% 
  filter(.pred_class != .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

```{r, eval=FALSE}
amr_type_pred %>% 
  filter(.pred_class == .true_class) %>% 
  pivot_longer(cols = colnames(amr_type_pred)[5:25],
               names_to = "AMR.gene",
               values_to = "n") %>% 
  ggplot(aes(strain, AMR.gene, fill=n)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="blue") 
```

