---
title: "NMF_PAP"
author: "Michele Rossi"
format: revealjs
editor: visual
jupyter: python3
---


```{python}
import os
os.getcwd()
```

```{python}
import numpy as np
import sklearn
from sklearn.decomposition import NMF
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import pandas as pd
import csv
import math
import seaborn
from sklearn.model_selection import StratifiedShuffleSplit



#file with PAP curves
ALL_TPZP= pd.read_csv('data/ALL_TZP_PAP_strain_frequencies.csv')
ALL_TPZP= ALL_TPZP.dropna(axis=1, how='all')

#file with HR 
hr_out = pd.read_csv('data/heteroresistance_testing_gr123.csv', delimiter=',').dropna()
ALL_TPZP
```

associate every PAP curve to the resitance type

```{python}
hr_out_d =  dict(zip(hr_out['strain'], hr_out['resistance']))
ALL_TPZP.columns = [hr_out_d.get(x, x) for x in ALL_TPZP.columns]
ALL_TPZP
keep_cols = ['nonHR', 'HR']

remove_cols = [col for col in ALL_TPZP.columns if col not in keep_cols]

ALL_TPZP = ALL_TPZP.drop(remove_cols, axis=1)
print("Removed columns:", remove_cols)

ALL_TPZP = ALL_TPZP.iloc[:, sorted(range(len(ALL_TPZP.columns)), key=lambda i: ALL_TPZP.columns[i])]
ALL_TPZP
```

#Heatmap of the PAP curves

```{python}
ax = seaborn.heatmap(ALL_TPZP)
plt.show()
plt.close()
```

remove last row

```{python}
ALL_TPZP = ALL_TPZP.drop(ALL_TPZP.index[-1])
```

# Non-negative Matrix Factorization

The matrix V (m x n) can be considered as the original data matrix. The goal of NMF is to approximate V as the product of two lower-rank matrices: W (m x r) and H (r x n), where r is typically smaller than m or n.

W is often interpreted as the basis matrix, capturing the underlying features in the dataset, while H is the coefficient matrix that expresses the original data points as a linear combination of these basis elements.

Here we take the number of components/ranks (r) equal as two as suggested by Zhai et al. (<https://doi.org/10.1101/2022.05.29.22275734>)

Since NMF has a random initiation we try to iterate 100 times to check if the results are coherent.

## 100 iterations of the whole dataset
```{python}
model = NMF(n_components=2, max_iter=500,init = "nndsvda")

W0_all = []
W1_all = []

for i in range(100):
    H = model.fit_transform(ALL_TPZP)
    W = model.components_
    W0_all.append(W[0])
    W1_all.append(W[1])
    
W0_all =np.array(W0_all)
W1_all =np.array(W1_all)
```

```{python}
ax = seaborn.heatmap(W0_all)
ax.set_title('W0')
plt.show()
plt.close()
```

```{python}
ax = seaborn.heatmap(W1_all)
ax.set_title('W1')
plt.show()
plt.close()
```

The results seem coherent.

but do HR and nonHR differ?

```{python}
W0_all_df = pd.DataFrame(W0_all, columns = ALL_TPZP.columns)
W1_all_df = pd.DataFrame(W1_all, columns = ALL_TPZP.columns)

column_means_W0 = W0_all_df.mean()
column_means_W1= W1_all_df.mean()

grouped = column_means_W0.groupby(level=0)

boxplot_data = [grouped.get_group(x).values for x in grouped.groups]

fig, ax = plt.subplots()
ax.boxplot(boxplot_data)


ax.set_xticklabels(grouped.groups.keys())


ax.set_xlabel('Group')
ax.set_ylabel('Value')
ax.set_title('W0')

plt.show()
```

```{python}
grouped = column_means_W1.groupby(level=0)

boxplot_data = [grouped.get_group(x).values for x in grouped.groups]

fig, ax = plt.subplots()
ax.boxplot(boxplot_data)


ax.set_xticklabels(grouped.groups.keys())


ax.set_xlabel('Group')
ax.set_ylabel('Value')
ax.set_title('W1')

plt.show()
```

The dataset is unbalanced,

```{python}
s_HR =ALL_TPZP["HR"]
s_HR.shape
```

```{python}
s_nHR =ALL_TPZP["nonHR"]
s_nHR.shape
```

```{python}
subset_nHR = s_nHR.iloc[:, np.random.randint(0, s_nHR.shape[1], size=10)]
subset_nHR
```

## 500 iterations of balanced subsets
we can try to build random subsets and iterate 500 times

```{python}
#subset_all.shape

model = NMF(n_components=2, max_iter=500,init = "nndsvda")

W0_all = []
W1_all = []

for i in range(500):
    subset_HR = s_HR.iloc[:, np.random.randint(0, s_HR.shape[1], size=15)]
    subset_nHR = s_nHR.iloc[:, np.random.randint(0, s_nHR.shape[1], size=15)]
    subset_all = pd.concat([subset_HR, subset_nHR], axis=1)
    H = model.fit_transform(subset_all)
    W = model.components_
    W0_all.append(W[0])
    W1_all.append(W[1])
    
W0_all =np.array(W0_all)
W1_all =np.array(W1_all)


W0_all_ps =pd.Series(W0_all[0])
W1_all_ps =pd.Series(W1_all[0])
```

in the following we see HR on the left and nonHR on the right.
### W0
```{python}
ax = seaborn.heatmap(W0_all, cmap='viridis')
ax.set_title('W0')
plt.show()
plt.close()
```

```{python}
W0_all_df = pd.DataFrame(W0_all, columns = subset_all.columns)
column_means_W0 = W0_all_df.mean()

grouped = column_means_W0.groupby(level=0)

boxplot_data = [grouped.get_group(x).values for x in grouped.groups]

fig, ax = plt.subplots()
ax.boxplot(boxplot_data)


ax.set_xticklabels(grouped.groups.keys())


ax.set_xlabel('Group')
ax.set_ylabel('Value')
ax.set_title('W0')

plt.show()
plt.close()
```

```{python}
fig, ax = plt.subplots()

W0_all_ps = W0_all_ps.rename(index=dict(zip(W0_all_ps.index, list(subset_all.columns))))
W0_all_ps_sorted = W0_all_ps.sort_values()

seaborn.scatterplot(x=W0_all_ps_sorted.values, y=range(30), data=W0_all_ps_sorted, hue=W0_all_ps_sorted.index)

ax.set_title('W0')
plt.show()
plt.close()
```


### W1
```{python}
ax = seaborn.heatmap(W1_all, cmap='viridis')
ax.set_title('W1')
plt.show()
plt.close()
```

```{python}
W0_all_df = pd.DataFrame(W1_all, columns = subset_all.columns)
column_means_W1 = W1_all_df.mean()

grouped = column_means_W1.groupby(level=0)

boxplot_data = [grouped.get_group(x).values for x in grouped.groups]

fig, ax = plt.subplots()
ax.boxplot(boxplot_data)


ax.set_xticklabels(grouped.groups.keys())


ax.set_xlabel('Group')
ax.set_ylabel('Value')
ax.set_title('W1')

plt.show()

```


```{python}
fig, ax = plt.subplots()

W1_all_ps = W1_all_ps.rename(index=dict(zip(W1_all_ps.index, list(subset_all.columns))))
W1_all_ps_sorted = W1_all_ps.sort_values()

seaborn.scatterplot(x=W1_all_ps_sorted.values, y=range(30), data=W1_all_ps_sorted, hue=W1_all_ps_sorted.index)

ax.set_title('W1')
plt.show()
plt.close()
```
