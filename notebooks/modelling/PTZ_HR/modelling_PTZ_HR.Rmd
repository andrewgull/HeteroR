---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
```

# Read data

Here I use processed and scaled data from EDA_PTZ.

All 'R' strains will be removed

```{r}
df <- readr::read_csv("data/features_ptz_strain.csv") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR"))) %>%
  mutate(n.beta.lac.3 = factor(if_else(n.beta.lac.3 == 1, "yes", "no"))) %>%
  mutate(n.beta.lac.4 = factor(if_else(n.beta.lac.4 == 1, "yes", "no")))

df 
```

```{r}
skim(df) %>% yank("factor")
```

Unbalanced target class

# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(df, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing

## Recipe

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe
```

## Bake ancd check

```{r}
train_data <- main_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

head(train_data)
```

## NZV predictors

```{r}
setdiff(names(df), names(train_data))
```


## Folds and metrics

>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10)
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

# Penalized Logistic regression

## Tuning

```{r}
# adjust the main recipe
lr_recipe <- main_recipe %>%
  step_corr(threshold = 0.75)

# set model type/engine
lr_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = lr_reg_grid,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(lr_res)
```

## Best models

```{r}
top_lr <-
  lr_res %>% 
  show_best("roc_auc", n = 5) %>% 
  arrange(penalty) 

top_lr %>% arrange(penalty)
```

### ROC of the best model

```{r}
lr_best <- lr_res %>%
  select_best(metric = "roc_auc")

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR")

autoplot(lr_auc)
```

# Multivariate adaptive regression splines (MARS)

## Tune

```{r}
# prune method default: ‘backward’ 
# other methods "backward", "none", "exhaustive", "forward", "seqrep", "cv"
mars_mod <- 
  mars(
    mode = "classification",
    engine = "earth",
    num_terms = tune(),
    prod_degree = tune(),
    prune_method = "backward") %>% 
  translate()

# define the workflow
mars_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
mars_res <- tune_grid(mars_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(mars_res)
```

## Best models

### Based on AUC

```{r}
mars_res %>% 
  show_best(metric = "roc_auc")
```

### Based on J-index

```{r}
mars_res %>% 
  show_best(metric = "j_index")
```


### ROC of the best model

```{r}
mars_best <- mars_res %>%
  select_best(metric = "roc_auc")

mars_auc <- 
  mars_res %>% 
  collect_predictions(parameters = mars_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "MARS")

autoplot(mars_auc)
```

# Linear support vector machines (lSVM)

## Tune

```{r}
svm_mod <- 
  svm_linear(
    cost = tune()) %>% # margin - for regression only
  set_mode("classification") %>%
  set_engine("kernlab")  # default

# define the workflow
svm_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_res <- tune_grid(svm_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```


## Metrics autoplot

```{r}
autoplot(svm_res)
```

## Best models

```{r}
svm_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
svm_best <- svm_res %>%
  select_best(metric = "roc_auc")

svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM")

autoplot(svm_auc)
```

# K-nearest neighbors (KNN)

## Tune

```{r}
knn_mod <- nearest_neighbor(
    neighbors = tune(),
    weight_func = tune(),
    dist_power = tune()) %>% #
  set_engine("kknn") %>%
  set_mode("classification")

# define the workflow
knn_workflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here


# train and tune the model
knn_res <- tune_grid(knn_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(knn_res)
```

## Best models

```{r}
knn_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
knn_best <- knn_res %>%
  select_best(metric = "roc_auc")

knn_auc <- 
  knn_res %>% 
  collect_predictions(parameters = knn_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "KNN")

autoplot(knn_auc)
```

# Random Forest (RF)

## Tune

```{r}
cores <- 4L

# model
rf_mod <- 
  rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

rm_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(main_recipe) # main recipe here

set.seed(5732)
# run
rf_res <- tune_grid(rm_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(rf_res)
```

## Best models

```{r}
# automatic choice of the best model
rf_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")

rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "RF")

autoplot(rf_auc)
```

# Boosted Trees (BT)

Using eXtreme Gradient Boosting

## Tune 

```{r}
set.seed(732)

# number of cores available on Kaggle
cores <- 4L 

# model specification
xgb_mod <- 
  boost_tree(
    trees = 50, 
    mtry = tune(), 
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = tune(), 
    loss_reduction = tune(), 
    sample_size = tune(), 
    stop_iter = tune()) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
xgb_cv_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(main_recipe)

# tune models, this takes time
xgb_res <- tune_grid(xgb_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Metrics autoplot

```{r, fig.width=14}
autoplot(xgb_res)
```

## Best models

```{r}
xgb_res %>% 
  show_best(metric = "roc_auc", n = 5)
```


```{r}
xgb_res %>% 
  show_best(metric = "j_index", n=40) %>% 
  filter(.config == "Preprocessor1_Model11")
```

### ROC of the best

```{r}
xgb_best <- xgb_res %>% 
  select_best(metric = "roc_auc")

xgb_auc <- xgb_res %>% 
  collect_predictions(parameters = xgb_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "BT")

autoplot(xgb_auc)
```


# Ensemble of the models

```{r}

```


# Comparison of all models

```{r, fig.width=14}
roc_plot <- bind_rows(xgb_auc, rf_auc, lr_auc, mars_auc, svm_auc, knn_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Dark2")

roc_plot
```

SVM is the best according to AUC, RF and BT are close

```{r}
ggsave("plots/roc.png", roc_plot)
```


# Final fit

```{r}
# the last model
# last_mod <- 
#   boost_tree(trees = 1000,
#              mtry = xgb_best$mtry, 
#              min_n = xgb_best$min_n, 
#              tree_depth = xgb_best$tree_depth,
#              learn_rate = xgb_best$learn_rate,
#              loss_reduction = xgb_best$loss_reduction,
#              sample_size = xgb_best$sample_size,
#              stop_iter = xgb_best$stop_iter) %>%
#   set_engine("xgboost", num.threads = cores) %>% 
#   set_mode("classification")

last_mod <-
  svm_linear(
    cost = svm_best$cost
  ) %>% 
  set_mode("classification") %>%
  set_engine("kernlab") 

# the last workflow
last_wf <- 
  svm_workflow %>% 
  update_model(last_mod)

# the last fit
set.seed(345)
last_fit <- 
  last_wf %>% 
  last_fit(data_split)

last_fit %>% 
  collect_metrics()
```

```{r}
last_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 77)
```

```{r}
last_fit %>% extract_fit_parsnip() 
```


## ROC

```{r}
last_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

## Confusion Matrix

```{r}
cm <- last_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

## Probability cutoff adjustment

```{r}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  last_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: linear SVM"
  )

sens_spec_j_plot
```

Moving threshold to the crossing point between sensitivity and specificity curves (0.405) doesn't imporve overall performance of the model.

```{r}
ggsave("plots/prob_cutoff.png", sens_spec_j_plot)
```


## Optimized confusion matrix

j-index

```{r}
pred_optimized <- last_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

cm_heatmap <- autoplot(cm_optimized, type = "heatmap")
cm_heatmap
```

```{r}
ggsave("plots/confusion_matrix.png", cm_heatmap)
```


```{r}
summary(cm_optimized)
```


---


```{r}
save.image("data/workspace.RData")
```

