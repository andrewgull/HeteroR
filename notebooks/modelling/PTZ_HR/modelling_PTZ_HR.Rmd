---
title: "Modelling PTZ HR"
author: "by A.G."
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
output:
  html_document:
    toc: yes
    df_print: paged
    code_folding: hide
    fig_width: 10
    fig_height: 6
    theme: cerulean
    highlight: kate
    toc_float: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 10, fig.height = 5, message = F, warning = F, cache = F)
library(tidymodels)
library(themis)
library(probably)
library(vip)
library(skimr)
library(stacks)
```

# Read data

Here I use processed and scaled data from EDA_PTZ.

All 'R' strains will be removed

```{r}
df <- readr::read_csv("data/features_ptz_strain.csv") %>% 
  filter(resistance != "R") %>% 
  mutate(resistance = factor(resistance, levels = c("HR", "nonHR"))) %>%
  mutate(n.beta.lac.3 = factor(if_else(n.beta.lac.3 == 1, "yes", "no"))) %>%
  mutate(n.beta.lac.4 = factor(if_else(n.beta.lac.4 == 1, "yes", "no")))

df 
```

```{r}
skim(df) %>% yank("factor")
```

We have unbalanced target class

# PCA

```{r}
pca_rec <- recipe(resistance ~., data = df) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)
```

>Let us access the third element (of steps) to get the standard deviations from the PCA analysis and use them to compute percentage of variation explained by each PC.

```{r}
# find the step number corresponding to PCA, here it's 4
sdev <- pca_prep$steps[[4]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

var_df <- data.frame(PC = paste0("PC", 1:length(sdev)),
                     var_explained = percent_variation,
                     stringsAsFactors = FALSE)

var_df <- var_df %>% 
  mutate(var_cum_sum = cumsum(var_explained))

var_df
```

```{r}
var_df %>%
  mutate(PC = forcats::fct_inorder(PC)) %>%
  ggplot(aes(x = PC, y = var_explained)) + 
  geom_col(aes(fill = var_cum_sum)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  scale_fill_continuous(type = "viridis")
```


## How much each predictor contributes to each Principal Component

```{r, fig.height=8}
tidied_pca <- tidy(pca_prep, 4)

tidied_pca %>%
  filter(component %in% paste0("PC", 1:6)) %>%
  mutate(component = forcats::fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)
```

## Top 8 absolute contributers to each Principal Components

```{r, fig.width=12}
library(tidytext)

my_colors <- RColorBrewer::brewer.pal(12, "Paired")[c(2,4)]

tidied_pca %>%
  filter(component %in% paste0("PC", 1:6)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive"
  )
```

```{r}
library(rgl)


pca_df <- juice(pca_prep) %>% mutate(color = if_else(resistance == "HR", "red", "blue"))


plot3d(x = pca_df$PC01, y = pca_df$PC02, z = pca_df$PC03,
       col = pca_df$color,
       type = "s", 
       radius = 0.2,
       xlab = "PC1",
       ylab = "PC2",
       zlab = "PC3")
```


```{r}
pca_df %>%
  ggplot(aes(PC01, PC02, label = strain)) +
  geom_point(aes(color = resistance, shape = resistance), alpha = 0.7, size = 2) +
  labs(color = NULL) +
  scale_color_brewer(palette = "Set1")
```

```{r}
pca_df %>%
  ggplot(aes(PC02, PC03, label = strain)) +
  geom_point(aes(color = resistance, shape = resistance), alpha = 0.7, size = 2) +
  labs(color = NULL) +
  scale_color_brewer(palette = "Set1")
```

```{r}
pca_df %>%
  ggplot(aes(PC03, PC01, label = strain)) +
  geom_point(aes(color = resistance, shape = resistance), alpha = 0.7, size = 2) +
  labs(color = NULL) +
  scale_color_brewer(palette = "Set1")
```


# Uniform Manifold Approximation and Projection (UMAP)

## UMAP from 'umap' package

Default settings

```{r}
library(umap)

umap_rec <- recipe(resistance ~., data = df) %>%
  update_role(strain, new_role = "id") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors()) 

umap_prep <- juice(prep(umap_rec))

umap_embed <- umap::umap(d = umap_prep[, -c(1, 67)], 
                         n_neighbors = 20, 
                         min_dist = 0.1, 
                         metric = "manhattan", 
                         n_epochs = 200)


umap_tib <- tibble(resistance = umap_prep$resistance,
                  UMAP1 = umap_embed$layout[, 1],
                  UMAP2 = umap_embed$layout[, 2])

ggplot(umap_tib, aes(UMAP1, UMAP2)) +
  geom_point(aes(color = resistance, shape = resistance), alpha = 0.7, size = 2) +
  labs(color = NULL) +
  scale_color_brewer(palette = "Set1")
  
```

## UMAP from 'embed' and 'tidyverse'

```{r}
library(embed)

umap_rec <- recipe(resistance ~., data = df) %>%
  update_role(strain, new_role = "id") %>%
  step_nzv(all_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors(), neighbors = 10)

umap_prep <- prep(umap_rec)

juice(umap_prep) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = resistance, shape = resistance), alpha = 0.7, size = 2) +
  labs(color = NULL) +
  scale_color_brewer(palette = "Set1")
```


# Data split

Stratified split

```{r}
set.seed(124)

data_split <- initial_split(df, prop = 0.8, strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

df_train %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

```{r}
df_test %>% 
  count(resistance) %>% 
  mutate(prop = n/sum(n))
```

# Preprocessing

## Basic Recipe

>Some models (notably neural networks, KNN, and support vector machines) require predictors that have been centered and scaled, so some model workflows will require recipes with these preprocessing steps. For other models, a traditional response surface design model expansion (i.e., quadratic and two-way interactions) is a good idea.

```{r}
main_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)
  
main_recipe
```

### Bake and check

```{r}
train_data <- main_recipe %>% 
  prep(training = df_train) %>% 
  bake(new_data = NULL) # df_train will be processed

head(train_data)
```

```{r}
train_data %>% group_by(resistance) %>% count()
```


### NZV predictors

```{r}
setdiff(names(df), names(train_data))
```


## PCA + Yeo-Johnson transformation recipe

From [here](https://www.tmwr.org/grid-search.html):

>Because of the high degree of correlation between predictors, it makes sense to use PCA feature extraction to decorrelate the predictors. The following recipe contains steps to transform the predictors to increase symmetry, normalize them to be on the same scale, then conduct feature extraction. The number of PCA components to retain is also tuned, along with the model parameters.

>While the resulting PCA components are technically on the same scale, the lower-rank components tend to have a wider range than the higher-rank components. For this reason, we normalize again to coerce the predictors to have the same mean and variance.

>Many of the predictors have skewed distributions. Since PCA is variance based, extreme values can have a detrimental effect on these calculations. To counter this, letâ€™s add a recipe step estimating a Yeo-Johnson transformation for each predictor (Yeo and Johnson 2000). While originally intended as a transformation of the outcome, it can also be used to estimate transformations that encourage more symmetric distributions

```{r}
pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>% 
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(resistance, over_ratio = 1, seed = 100)

#  step_YeoJohnson(all_numeric_predictors()) %>% 
#  step_normalize(all_numeric_predictors()) %>% 
#  step_pca(all_numeric_predictors(), num_comp = tune()) %>% 
#  step_normalize(all_numeric_predictors())

pca_recipe
```

## LR recipe

```{r}
# adjust the main recipe
lr_recipe <- main_recipe %>%
  step_corr(threshold = 0.75)
```

## Folds and metrics

>If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities.

```{r}
# Stratified, repeated 10-fold cross-validation is used to resample the model:
cv_folds <- vfold_cv(df_train, strata = "resistance", v = 10, repeats = 10) # is better than v=5
# metrics for imbalanced classes
cls_metrics <- metric_set(roc_auc, j_index)
```

## Grid

For creating a tuning grid the maximum entropy design is used `tune()`'s default.

# Penalized Logistic regression

## Space-filling grid: LR recipe

```{r}
set.seed(333)
# set model type/engine
lr_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1) %>% 
  set_engine("glmnet")

# define the workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# create a tune grid
# lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))

# train and tune the model
lr_res <- tune_grid(lr_workflow,
              grid = 30,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)

```

### Metrics autoplot

```{r}
autoplot(lr_res)
```

### Best models

```{r}
lr_res %>% 
  show_best("roc_auc", n = 5) 
```

#### ROC of the best model

```{r}
lr_best <- lr_res %>%
  select_best(metric = "roc_auc")

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR")

autoplot(lr_auc)
```

## Space-filling grid: PCA recipe

```{r}
lr_pca_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
lr_pca_res <- tune_grid(lr_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(lr_pca_res)
```


### Best models

```{r}
lr_pca_res %>% 
  show_best("roc_auc", n = 5)
```

### ROC

```{r}
lr_pca_best <- lr_pca_res %>%
  select_best(metric = "roc_auc")

lr_pca_auc <- 
  lr_pca_res %>% 
  collect_predictions(parameters = lr_pca_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "LR + PCA")

autoplot(lr_pca_auc)
```

## Bayesian grid: LR recipe

```{r}
# extraxt settings
lr_set <- extract_parameter_set_dials(lr_workflow)

set.seed(12)

lr_bres <-
  lr_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = lr_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE)
  )

# roc_auc = 0.8889232 (+/-0.00785)
# penalty=  0.009127915	
```

## Bayesian grid: PCA recipe

```{r}
# extraxt settings
lr_set <- extract_parameter_set_dials(lr_workflow)

set.seed(12)

lr_pca_bres <-
  lr_pca_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = lr_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE)
  )
```


# Multivariate adaptive regression splines (MARS)

## Space-filling grid

```{r}
# prune method default: â€˜backwardâ€™ 
# other methods "backward", "none", "exhaustive", "forward", "seqrep", "cv"
mars_mod <- 
  mars(
    mode = "classification",
    engine = "earth",
    num_terms = tune(),
    prod_degree = tune(),
    prune_method = "backward") %>% 
  translate()

# define the workflow
mars_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
mars_res <- tune_grid(mars_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Metrics autoplot

```{r}
autoplot(mars_res)
```

### Best models

#### Based on AUC

```{r}
mars_res %>% 
  show_best(metric = "roc_auc")
```

#### Based on J-index

```{r}
mars_res %>% 
  show_best(metric = "j_index")
```


#### ROC of the best model

```{r}
mars_best <- mars_res %>%
  select_best(metric = "roc_auc")

mars_auc <- 
  mars_res %>% 
  collect_predictions(parameters = mars_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "MARS")

autoplot(mars_auc)
```

## Space-filling grid with PCA recipe

Works better than with LR recipe

```{r}
mars_pca_workflow <- 
  workflow() %>% 
  add_model(mars_mod) %>% 
  add_recipe(pca_recipe)

# train and tune the model
mars_pca_res <- tune_grid(mars_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

### Best models

```{r}
mars_pca_res %>% 
  show_best("roc_auc", n = 5)

# best auc = 0.87
```



# Linear support vector machines (lSVM)

## Space-filling grid: LR recipe

```{r}
svm_mod <- 
  svm_linear(
    cost = tune()) %>% # margin - for regression only
  set_mode("classification") %>%
  set_engine("kernlab")  # default

# define the workflow
svm_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_res <- tune_grid(svm_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Space-filling grid: PCA recipe

```{r}
# define the workflow
svm_pca_workflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(pca_recipe) # recipe for LR here

# train and tune the model
svm_pca_res <- tune_grid(svm_pca_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
# roc_auc = 0.8513420
# cost = 0.002377809
```


## Bayesian grid

```{r}
# extract settings
svm_set <- extract_parameter_set_dials(svm_workflow)

set.seed(12)

svm_bres <-
  svm_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE, save_pred = TRUE)
  )

svm_bres %>% 
  show_best(metric = "roc_auc")

# cost = 0.01040482
# mean auc = 0.8893831	
```


## Metrics autoplot

```{r}
autoplot(svm_res)
```

## Best models

```{r}
svm_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
svm_best <- svm_res %>%
  select_best(metric = "roc_auc")

svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM")

autoplot(svm_auc)
```

# Polynomial SVM

## Tune with space-filling grid

```{r}
set.seed(101)

svm_poly_mod <- svm_poly(
    cost = tune(),
    degree = tune(),
    scale_factor = tune(),
    margin = NULL ) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab", num.threads = 4L)  # default

# define the workflow
svm_poly_workflow <- 
  workflow() %>% 
  add_model(svm_poly_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_poly_res <- tune_grid(svm_poly_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```


## Tune with Bayesian-optimized grid

```{r}
svm_set <- extract_parameter_set_dials(svm_poly_workflow)
svm_set
```



```{r}
library(doParallel)
registerDoParallel(cores=4)
library(foreach)

set.seed(12)

# use foreach here

svm_poly_bres <-
  svm_poly_workflow %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = TRUE)
  )


```


## Metrics autoplot

```{r}
autoplot(svm_poly_res)
```

## Best models

```{r}
svm_poly_res %>% 
  show_best(metric = "roc_auc")
```

### ROC of the best model

```{r}
svm_poly_best <- svm_poly_res %>%
  select_best(metric = "roc_auc")

svm_poly_auc <- 
  svm_poly_res %>% 
  collect_predictions(parameters = svm_poly_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_polynomial")

autoplot(svm_poly_auc)
```


# Radial basis function SVM

## Tune

```{r}
svm_rbf_mod <- 
  svm_rbf(
    cost = tune(),
    rbf_sigma = tune(),
    margin = NULL) %>% # regression only 
set_mode("classification") %>%
set_engine("kernlab")  # default

# define the workflow
svm_rbf_workflow <- 
  workflow() %>% 
  add_model(svm_rbf_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here

# train and tune the model
svm_rbf_res <- tune_grid(svm_rbf_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(svm_rbf_res)
```

## Best models

```{r}
svm_rbf_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
svm_rbf_best <- svm_rbf_res %>%
  select_best(metric = "roc_auc")

svm_rbf_auc <- 
  svm_rbf_res %>% 
  collect_predictions(parameters = svm_rbf_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "SVM_radial")

autoplot(svm_rbf_auc)
```


# K-nearest neighbors (KNN)

## Tune

```{r}
knn_mod <- nearest_neighbor(
    neighbors = tune(),
    weight_func = tune(),
    dist_power = tune()) %>% #
  set_engine("kknn") %>%
  set_mode("classification")

# define the workflow
knn_workflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(lr_recipe) # recipe for LR here


# train and tune the model
knn_res <- tune_grid(knn_workflow,
              grid = 25,
              resamples = cv_folds,
              control = control_grid(save_pred = TRUE),
              metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(knn_res)
```

## Best models

```{r}
knn_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
knn_best <- knn_res %>%
  select_best(metric = "roc_auc")

knn_auc <- 
  knn_res %>% 
  collect_predictions(parameters = knn_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "KNN")

autoplot(knn_auc)
```

# Random Forest (RF)

## Tune

```{r}
cores <- 4L

# model
rf_mod <- 
  rand_forest(
      mtry = tune(), 
      min_n = tune(), 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

rm_cv_wf <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(main_recipe) # main recipe here

set.seed(5732)
# run
rf_res <- tune_grid(rm_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Metrics autoplot

```{r}
autoplot(rf_res)
```

## Best models

```{r}
# automatic choice of the best model
rf_res %>% 
  show_best(metric = "roc_auc")
```


### ROC of the best model

```{r}
rf_best <- 
  rf_res %>% 
    select_best(metric = "roc_auc")

rf_auc <- 
  rf_res %>% 
    collect_predictions(parameters = rf_best) %>% 
    roc_curve(resistance, .pred_HR) %>% 
    mutate(model = "RF")

autoplot(rf_auc)
```

### Variable Importance

```{r}
# the last model

rf_best_mod <-
  rand_forest(
      mtry = rf_best$mtry, 
      min_n = rf_best$min_n, 
      trees = 1000) %>% 
  set_engine("ranger", num.threads = 2) %>% 
  set_mode("classification")

# the last workflow
rf_best_wf <- 
  svm_poly_workflow %>% 
  update_model(rf_best_mod)

# the last fit
set.seed(345)
rf_best_fit <- 
  rf_best_wf %>% 
  last_fit(data_split)

rf_best_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 67)
```


# Boosted Trees (BT)

Using Extreme Gradient Boosting

## Space-fiiling grid 

```{r}
set.seed(732)

# number of cores available on Kaggle
cores <- 4L 

# model specification
xgb_mod <- 
  boost_tree(
    trees = 50, 
    mtry = tune(), 
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = tune(), 
    loss_reduction = tune(), 
    sample_size = tune(), 
    stop_iter = tune()) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification")

# join model and processing recipe
xgb_cv_wf <- workflow() %>% 
  add_model(xgb_mod) %>% 
  add_recipe(main_recipe)

# tune models, this takes time
xgb_res <- tune_grid(xgb_cv_wf,
            grid = 25,
            resamples = cv_folds,
            control = control_grid(save_pred = TRUE),
            metrics = cls_metrics)
```

## Bayesian tuning

```{r}
# extract settings
xgb_set <- extract_parameter_set_dials(xgb_cv_wf) %>%
  finalize(x = df_train %>% select(-resistance))

set.seed(12)

xgb_bres <-
  xgb_cv_wf %>% 
  tune_bayes(
    resamples = cv_folds,
    # To use non-default parameter ranges
    param_info = xgb_set,
    # Generate 5x7 at semi-random to start
    initial = 35,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = FALSE)
  )

xgb_bres %>% 
  show_best(metric = "roc_auc")

# mtry = 74 or 13
# min_n = 2 or 2
# tree_depth = 10 or 7
# learn_rate = 0.18751350 or 0.10381054
# loss_reduction = 5.456852e-07 or 1.287068e-06
# sample_size = 0.6358425 or 0.6781397
# stop_iter = 6 or 6
# roc_auc = 0.8840152 or 0.8811742	
# n = 100
```

## Metrics autoplot

```{r, fig.width=14}
autoplot(xgb_res)
```

## Best models

```{r}
xgb_res %>% 
  show_best(metric = "roc_auc", n = 5)
```


```{r}
xgb_res %>% 
  show_best(metric = "j_index", n = 40) %>% 
  filter(.config == "Preprocessor1_Model11")
```

### ROC of the best

```{r}
xgb_best <- xgb_res %>% 
  select_best(metric = "roc_auc")

xgb_auc <- xgb_res %>% 
  collect_predictions(parameters = xgb_best) %>% 
  roc_curve(resistance, .pred_HR) %>% 
  mutate(model = "BT")

autoplot(xgb_auc)
```

### Variable Importance

```{r}
xgb_res %>% vip() 
```


# Ensemble of the models

```{r}

```


# Comparison of all models

```{r, fig.width=8}
roc_plot <- bind_rows(xgb_auc, rf_auc, lr_auc, mars_auc, svm_auc, svm_poly_auc, svm_rbf_auc, knn_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_brewer(palette = "Dark2")

roc_plot
```

SVM is the best according to AUC, RF and BT are close

```{r}
ggsave("plots/roc.png", roc_plot)
```


# Final fit

```{r}
# the last model
# svm poly
last_mod <-
  svm_poly(
    cost = svm_poly_best$cost,
    degree = svm_poly_best$degree,
    scale_factor = svm_poly_best$scale_factor
  ) %>% 
  set_mode("classification") %>%
  set_engine("kernlab") 

# xgb bayes
# mtry = 74 or 13
# min_n = 2 or 2
# tree_depth = 10 or 7
# learn_rate = 0.18751350 or 0.10381054
# loss_reduction = 5.456852e-07 or 1.287068e-06
# sample_size = 0.6358425 or 0.6781397
# stop_iter = 6 or 6

last_mod <-
  boost_tree(
    mtry = 74,
    min_n = 2,
    tree_depth = 10,
    learn_rate = 0.18751350,
    loss_reduction = 5.456852e-07,
    sample_size = 0.6358425,
    stop_iter = 6
  ) %>% 
  set_engine("xgboost", num.threads = cores) %>% 
  set_mode("classification") 

# the last workflow
xgb_last_wf <- 
  xgb_cv_wf %>% 
  update_model(last_mod)

# the last fit
set.seed(345)
xgb_last_fit <- 
  xgb_last_wf %>% 
  last_fit(data_split)

xgb_last_fit %>% 
  collect_metrics()
```

```{r}
xgb_last_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 77)
```

```{r}
last_fit %>% extract_fit_parsnip() 
```


## ROC

```{r}
last_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```

```{r}
xgb_last_fit %>% 
  collect_predictions() %>% 
  roc_curve(resistance, .pred_HR) %>% 
  autoplot()
```


## Confusion Matrix

```{r, fig.width=6}
cm <- last_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm, type = "heatmap")
```

```{r, fig.width=6}
cm_xgb <- xgb_last_fit %>%
  collect_predictions() %>%
  conf_mat(truth = resistance, estimate = .pred_class)

autoplot(cm_xgb, type = "heatmap")
```

## Performance

```{r}
summary(cm)
```

```{r}
summary(cm_xgb)
```

## Probability cutoff adjustment

```{r, fig.width=6}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  last_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: polynomial SVM"
  )

sens_spec_j_plot
```

Moving threshold to the crossing point between sensitivity and specificity curves (0.405) doesn't imporve overall performance of the model.

```{r, fig.width=6}
# collect sens, spec, j-index at various cut-offs
threshold_data <- 
  xgb_last_fit %>%
  collect_predictions() %>%
  threshold_perf(resistance, .pred_HR, thresholds = seq(0.0, 1, by = 0.05)) %>% 
  filter(.metric != "distance") %>%
  mutate(group = case_when(
    .metric == "sens" | .metric == "spec" ~ "1",
    TRUE ~ "2"
  ))

# find max j-index
max_j_index_threshold_BT <- threshold_data %>%
  filter(.metric == "j_index") %>%
  filter(.estimate == max(.estimate)) %>%
  pull(.threshold)

# plot metrics v cut-offs
sens_spec_j_plot <- ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
  geom_line(size = 1) +
  #theme_minimal() +
  #scale_color_viridis_d(end = 0.9) +
  scale_color_brewer(palette = "Set1") +
  scale_alpha_manual(values = c(.4, 1), guide = "none") +
  geom_vline(xintercept = max_j_index_threshold_BT, alpha = .8, color = "grey30", linetype = "longdash") +
  labs(
    x = "Probability",
    y = "Metric Estimate",
    title = "Final model: BT"
  )

sens_spec_j_plot
```

```{r}
ggsave("plots/prob_cutoff_BT_final.png", sens_spec_j_plot)
```


## Optimized confusion matrix

j-index

```{r, fig.width=6}
pred_optimized <- last_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

cm_heatmap <- autoplot(cm_optimized, type = "heatmap")
cm_heatmap
```

```{r}
summary(cm_optimized)
```

Metrics are better after the probability threshold adjustment.

```{r, fig.width=6}
pred_optimized <- xgb_last_fit %>%
  collect_predictions() %>% 
  mutate(
    .pred = make_two_class_pred(
      estimate = .pred_HR, 
      levels = levels(resistance), 
      threshold = max_j_index_threshold_BT
    )
  ) %>%
  select(resistance, contains(".pred"))

cm_optimized <- pred_optimized %>% 
  conf_mat(truth = resistance, estimate = .pred)

cm_heatmap <- autoplot(cm_optimized, type = "heatmap")
cm_heatmap
```

```{r}
summary(cm_optimized)
```

```{r}
ggsave("plots/confusion_matrix_BT.png", cm_heatmap)
```

---


```{r}
# updated workspace
save.image("data/all_models_local_update.RData")
```

