---
title: "Training and validation of models using workflowsets"
author: "by M.R."
output: html_document
date:  "last update: `r format(Sys.Date(), format = '%d %B %Y')`"
---

```{r}
library(tidymodels)
library(workflowsets)
library(bonsai)
library(themis)
library(colino)
library(bestNormalize)
library(baguette)
library(lightgbm)
library(tidyposterior)
library(ggplot2)
source("functions.R")
```

# Data 

```{r}
data_strain <- readr::read_csv("data/features_strain.csv",
                               na = c("NA", "-Inf"),
                               show_col_types = FALSE)

hr_testing <- readr::read_csv("data/heteroresistance_testing.csv",
                              show_col_types = FALSE)

data_strain <- data_strain %>%
  left_join(hr_testing, by = "strain")

data_strain <- data_strain %>%
  mutate(n.beta.lac.4 = factor(ifelse(n.beta.lac > 4, "yes", "no"))) %>%
  relocate(n.beta.lac.4, .before = "n.plasmids") %>%
  filter(resistance != "R", strain != "DA63310") %>%
  mutate(
    resistance = factor(resistance, levels = c("HR", "nonHR")),
    chrom.status = factor(chrom.status)
  )

data_strain$N50 <- NULL
data_strain[is.na(data_strain)] <- 0
data_strain <- data_strain %>% 
  select(-contains("oriC")) %>% 
  select(-contains("plus"))

#### SPLIT ####
# same seed number as in modelling.Rmd
set.seed(124)

# splitting proportion should be the same
data_split <- initial_split(data_strain,
                            prop = 0.8,
                            strata = resistance)

df_train <- training(data_split)
df_test <- testing(data_split)

cv_folds <- vfold_cv(df_train,
                     strata = "resistance",
                     v = 10,
                     repeats =10)

# metrics for imbalanced classes
imbalanced_metrics <- metric_set(roc_auc, j_index, mcc, pr_auc)


cores <- 8
```

# Recipies 

```{r}
base_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

pca_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>%
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_normalize(all_numeric_predictors())

base_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

base_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100)

ncorr_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))


ncorr_yj_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))


ncorr_orq_recipe <- recipe(resistance ~ ., data = df_train) %>%
  update_role(strain, new_role = "ID") %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(resistance, over_ratio = 1, seed = 100) %>% 
  step_corr(all_predictors(), threshold = tune("corr_tune"))


base_boruta_recipe <- base_recipe %>%
  step_select_boruta(all_predictors(), outcome = "resistance")

```

# Model specs 

```{r}
lr1 <- logistic_reg(
        penalty = tune(),
        mixture = 1) %>%
      set_engine("glmnet")

mars <- mars(
      mode = "classification",
      engine = "earth",
      num_terms = tune(),
      prod_degree = tune(),
      prune_method = "backward") %>%
      translate()

lsvm <- svm_linear(
        cost = tune()) %>% # margin - for regression only
      set_mode("classification") %>%
      set_engine("kernlab")  # default

rf <- rand_forest(
      mtry = tune(),
      min_n = tune(),
      trees = 1000) %>%
      set_engine("ranger", num.threads = cores) %>%
      set_mode("classification")

bt <- boost_tree(
      trees = 50,
      mtry = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune(),
      sample_size = tune(),
      stop_iter = tune()) %>%
      set_engine("lightgbm", num.threads = cores) %>%
      set_mode("classification")

bt_light <- boost_tree(
      trees = tune(),
      mtry = tune(),
      min_n = tune(),
      tree_depth = tune(),
      learn_rate = tune(),
      loss_reduction = tune()) %>%
      set_engine("lightgbm", num.threads = cores) %>%
      set_mode("classification") %>% translate()

knn <- nearest_neighbor(
      neighbors = tune(),
      weight_func = tune(),
      dist_power = tune()) %>%
      set_engine("kknn") %>%
      set_mode("classification")

psvm <- svm_poly(
      cost = tune(),
      degree = tune(),
      scale_factor = tune(),
      margin = NULL) %>% # regression only
      set_mode("classification") %>%
      set_engine("kernlab", num.threads = cores)

rbfsvm <- svm_rbf(
      cost = tune(),
      rbf_sigma = tune(),
      margin = tune(),
      mode = "classification"
    ) %>%
      set_engine("kernlab", num.threads = cores)

mlp_nnet <-
      mlp(hidden_units = tune(),
          penalty = tune(),
          epochs = tune()) %>%
      set_mode("classification") %>%
      set_engine("nnet", num.threads = cores)

mars_bag <- bag_mars(
      mode = "classification",
      num_terms = tune(),
      prod_degree = tune(),
      prune_method = tune(),
      engine = "earth"
    )
    
mlp_bag <- bag_mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
      set_engine("nnet", num.threads = cores) %>%
      set_mode("classification") %>%
      translate()


```

# A function to read or run workflowsets

```{r}
get_models_set <- function(file_path, model_specs_list, recipes_list) {
  if (!file.exists(file_path)) {
    print("File does not exist. Proceeding with training and validation.")
    models_set <-
      workflow_set(
        preproc = recipes_list,
        models = model_specs_list,
        cross = TRUE
      )
    set.seed(124)
    models_set <-
      models_set %>%
      workflow_map(
        "tune_grid",
        resamples = cv_folds,
        grid = 30,
        metrics = imbalanced_metrics,
        verbose = TRUE,
        control = control_grid(save_pred = TRUE,
                               save_workflow = TRUE)
      )
    saveRDS(object = models_set, file =  file_path)
  } else {
    print("File exists.  It will be loaded into memory.")
    models_set <- readRDS(file_path)
  }
  
  return(models_set)
}
```


# LLR 

```{r}
models_llr <-
  get_models_set(
    file_path = "~/Data/HeteroR/results/models/scheme12/models_llr.rds",
    model_spec = list(LLR = lr1),
    recipes_list = list(
      base = base_recipe,
      baseyj = base_yj_recipe,
      baseorq = base_orq_recipe,
      pca = pca_recipe
    )
  )
```

```{r}
autoplot(models_llr, select_best= TRUE)
```

```{r, fig.width=10, fig.height=8}
autoplots <- map(models_llr$result, ~autoplot(.x))

ggpubr::ggarrange(
  plotlist =  autoplots,
  ncol = 2,
  nrow = 2,
  labels = c("Base", "Base+YJ", "Base+ORQ", "PCA"), font.label = list(size=12)
)
```

remove models object to save memory

```{r}
rm(models_llr)
```

# SVM

Linear, polynomial and radial-basis function

```{r}
models_svm <-
  get_models_set(
    file_path = "~/Data/HeteroR/results/models/scheme12/models_svm.rds",
    model_specs_list = list(
      linSVM = lsvm,
      polySVM = psvm,
      rbfSVM = rbfsvm
    ),
    recipes_list = list(
      ncorr = ncorr_recipe,
      ncorryj = ncorr_yj_recipe,
      base_orq = ncorr_orq_recipe,
      pca = pca_recipe
    )
  )
```


```{r}
autoplot(models_svm, select_best= TRUE)
```

```{r, fig.width=10, fig.height=14, eval=F}
autoplots <- map(models_svm$result, ~autoplot(.x))

ggpubr::ggarrange(
  plotlist =  autoplots,
  ncol = 2,
  nrow = 6
  #labels = c("no_corr", "no_corr+YJ", "no_corr+ORQ", "PCA"), font.label = list(size=12)
)
```

remove the object to free memory
```{r}
rm(models_svm)
```


# MLP

```{r}
models_mlp <- get_models_set(
  file_path = "~/Data/HeteroR/results/models/scheme12/models_mlp.rds",
  model_specs_list = list(MLP = mlp_nnet,
                          MLP_BAG = mlp_bag),
  recipes_list = list(
    ncorr = ncorr_recipe,
    ncorryj = ncorr_yj_recipe,
    base_orq = ncorr_orq_recipe,
    pca = pca_recipe
  )
)
```

```{r}
autoplot(models_mlp, select_best = TRUE)
```


remove models object 

```{r}
rm(models_svm_mlp)
```

# KNN

```{r}
models_knn <- get_models_set(
  file_path = "~/Data/HeteroR/results/models/scheme12/models_knn.rds",
  model_specs_list = list(kNN = knn),
  recipes_list = list(
    ncorr = ncorr_recipe,
    ncorryj = ncorr_yj_recipe,
    ncorrorq = ncorr_orq_recipe ,
    pca = pca_recipe
  )
)
```


```{r}
autoplot(models_knn, select_best= TRUE)
```

remove to free memory

```{r}
rm(models_knn)
```

# MARS

```{r}
models_mars <- get_models_set(
  file_path = "~/Data/HeteroR/results/models/scheme12/models_mars.rds",
  model_specs_list = list(MARS = mars,
                          MARS_BAG = mars_bag),
  recipes_list = list(
    ncorr = ncorr_recipe,
    ncorryj = ncorr_yj_recipe,
    ncorrorq = ncorr_orq_recipe ,
    pca = pca_recipe
  )
)
```


```{r}
autoplot(models_mars, select_best= TRUE)
```

remove models object 

```{r}
rm(models_mars)
```

# RF

```{r}
models_rf <- get_models_set(
  file_path = "~/Data/HeteroR/results/models/scheme12/models_rf.rds",
  model_specs_list = list(RF = rf),
  recipes_list = list(base = base_recipe,
                      base_boruta = base_boruta_recipe)
)
```

```{r}
autoplot(models_rf, select_best = TRUE)
```


# BT

Use LightGBM

```{r}
models_rf <- get_models_set(
  file_path = "~/Data/HeteroR/results/models/scheme12/models_bt.rds",
  model_specs_list = list(BT = bt_light),
  recipes_list = list(base = base_recipe,
                      base_boruta = base_boruta_recipe)
)
```


```{r}
autoplot(models_bt, select_best = TRUE)
```

# RF BT light 

```{r, eval=F}
read_models_xgb_rf_light <- function() {
  file_path <-
    "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/models_xgb_rf_light.rds"
  
  if (!file.exists(file_path)) {
    print("File does not exist. Proceeding rith building the workflowset of the models.")
models_xgb_rf_light <- 
   workflow_set(
      preproc = list(base = base_recipe,
                     base_boruta = base_boruta_recipe ), 
      
      models = list(rf = rf, bt_light = bt_light),
      cross = TRUE
   )
set.seed(124)
models_xgb_rf_light <-
  models_xgb_rf_light %>%
 workflow_map(
    "tune_grid",
    resamples = cv_folds,
    grid = 30,
    metrics = imbalanced_metrics,
    verbose = TRUE,
      control = control_grid(save_pred = TRUE,
                             save_workflow = TRUE)
  )

saveRDS(object = models_xgb_rf_light, file =  "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/models_xgb_rf_light.rds")
  } else {
    models_xgb_rf_light <- readRDS(file_path)
  }
  
  return(models_xgb_rf_light)
}

models_xgb_rf_light <- read_models_xgb_rf_light()
```

```{r, eval=F}
autoplot(models_xgb_rf_light, select_best= TRUE)
#rank_results(models_xgb_rf_light) %>%  select(wflow_id) %>%  unique()
```


## base_bt_bres 

The following code uses Bayesian optimization on toop of the previous grid search

```{r, eval=F}
read_base_bt_bres_light <- function() {
  file_path <-
    "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/base_bt_bres_light.rds"
  
  if (!file.exists(file_path)) {
    print("File does not exist. Proceeding with optimizing the models.")
base_bt_light_wf <- workflow() %>%
  add_model(bt_light) %>%
  add_recipe(base_recipe)

param_set_base_bt <- extract_parameter_set_dials(base_bt_light_wf) %>%
    finalize(x = df_train %>% select(-resistance))

base_bt_bres <- tune_bayes(base_bt_light_wf,  resamples = cv_folds, initial = models_xgb_rf_light$result[[2]],
    iter= 30,
    metrics = metric_set(roc_auc),
    param_info = param_set_base_bt,
      control = control_bayes(no_improve =30,
                             save_pred = TRUE,
                             verbose = FALSE,
                             save_workflow = TRUE))
                            
  
saveRDS(object = base_bt_bres, file =  "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/base_bt_bres_light.rds")      
  } else {
    base_bt_bres_light <- readRDS(file_path)
  }
  
  return(base_bt_bres_light)
}

base_bt_bres_light <- read_base_bt_bres_light()
show_best(base_bt_bres_light)
```

```{r, eval=F}
autoplot(base_bt_bres_light)
```

## base_boruta_bt_bres_light

```{r, eval=F}
read_base_boruta_bt_bres_light <- function() {
  file_path <-
    "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/base_bt_boruta_bres_light.rds"
  
  if (!file.exists(file_path)) {
    print("File does not exist. Proceeding with optimizing the models.")
base_bt_boruta_light_wf <- workflow() %>%
  add_model(bt_light) %>%
  add_recipe(base_boruta_recipe)

param_set_base_bt_boruta <- extract_parameter_set_dials(base_bt_boruta_light_wf) %>%
    finalize(x = df_train %>% select(-resistance))

base_bt_boruta_bres_light <- tune_bayes(base_bt_boruta_light_wf,  resamples = cv_folds, initial = models_xgb_rf_light$result[[4]],
    iter= 30,
    metrics = metric_set(roc_auc),
    param_info = param_set_base_bt_boruta,
      control = control_bayes(no_improve =30,
                             save_pred = TRUE,
                             verbose = FALSE,
                             save_workflow = TRUE))
                            
  
saveRDS(object = base_bt_boruta_bres_light, file =  "/mnt/data/andrei/Data/HeteroR/results/models/scheme12/base_bt_boruta_bres_light.rds")      
  } else {
    base_bt_boruta_bres_light <- readRDS(file_path)
  }
  
  return(base_bt_boruta_bres_light)
}

base_bt_boruta_bres_light <- read_base_boruta_bt_bres_light()
show_best(base_bt_boruta_bres_light)
```

```{r, eval=F}
autoplot(base_bt_boruta_bres_light)
```

remove models object 
```{r, eval=F}
rm(base_bt_bres_light) 
rm(models_xgb_rf_light) # this has to be removed here as it is used for bayesian optimization
rm(base_bt_boruta_bres_light)
```
